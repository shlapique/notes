---
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
linkcolor: blue

mainfont: Linux Libertine O
fontsize: 12pt
numbersections: true
indent: true

header-includes:
- \usepackage{indentfirst}
- \usepackage[russian, english]{babel}
- \usepackage{fancyhdr}
- \usepackage{cancel}

- \pagestyle{fancy}
- \fancyhead{}
- \fancyhead[LO, LE]{\leftmark}

- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Ll}{\mathbb{L}}
- \newcommand{\N}{\mathbb{N}}
- \newcommand{\F}{\mathbb{F}}

- \setlength{\headheight}{15pt} 
---
\input{./files/title.tex}

\tableofcontents

\vspace*{\fill}

\underline{Last build:} \today

\clearpage

# Источники {-}

* [Ивченко Г. И., Медведев Ю. И. "Математическая статистика", изд. "Высшая школа", 1984](https://www.hse.ru/pubs/share/direct/content_document/103185710)

* [Кибзун А. И., Наумов А. В., Горяинова Е. Р. "Теория вероятностей и математическая статистика. Базовый курс с примерами и задачами", изд "ФИЗМАТЛИТ", 2013](https://ami.nstu.ru/~post/teaching/tv_ms/kibzun.pdf)

* [Панков А. Р., Платонов Е. Н. "Практикум по математической статистике", изд. "МАИ", 2006](http://zyurvas.narod.ru/knyhy2/Pankov_matstat.pdf)

\pagebreak

# Многомерное нормальное распределение

## Замечание {-}

Вектор $X = (X_{1}, \dots, X_{n})^{T}$ называется ***случайным***, если $X_{1}, \dots, X_{n}$ --- *случайные величины* *(далее **с.в**)*, определенные на одном вероятностном пространстве.

Через $M[X] = m_{X}$ обозначим ***вектор математического ожидания***:

\begin{align*}
M[X] = m_{X} = 
    \begin{pmatrix}
    M[X_1] \\
    \vdots \\
    M[X_n]
    \end{pmatrix}
\end{align*}

Через $K_x$ обозначим *ковариационную матрицу* с.в $X$:

\begin{align*}
K_{X} = 
    \begin{pmatrix}
    \mathrm{cov}(X_1, X_1) & \dots & \mathrm{cov}(X_1, X_n) \\
    \vdots & \ddots & \vdots   \\ 
    \mathrm{cov}(X_n, X_1) & \dots & \mathrm{cov}(X_n, X_n) \\
    \end{pmatrix}
\end{align*}

## Лемма 1

Пусть $K_{X} \in \R^{n \times n}$ --- ковариационная матрица с.в $X$. Тогда:

1. $K_{X} \geqslant 0$, т.е. $\forall x \in \R^{n} \backslash \{0\}, x^{T}K_{X}x \geqslant 0$;

2. $K_{X}^{T} = K_{X}$

## Определение 1

Случайный вектор $X = (X_1, \dots, X_n)^{T}$ называется ***невырожденным нормальным вектором***:

$$X \sim N(m_{X}, K_{X})$$

если совместная плотность вероятности имеет вид:

$$f_{X}(x) = ((2\pi)^{n} \det K_{X})^{\frac{-1}{2}} \exp\{\frac{-1}{2}(x - m_{X})^{T}K_{X}^{-1}(x - m_{X})\}$$

где $m_{X} \in \R^{n}, K_{X} \in \R^{n \times n}, K_{X} > 0, K_{X}^{T} = K_{X}$

## Лемма 2 {#lemma-1-2}

Пусть $X$ --- *невырожденный нормальный вектор* с параметрами $m_{X}$ и $K_{X}$.

Тогда $M[X] = m_{X}$, а $K_{X}$ --- ковариационная матрица $X$.

Рассмотрим основные свойства многомерного нормального распределения.

## Лемма 3 {#lemma-1-3}

Пусть $X \sim N(m_{X}, K_{X}), A \in \R^{m \times n}, b \in \R^{m}$.

Тогда:

\begin{align*}
Y = AX + b \sim N(m_Y, K_Y), \\
m_Y = Am_X + b, \\
K_Y = AK_XA^{T}.
\end{align*}

## Лемма 4 {#lemma-1-4}

Пусть $X \sim N(m_{X}, K_{X})$.

Тогда компоненты вектора $X$ ***независимы*** тогда и только тогда, когда они *некоррелированы*.

### note {-}

Доказательство данных утверждений при помощи аппарата функций распределения и плотности довольно сложно.
Поэтому рассмотрим аппарат характеристических функций.

## Определение 2

Пусть $X = (X_1, \dots, X_n)^{T}$ --- случайный вектор.

Тогда ***характеристической функцией*** называется:

$$\psi_{X}(\lambda) = M[e^{i\lambda^{T}X}] = \int\limits_{\R^{n}}e^{i\lambda^{T}X}dF_X(x)$$

## Замечание {-}

*Характеристическая функция* определена для любого случайного вектора или с.в.

Если с.в **дискретная**, то:

$$\psi_{X}(\lambda) = \sum\limits_{k = 1}^{\infty}e^{i\lambda X_{k}}p_{k}$$

Если с.в **абсолютно непрерывная**, то

$$\psi_{X}(\lambda) = \int\limits_{\R}e^{i\lambda X}f_{X}(x)dx$$

В этом случае $\psi_{X}(\lambda)$ является ***преобразованием Фурье $f_X$***.

Поскольку преобразование Фурье взаимно однозначно, а $f_X$ однозначно определяет распределение, то характеристическая функция $\psi_{X}(x)$ также однозначно определяет распределение с.в $X$.

Причем:

$$f_{X}(x) = \frac{1}{(2\pi)^{n}} \int\limits_{\R^{n}}e^{-i\lambda^{T} X}\psi_{X}(\lambda)d\lambda$$


## Лемма 5 {#lemma-1-5}

Пусть $X$ --- случайный вектор, $A \in \R^{n \times n}, b \in \R^{n}$.

Тогда:

1. для $Y = AX + b$
$$\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda)$$

2. компоненты вектора $X$ ***независимы*** тогда и только тогда, когда
$$\psi_{Y}(\lambda) = \prod\limits_{k = 1}^{n} \psi_{X_k}(\lambda_k)$$

### Доказательство {-}

1. $\psi_{Y}(\lambda) = M[e^{i\lambda^{T}Y}] = M[e^{i\lambda^{T} AX} e^{i\lambda^{T} b}] 
= e^{i\lambda^{T} b} M[e^{i(A^{T}\lambda)^{T} X}] = e^{i\lambda^{T} b} \psi_{X}(A^{T}\lambda)$

2. $\psi_{X}(\lambda) = \int\limits_{\R} \dots \int\limits_{\R} e^{i(\lambda_{1} x_{1} + \dots + \lambda_{n} x_{n})} 
f_{X}(x_1, \dots, x_n)dx_1 \cdot \dots \cdot dx_n \overset{\text{н/з}}{=}
\int\limits_{\R} \dots \int\limits_{\R} e^{i\lambda_{1} x_{1}} \cdot \dots \cdot e^{i\lambda_{n}
x_{n}} \cdot f_{X_{1}}(x) \cdot \dots \cdot f_{X_{n}}dx_1 \cdot \dots \cdot dx_n =
\int\limits_{\R} e^{i\lambda_{1} x_{1}} f_{x_1}(x_1)dx_1 \cdot \dots \cdot \int\limits_{\R} e^{i\lambda_{n} x_{n}} f_{x_n}(x_n)dx_n=
\prod\limits_{k = 1}^{n} \psi_{X_k}(\lambda_{k})$

\hfill $\blacksquare$

## Замечание {-}

При помощи характеристической функции можно дать другое определение нормального распределения. В том числе для вырожденного $K_{X}$.

## Определение 3

Случайный вектор $X$ называется ***нормальным***: $X \sim N(m_{X}, K_{X})$, если:

$$\psi_{X}(\lambda) = \exp\{i\lambda^{T} m_{X} - \frac{1}{2} \lambda^{T} K_{X}\lambda \}$$

### Доказательство леммы 3

В силу [Леммы 5, п.1](#lemma-1-5)

\begin{align*}
\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda) = e^{i\lambda^{T}b} \exp\{i\lambda^{T} Am_x - \frac{1}{2} \lambda^{T}
AK_{X}A^{T}\lambda\} = \\
= \exp\{i\lambda^{T} \underbrace{(Am_x + b)}_{m_{Y}} - \frac{1}{2}\lambda^{T} \underbrace{(AK_xA^{T})}_{K_{Y}}\lambda\} \\
\blacksquare
\end{align*}

### Доказательство леммы 4

Пусть $X_{i}, \dots, X_{n}$ *попарно некоррелированы*. Тогда $\mathrm{cov}(X_{i}, X_{j}) = 0$, 
$i \ne 0$, т.е. :

\begin{align*}
K_x = diag(\sigma_{X_1}^{2}, \dots, \sigma_{X_n}^{2}) = \\
    =  
    \begin{pmatrix}
    \sigma_{X_1}^{2} & \dots & 0 \\
    \vdots & \ddots & \vdots   \\ 
    0 & \dots & \sigma_{X_n}^{2} \\
    \end{pmatrix}
\end{align*} 

$\psi_{X}(\lambda) = \exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2} \lambda^{T} K_{X} \lambda \} =
\exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2}(\lambda_{1}^{2} \sigma_{X_1}^{2} + \dots + 
\lambda_{n}^{2} \sigma_{X_n}^{2})\} = \prod\limits_{k = 1}^{n} \exp\{i\lambda_{k} m_{X_k} - \frac{1}{2} \lambda_{k}^{2} \sigma_{K_k}^{2}\} = \prod\limits_{k = 1}^{n} \psi_{X_k} (\lambda_{k})$.

Откуда с учетом [Леммы 5, п.1](#lemma-1-5) $X_1, \dots, X_n$ --- **н/з**.

Пусть $X_1, \dots, X_n$ --- н/з. Тогда $X_1, \dots, X_n$ *попарно некоррелированы*. $\blacksquare$

## Замечание {-}

Поскольку $K_{X}$ --- невырожденная, симметрическая и положительноопределенная, то существует $S \in \R^{n \times n}$
--- *ортогональная*
(т. е. $S^{T} = S^{-1}$) такая, что:

$$S^{T}K_{X}S = \Lambda = diag(\lambda_1, \dots, \lambda_n)$$

где $\lambda_{i} > 0, i = \overline{1, n}$

Определим матрицу $\Lambda^{- \frac{1}{2}} = diag(\lambda_{1}^{- \frac{1}{2}}, \dots, \lambda_{n}^{- \frac{1}{2}})$.

Рассмотрим вектор

$$Y = \Lambda^{- \frac{1}{2}} S^{T}(X - m_{X})$$

Тогда $A = \Lambda^{- \frac{1}{2}}S^{T}, b = - \Lambda^{- \frac{1}{2}}S^{T}m_{X}$.

В силу [Леммы 3](#lemma-1-3):

<!-- не было m_X -->

$$m_{Y} = Am_{X} + b = \Lambda^{- \frac{1}{2}}S^{T}m_{X} - \Lambda^{- \frac{1}{2}}S^{T}m_{X} = 0,$$
$$K_{Y} = AK_{X}A^{T} = \Lambda^{- \frac{1}{2}}S^{T}K_{X}S\Lambda^{- \frac{1}{2}} = I,$$

т. е. $Y \sim N(0, I)$.

При помощи невырожденного линейного преобразования с.в. $X$ может быть преобразован в стандартный нормальный вектор. 

Верно и обратное:

$$X = m_{X} + S\Lambda^{\frac{1}{2}} Y,$$

откуда следует [Лемма 2](#lemma-1-2)

\pagebreak

# Теорема о нормальной корреляции

## Определение 1

***Условным математическим ожиданием абсолютно непрерывного случайного вектора*** $X$ относительно абсолютно непрерывного случайного 
вектора $Y$ называется:

$$M[X \mid Y] = \int\limits_{- \infty}^{+ \infty} xf_{X | Y} (x \mid Y)dx,$$

где $f_{X \mid Y} (x \mid Y) = \frac{f_{z}(x, Y)}{f_{y}(Y)}, z = 
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix}$

## Основные свойства условного М. О.

### Свойство 1 {#attr-1-1}

\fbox{$M[C \mid Y] = C$}

**\underline{Доказательство}**

$M[C \mid Y] = \int\limits_{- \infty}^{+ \infty} Cf_{X \mid Y} (x \mid Y)dx = \frac{C \int\limits_{- \infty}^{+ \infty} f_{z}(x, Y)dx}
{f_{Y}(Y)} = C \frac{f_{Y}(Y)}{f_{Y}(Y)} = C$. $\blacksquare$

### Свойство 2 {#attr-1-2}

\fbox{$M[X \phi(Y) | Y] = \phi(Y)M[X \mid Y]$}

**\underline{Доказательство}**

$M[\phi(Y) X \mid Y] = \int\limits_{- \infty}^{+ \infty} \phi(Y)X f_{X \mid Y}(x \mid Y)dx = \phi(Y) \int\limits_{- \infty}^{+ \infty} x
f_{X \mid Y} (x \mid Y)dx =$

$= \phi(Y) M[X \mid Y]$. $\blacksquare$

### Свойство 3 {#attr-1-3}

\fbox{$M[\alpha X_{1} + \beta X_{2} \mid Y] = \alpha M[X_{1} \mid Y] + \beta M[X_{2} \mid Y]$}

### Свойство 4 {#attr-1-4}

Пусть $X, Y$ --- *независимые*. Тогда \fbox{$M[X \mid Y] = M[X]$}

**\underline{Доказательство}**

$M[X \mid Y] = \int\limits_{- \infty}^{+ \infty} xf_{X \mid Y} (x \mid Y)dx = \int\limits_{- \infty}^{+ \infty} x \frac{f_{z}(x, Y)}
{f_{Y}(Y)}dx = \int\limits_{- \infty}^{+ \infty}x \frac{f_{X}(x) f_{Y}(Y)}{f_{Y}(Y)} dx = M[X]$. $\blacksquare$

### Свойство 5 {#attr-1-5}

\fbox{$M[M[X \mid Y]] = M[X]$} ***(формула повторного М. О.)***

\pagebreak

**\underline{Доказательство}**

$M[M[X \mid Y]]= \int\limits_{- \infty}^{+ \infty} M[X \mid Y] f_{Y} (y)dy = \int\limits_{- \infty}^{+ \infty} f_{Y}(y) 
\int\limits_{- \infty}^{+ \infty} x f_{X \mid Y} (x \mid y)dxdy = \int\limits_{- \infty}^{+ \infty}
\int\limits_{- \infty}^{+ \infty} x \frac{f_{Y}(y) f_{z}(x, y)}{f_{Y}(y)} dydx = \int\limits_{- \infty}^{+ \infty} x
\int\limits_{- \infty}^{+ \infty} f_{z} (x, y)dydx = \int\limits_{- \infty}^{+ \infty} x f_{X}(x)dx = M[X]$. $\blacksquare$

## Лемма 1 {#lemma-2-1}

Пусть $X, Y$ --- случайные векторы с конечными вторыми моментами. Тогда:

<!-- check power of 'phi(Y)..' -->
$M[(X - \hat{X})\phi(Y)^{T}] = 0$

где $\hat{X} = M[X \mid Y]$

### Доказательство {-}

$M[(X - \hat{X})\phi(Y)^{T}] = M[X \phi(Y)^{T}] - M[M[X \mid Y] \phi(Y)^{T}] =$

= по [Свойству 2](#attr-1-2) $= M[X \phi(Y)^{T}] - M[M[X \phi(Y)^{T} \mid Y]] =$ по [Свойству 5](#attr-1-5) $=
M[X \phi(Y)^{T}] - M[X \phi(Y)^{T}]
= 0$. $\blacksquare$

## Замечание {-}

Если рассмотреть евклидово пространство $\Ll_{2}(\Omega)$ со скалярным произведением:

$$(X, Y) = M[X \cdot Y]$$

то *условное* М. О. --- ***оператор ортогонального проектирования*** $X$ на подпространство, порождаемое $Y$.

## Определение 2

***Оценкой $X$ по наблюдениям $Y$*** называется любая измеримая функция $\phi(Y)$.

## Определение 3

***Оценка $\hat{X}$ называется с.к.-оптимальной оценкой $X$***, если для любой другой оценки $\tilde{X}$ верно

$$M[| X - \hat{X}|^{2}] \leqslant M[| X - \tilde{X} |^{2}]$$ 

## Теорема 1

$M[X \mid Y]$ --- ***с.к.-оптимальная оценка $X$ по наблюдениям $Y$***.

### Доказательство {-}

$M[|X - \tilde{X}|^{2}] = M[|X - \hat{X} + \hat{X} - \tilde{X}|^{2}] = M[|X - \hat{X}|^{2}] + 2M[(X - \hat{X})^{T} (\hat{X} -
\tilde{X})] + M[|\hat{X} - \tilde{X}|^{2}] \overset{*}{=}$

Поскольку по определению $\tilde{X} - \hat{X} = \phi(Y)$, то в силу [Леммы 2.1](#lemma-2-1) $M[(X - \hat{X})^{T}(\tilde{X} - \hat{X})] = 0$.

$\overset{*}{=} M[|X - \hat{X}|^{2}] + M[|\hat{X} - \tilde{X}|^{2}] \geqslant M[|X - \hat{X}|^{2}]$. $\blacksquare$

## Теорема 2 (О нормальной корреляции) {#theorem-2-2}

Пусть 
\begin{align*}
    \begin{pmatrix}
        X \\
        Y \\
    \end{pmatrix}
    \sim N
    \begin{pmatrix}
        \begin{pmatrix}
        m_{X} \\
        m_{Y} \\
        \end{pmatrix},
        \begin{pmatrix}
        K_{X} & K_{XY} \\
        K_{XY}^{T} & K_{Y} \\
        \end{pmatrix}
    \end{pmatrix}
\end{align*}

Тогда 

1. $Law(X \mid Y) = N(\mu(Y), \Delta)$,

где 

$$\mu(Y) = M[X \mid Y] = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

$$\Delta = K_{X} - K_{XY}K_{Y}^{-1}K_{YX}$$

2. $M[|X - \mu(Y)|^{2}] = tr(K_{X} - K_{XY}K_{Y}^{-1}K_{YX})$

### Доказательство {-}

Рассмотрим линейное преобразование $Y$:

$$\mu(Y) = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

В силу [Леммы 1.3](#lemma-1-3)

$X - \mu(Y) = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} \sim N(\mu, K)$

$\mu = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    m_{X} \\
    m_{Y} \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} = 0$

$K = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    K_{X} & K_{XY} \\
    K_{XY}^{T} & K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\ 
    -(K_{XY}K_{Y}^{-1})^{T} \\
\end{pmatrix} =$

$= \begin{pmatrix}
    K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{T} & K_{XY} - K_{XY}K_{Y}^{-1}K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\
    K_{Y}^{-1}K_{XY}^{T} \\
\end{pmatrix} =$

$= K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{-1} = \Delta$

$\mathrm{cov}(X - \mu(Y), Y) = \mathrm{cov}(X, Y) - \mathrm{cov}(\mu(Y), Y) = \mathrm{cov}(X, Y) - \mathrm{cov}(K_{XY}K_{Y}^{-1}Y + m_{X} - K_{XY}K_{Y}^{-1}m_{Y}, Y) =
\mathrm{cov}(X, Y) - K_{XY}K_{Y}^{-1}\mathrm{cov}(Y, Y) = K_{XY} - K_{XY}K_{Y}^{-2}K_{Y} = 0$

т.е. $X - \mu(Y)$ и $Y$ **некорреливаны**.

Тогда в силу [Леммы 1.5, п.2](#lemma-1-5) $X - \mu(Y)$ и $Y$ **независимы**. Построим характеристическую функцию условного распределения $X$
относительно $Y$:

$\psi_{X \mid Y}(\lambda \mid Y) = \int\limits_{\R^{n}} e^{i\lambda^{T} X} f_{X \mid Y}(x \mid Y)dx = M[e^{i\lambda^{T}X} \mid Y] =
M[e^{i\lambda^{T}(X - \mu(Y))} e^{i\lambda^{T} \mu(Y)} \mid Y] \overset{*}{=}$

в силу [Леммы 1.2](#lemma-1-2) и *независимости* $X - \mu(Y)$ и $Y$

$\overset{*}{=} M[e^{i\lambda^{T}(X - \mu(Y))} \mid Y] \cdot M[e^{i\lambda^{T}\mu(Y)} \mid Y] = M[e^{i\lambda^{T}(X - \mu(Y))}]
e^{i\lambda^{T} \mu(Y)} = \psi_{X - \mu(Y)}(\lambda) e^{i\lambda^{T} \mu(Y)} = \exp\{- \frac{1}{2} \lambda^{T} \Delta \lambda \}
\cdot \exp\{i\lambda^{T} \mu(Y) - \frac{1}{2} \lambda^{T} \Delta \lambda\}$

т.е. Условное распределение ***нормальное***:

$$X(Y \sim N(\mu(Y), \Delta)$$

Вычислим с.к. ошибку:

$M[|X - \mu(Y)|^{2}] = M[\Delta X_{1}^{2} + \Delta X_{2}^{2} + \dots + \Delta X_{n}^{2}] = \sum\limits_{k = 1}^{n}M[\Delta X_{k}^{2}] = 
\sum\limits_{k = 1}^{n} D[\Delta X_{k}] = \sum\limits_{k = 1}^{n} \Delta_{kk} = tr\Delta$. $\blacksquare$

## Замечание {-}

1. Из [Теоремы о нормальной корреляции](#theorem-2-2) следует, что в *гауссовском* случае с.к.-оптимальная оценка является ***линейной***.

2. Если $X$ и $Y$ --- *независимы*, то с.к.-оптимальная оценка --- $m_{X}$.

3. С.к.-оптимальная оценка ***несмещенная***, т.к. $M[X - \mu(Y)] = 0$. 

\pagebreak

# Виды сходимости последовательностей случайных величин

## Определение 1

Говорят, что $\{X_{n}\}_{n = 1}^{\infty}$ образует ***последовательность случайных величин***, если $\forall N \in \N$ $X_{n}$
определены на одном вероятностном пространстве.

## Определение 2

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится по вероятности*** к с.в. $X$, если $\forall \varepsilon > 0$:

$$\lim\limits_{n \to \infty} P(|X_{n} - X| \leqslant \varepsilon) = 1$$

ИЛИ

$$\lim\limits_{n \to \infty} P(|X_{n} - X| > \varepsilon) = 0$$

## Определение 3

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится почти наверное*** к с.в. $X$, если

$$P(\{\omega :X_{n}(\omega) \cancel{\overset{n \to \infty}{\longrightarrow}} X(\omega)\}) = 0$$

ИЛИ

$$P(\{\omega :X_{n}(\omega) \overset{n \to \infty}{\longrightarrow} X(\omega)\}) = 1$$

## Определение 4

Говорят, что последовательность с.в. $\{X\}_{n \in \N}$ ***сходится в среднем квадратическом*** к с.в. $X$, если

$$M[|X_{n} - X|^{2}] \overset{n \to \infty}{\longrightarrow} 0$$

## Пример 1

Пусть $\{X_{n}\}_{n \in \N}$ --- случайная последовательность.

$$X \sim 
\begin{pmatrix}
    0 & n \\
    1 - \frac{1}{n^{2}} & \frac{1}{n^{2}} \\
\end{pmatrix}
$$

$P(\{ \omega : \lim\limits_{n \to \infty} X_{n}(\omega) \neq 0 \}) = P(\{ \omega : \forall N \in \N, 
\exists n \geqslant N: X_{n}(\omega)
= n \}) = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) =n \}) \overset{*}{=}$

1. $\sum\limits_{n = N + 1}^{\infty}\{\omega : X_{n}(\omega) = n\} \subset \sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) = n\}$

2. $P(\sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) = n\}) \leqslant \sum\limits_{n = N}^{\infty}
P(\{\omega : X_{n}(\omega) = n \}) = \sum\limits_{n = N}^{\infty} \frac{1}{n^2}
\overset{n \to \infty}{\longrightarrow} 0$, 

т.к.  $\sum\limits_{n = 1}^{\infty} \frac{1}{n^2} < \infty$

Тогда в силу *аксиомы непрерывности* $\overset{*}{=} 0$, т.е.
$X_{n} \overset{\text{п.н.}}{\longrightarrow} 0$

## Пример 2

Рассмотрим ту же последовательность. Выберем $\varepsilon > 0$

$$P(|X_{n} - 0| \leqslant \varepsilon) =
\begin{cases}
    1, & \varepsilon \geqslant n \\
    1 - \frac{1}{n^2}, & \varepsilon \in (0; n) \\
\end{cases} \overset{n \to \infty}{\longrightarrow} 1 $$

Тогда $X_{n} \overset{P}{\longrightarrow} 0$

## Пример 3

Рассмотрим ту же последовательность:

$M[|X_{n} - 0|^{2}] = M[X_{n}^2] = 0^{2}(1 - \frac{1}{n^2}) + n^{2} \frac{1}{n^2} = 1 
\cancel{\overset{n \to \infty}{\longrightarrow}} 0$

Тогда $X_{n} \cancel{\overset{\text{с.к.}}{\longrightarrow}} 0$

## Пример 4

Пусть $f_{nk}:[0;1] \longrightarrow \{0; 1\}, n \in \N, k = \overline{1, n}$,

$$f_{n, k} =
\begin{cases}
    0, & t \notin [\frac{k - 1}{n}; \frac{k}{n}], \\
    1, & t \in [\frac{k - 1}{n}; \frac{k}{n}]. \\
\end{cases}$$

Пусть $X \sim R(0; 1)$. Рассмотрим последовательность с.в. $X_{nk} = f_{nk}(X)$. $\forall \omega \in
\Omega X(\omega) \in [0;1]$. Тогда $\forall n \in \N, \exists k = \overline{1, n}$ такое, что
$X(\omega) \in [\frac{k - 1}{n}; \frac{k}{n}]$. Т.е. если $\varepsilon = \frac{1}{2}$, то $\forall n
\in \N$ найдется $k = \overline{1, n}$ такой, что

$$|f_{nk}(X(\omega)) - 0| > \varepsilon$$

Тогда $X_{nk}(\omega) \cancel{\overset{n \to \infty}{\longrightarrow}} 0$, т.е.

$$ \{\omega : \lim\limits_{n,k \to \infty} X_{nk}(\omega) = 0\} = \varnothing$$

$$X_{nk} \cancel{\overset{\text{п. н.}}{\longrightarrow}} 0$$

При этом $\forall \varepsilon > 0$

$$R(|X_{nk} - 0| > \varepsilon) =
\begin{cases}
    0, & \varepsilon \geqslant 1, \\
    P(X \in [\frac{k - 1}{n}, & \frac{k}{n}]), \varepsilon \in (0; 1) \\
\end{cases} =
\begin{cases}
    0, & \varepsilon \geqslant 1, \\
    \frac{1}{n}, & \varepsilon \in (0; 1) \\
\end{cases} \overset{n \to \infty}{\longrightarrow} 0$$

$$X_{nk} \overset{P}{\longrightarrow} 0$$

$M[|X_{nk} - 0|^{2}] = M[X_{nk}] = M[f_{nk}(X)] = \int\limits_{0}^{1}f_{nk}(x) \underbrace{f_{X}(x)}_{1}dx = \int\limits_{0}^
{1} f_{nk}(x)dx = \int\limits_{\frac{k -1}{n}}^{\frac{k}{n}} 1 dx = \frac{1}{n} \overset{n \to \infty}
{\longrightarrow} 0$

$$X_{nk} \overset{\text{с.к}}{\longrightarrow} 0$$

## Пример 5

Рассмотрим последовательность с.в. $Y_{n_{1}k} = nK_{n_{1}k}$ Тогда $Y_{n_{1}k} \overset{\text{п.н.}}
{\longrightarrow} 0$. $\forall \varepsilon > 0$.

$P(|Y_{n_{1}k} - 0| > \varepsilon) =
\begin{cases}
    0, & \varepsilon \geqslant n, \\
    P(X \in [\frac{k -1}{n}; \frac{k}{n}]), & \varepsilon \in (0; n) \\
\end{cases} =
\begin{cases}
    0, & \varepsilon \geqslant n, \\
    \frac{1}{n}, & \varepsilon \in (0; n) \\
\end{cases}
\overset{n \to \infty}{\longrightarrow} 0$

$$Y_{n_{1}k} \overset{P}{\longrightarrow} 0$$

$M[|Y_{n_{1}k} - 0|^{2}] = M[n^{2} X_{nk}^{2}] = n^{2}M[X_{nk}] = n \overset{P}{\longrightarrow} \infty$

$$Y_{n_{1}k} \cancel{\overset{\text{с.к}}{\longrightarrow}} 0$$

## Замечание {-}

Согласно определению для исследования на сходимость нужно знать совместное распределение с.в. $X_{n}$ и $X$,
 а для случая *сходимости почти наверное* совместное распределение всей последовательности
$\{X_{n}\}_{n \in \N}$ и $X$. Поэтому исследование на сходимость иначе, чем к детерминированной константе, довольно проблематично.

## Лемма 1

Пусть $X_{n} \overset{\text{п.н.}}{\longrightarrow} X$. Тогда $X_{n} \overset{P}{\longrightarrow} X$.

### Доказательство {-}

$0 = P(\{\omega : \lim\limits_{n \to \infty} X_{n}(\omega) \neq X(\omega)\}) = P(\{\omega : \exists 
\varepsilon > 0 \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega)| > \varepsilon\}) = 
P(\sum\limits_{\varepsilon > 0} \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty}
\{ \omega : |X_{n}(\omega) - X(\omega)| > \varepsilon \}) \geqslant
P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| >
\varepsilon '\})$, $\forall \varepsilon ' > 0$

Тогда $0 = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - 
X(\omega)| > \varepsilon\})$,

$$\sum\limits_{n = N + 1}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| > \varepsilon\} \subset
\sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| > \varepsilon\}$$

В силу *аксиомы непрерывности*:

$0 = \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| >
\varepsilon\}) \geqslant \lim\limits_{N \to \infty} P(\{\omega : |X_{N}(\omega) - X(\omega)| > 
\varepsilon\})$

$$X_{N} \overset{P}{\longrightarrow} X$$ 
\hfill $\blacksquare$

## Лемма 2 (Неравенство Маркова)

Пусть $P(X \geqslant 0) = 1$, $M[X] < \infty$. Тогда $\forall \varepsilon > 0$

$$P(X > \varepsilon) \leqslant \frac{M[X]}{\varepsilon}$$

### Доказательство {-}

$M[X] = \int\limits_{0}^{+ \infty} xdF_{X}(x) \geqslant \int\limits_{\varepsilon}^{+ \infty} xdF_{X}(x)
\geqslant \varepsilon \int\limits_{\varepsilon}^{+ \infty} dF_{X}(x) = \varepsilon P(X > \varepsilon)$.
$\blacksquare$

## Следствие 1

Пусть $M[X^{k}] < \infty$. Тогда $\forall \varepsilon > 0$:

$$P(|X| > \varepsilon) \leqslant \frac{M[|X|^{k}|]}{\varepsilon^{k}}$$

### Доказательство {-}

$P(|X| > \varepsilon) = P(|X|^{k} > \varepsilon^{k}) \leqslant \frac{M[|X|^{k}]}{\varepsilon^{k}}$.
$\blacksquare$

## Следствие 2 (Неравенство Чебышёва)

Пусть $M[X^{2}] < \infty$. Тогда

$$P(|X - M[X]| > \varepsilon) \leqslant \frac{D[X]}{\varepsilon^{2}}$$

### Доказательство {-}

$P(|X - M[X]| > \varepsilon) \leqslant \frac{M[|X - M[X]|^{2}]}{\varepsilon^{2}} = \frac{D[X]}
{\varepsilon^{2}}$. $\blacksquare$

## Лемма 3 {#lemma-3-3}

Пусть $X_{n} \overset{\text{с.к.}}{\longrightarrow} X$. Тогда $X_{n} \overset{P}{\longrightarrow} X$.

### Доказательство {-}

$P(|X_{n} - X| > \varepsilon) \leqslant \frac{M[|X_{n} - X|^{2}]}{\varepsilon^{2}} 
\overset{n \to \infty}{\longrightarrow} 0$. $\blacksquare$

## Теорема 1 (Бореля --- Кантелли) {#theorem-3-1}

Пусть $A_{1}, \dots, A_{n} \subset \Omega, B = \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty}
A_{n}$. Тогда

1. Если $\sum\limits_{n = 1}^{\infty} P(A_{n}) < \infty$, то $P(B) = 0$;

2. Если $A_{1}, \dots, A_{n}$ **независимы в совокупности** и $\sum\limits_{n = 1}^{\infty} P(A_{n}) = \infty$
, то $P(B) = 1$.

### Доказательство {-}

1. $P(B) = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} A_{n}) \overset{*}{=}$

т.к. $\sum\limits_{n = N + 1}^{\infty} A_{n} \subset \sum\limits_{n = N}^{\infty} A_{n}$, то *по
аксиоме непрерывности* $\overset{*}{=} \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} A_{n})
\leqslant \lim\limits_{N \to \infty} \sum\limits_{n = N}^{\infty} P(A_{n})
\overset{N \to \infty}{\longrightarrow} 0$, т.к. $\sum\limits_{n = 1}^{\infty} P(A_{n}) < \infty$

2. $P(B) = \dots = \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} A_{n}) = 
\lim\limits_{N \to \infty} (1 - P(\prod\limits_{n = N}^{\infty} \overline{A_{n}})) = 
1 - \lim\limits_{N \to \infty} P(\prod\limits_{M = N}^{\infty} \prod\limits_{n = N}^{M} \overline{A_{n}})
\overset{*}{=}$

т.к. $\prod\limits_{n = N}^{M + 1} \overline{A_{n}} \subset \prod\limits_{n = N}^{M} \overline{A_{n}}$, то
по *аксиоме непрерывности*

$\overset{*}{=} 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} P(\prod\limits_{n = N}^{M}
\overline{A_{n}}) = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} \prod\limits_{n = N}^{M}
(1 - P(A_{n})) = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} \prod\limits_{n = N}^{M} 
e^{\ln(1 - P(A_{n}))} = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty}
e^{\sum\limits{n = N}^{M} \ln(1 - P(A_{n}))} \overset{*}{\geqslant}$

т.к. $\ln(1 - t) = -t - \frac{t^{2}}{2} - \frac{t^{3}}{3} - \frac{t^{4}}{4} - \dots < -t$, то

$\overset{*}{\geqslant} 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty}
e^{- \sum\limits_{n = N}^{M} P(A_{n})} = 1 - \lim\limits_{n \to \infty} 0 = 1$. $\blacksquare$

<!-- тут в конспекте опечатка было 'Лемма 3' -->
## Лемма 4 {#lemma-3-4}

Пусть $X_{n} \overset{P}{\longrightarrow} X$,

$$\sum\limits_{n = 1}^{\infty} P(|X_{n} - X| > \varepsilon) < \infty$$ 
Тогда $X_{n} \overset{\text{п.н.}}{\longrightarrow} X$

### Доказательство {-}

В силу [Теоремы 3.1](#theorem-3-1) $\forall \varepsilon > 0$

$$P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n} - X| > \varepsilon \}) = 0$$

Тогда

$P(\{\omega : \lim\limits_{n \to \infty} X_{n} X_{n}(\omega) \neq X(\omega)\}) =
P(\{\omega : \exists \varepsilon > 0, \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega)|> 
\varepsilon \}) = 
P(\{\omega : \exists M \in \N, \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega)|>
\frac{1}{M}\}) = P(\sum\limits_{M = 1}^{\infty} \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} 
\{ \omega : |X_{n} - X| > \frac{1}{M} \}) \leqslant 
\sum\limits_{M = 1}^{\infty} P(\prod\limits_{N = 1}^{\infty} \sum\limits{n = N}^{\infty} 
\{ \omega : |X_{n} - X| > \frac{1}{M} \}) = 0$. $\blacksquare$

## Замечание {-}

1.

$X_{n} \overset{\text{с.к.}}{\longrightarrow} X \implies X_{n} \overset{P}{\longrightarrow} X$

ИЛИ

$X_{n} \overset{\text{п.н.}}{\longrightarrow} X \implies X_{n} \overset{P}{\longrightarrow} X$

2. В силу *теоремы Рисса* (функциональный анализ) если $X_{n} \overset{P}{\longrightarrow} X$,
то существует *подпоследовательность* $\{X_{n_{k}}\}_{k \in \N} : X_{n_{k}} 
\underset{k \to \infty}{\overset{\text{п.н.}}{\longrightarrow}} X$

3. В силу *теоремы о мажорирующей сходимости*, если $X_{n} \overset{P}{\longrightarrow} X$ и $\exists Y$ ---
с.в.: $|X_{n}| \leqslant Y, M[Y^{2}] < \infty$, то $X_{n} \overset{\text{с.к.}}{\longrightarrow} X$

4. Также из функционального анализа известно, что операция предела (по мере, почти наверное, в средне
квадратическом) замкнута относительно линейных операций и непрерывных преобразований.

\pagebreak

# Закон больших чисел

## Определение 1

***Выборкой*** объема $n$ будем называть с.в. $Z_{n} = (X_{1}, \dots, X_{n})^{T}$, где 
$X_{1}, \dots, X_{n}$ --- *независимые* с.в.

Через $F_{k}(x)$ обозначим *функцию распределения* k-го элемента выборки.

Если $F_{k} = F_{1}, k = \overline{2, n}$, то выборка называется ***однородной***.

## Определение 2

***Выборочным средним*** $\overline{X_{n}}$ выборки $Z_{n}$ называется $\overline{X_{n}} = \frac{1}{n}
\sum\limits_{k = 1}^{n} X_{k}$

## Теорема 1 (Закон Больших Чисел Чебышёва)

Пусть $Z_{n}$ --- однородная выборка, $M[X_{k}^{2}] < \infty$.

Тогда $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}, \overline{X_{n}} 
\overset{P}{\longrightarrow} m_{X}$

### Доказательство {-}

$M[|\overline{X_{n}} - m_{X}|^{2}] = M[|\overline{X_{n}} - M[\overline{X_{n}}]|^{2}] = 
D[\overline{X_{n}}] = \frac{1}{n^{2}} D[\sum\limits_{k = 1}^{n} X_{k}] = 
\frac{n D[X_{1}]}{n^{2}} \overset{n \to \infty}{\longrightarrow} 0$

Т.е. по определению $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}$.

С учетом [Леммы 3.3](#lemma-3-3) $\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$. $\blacksquare$

## Теорема 2 (Закон Больших Чисел Колмогорова) {#theorem-4-2}

Пусть $Z_{n}$ --- *однородная* выборка, $M[X_{k}] = m_{X} < \infty$.

Тогда $\overline{X_{n}} \overset{\text{п.н.}}{\longrightarrow} m_{X}$

## Замечание {-}
Т.о. для *однородной* выборки $\overline{X_{n}}$ сходится **почти наверное** и **по вероятности** к $m_{X}$,
\underline{если оно существует}, и **в среднем квадратичном**, \underline{если существует дисперсия}.

## Теорема 3 {#theorem-4-3}

Пусть $Z_{n}$ --- *неоднородная* выборка, $M[X_{k}] = m_{X} < \infty, D[X_{k}] = D_{k} \leqslant D_{max} <
\infty$, где $k \in \N$

Тогда $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}, 
\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$.

### Доказательство {-}

$M[|\overline{X_{n}} - m_{X}|^{2}] = M[|\overline{X_{n}} - M[\overline{X_{n}}]|^{2}] = D[X_{n}] =
\frac{1}{n^{2}} \sum\limits_{k = 1}^{n} D_{k} \leqslant \frac{n D_{max}}{n^{2}} = 
\frac{D_{max}}{n} \overset{n \to \infty}{\longrightarrow} 0$

Т.о. $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}$. Тогда в силу [Леммы 3.3](#lemma-3-3)
$\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$. $\blacksquare$

## Следствие 1

Пусть $Z_{n}$ --- *неоднородная* выборка, $D[X_{k}] = D_{k} \leqslant D_{max} < \infty, k \in \N$.

Тогда $\overline{X_{n}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} \overset{\text{с.к.}}{\longrightarrow}
0, \overline{X_{n}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} \overset{P}{\longrightarrow} 0$.

### Доказательство {-}

$\overline{X_{n}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} = 
\frac{1}{n} \sum\limits_{k = 1}^{n} (X_{k} - m_{X_{k}}) = \frac{1}{n} \sum\limits_{k = 1}^{n} Y_{k}$,

где $M[Y_{k}] = 0, D[Y_{k}] = D[X_{k}] \leqslant D_{max} < \infty$

Тогда $\overline{Y_{k}}$ удовлетворяет условиям [Теоремы 4.3](#theorem-4-3). $\blacksquare$

## Теорема 4 {#theorem-4-4}

Пусть $Z_{n}$ --- *неоднородная* выборка, $M[X_{k}] = m_{X} < \infty, D[X_{k}] = D_{k} < \infty$,

$$\sum\limits_{k = 1}^{n} \frac{D[X_{k}]}{k^{2}} < \infty$$

Тогда

$$\overline{X_{n}} \overset{\text{п.н.}}{\longrightarrow} m_{X}$$

## Замечание {-}

Условие [Теоремы 4](#theorem-4-4) более мягкое, чем условие [Теоремы 3](#theorem-4-3). Пусть 
$D[X_{k}] \leqslant D_{max}, k \in \N$. Тогда 

$$\sum\limits_{k = 1}^{n} \frac{D[X_{k}]}{k^{2}} \leqslant \sum\limits_{k = 1}^{n} \frac{D_{max}}{k^{2}} =
\frac{\pi^{2} D_{max}}{\sigma} < \infty$$

## Следствие 2

Пусть $N(A)$ --- число появления события $A$ в серии из $N$ *независимых* опытов. Тогда

$$\frac{N(A)}{N} \overset{\text{п.н.}}{\longrightarrow} P(A), \frac{N(A)}{N} 
\overset{\text{с.к.}}{\longrightarrow} P(A)$$

### Доказательство {-}

По условию $N(A) \sim Bi(N; P(A))$. Тогда $\exists X_{1}, \dots, X_{n} \sim Be(P(A))$ --- независимые с.в.

При этом $M[X_{1}] = P(A), D[X_{1}] = P(A)(1 - P(A)) \leqslant \frac{1}{4}$.

Тогда в силу [Теоремы 4](#theorem-4-1)

$$\frac{N(A)}{N} = \frac{1}{N} \sum\limits_{k = 1}^{N} X_{k} \overset{\text{с.к.}}{\longrightarrow} P(A)$$

в силу [Теоремы 2](#theorem-4-2)

$$\frac{N(A)}{N} = \frac{1}{N} \sum\limits_{k = 1}^{N} X_{k} \overset{\text{п.н.}}{\longrightarrow} P(A)$$

\hfill $\blacksquare$

\pagebreak 

# Центральная предельная теорема (ЦПТ)

## Замечание {-}

Сходимости ***с.к.***, ***п.н.*** и ***$P$*** в общем случае исследования предполагают либо знание
совместного распределения элементов последовательности, либо наличие точкой функциональной зависимости
от $\omega \in \Omega$.

Как правило, в теории вероятностей это неизвестно, а с.в. описываются при помощи их распределений, а не
как функции. При этом если у двух величин совпадают распределения, то это вовсе не значит, что они равны.

Поэтому довольно важным является вид сходимости *по распределению*, т.е. в смысле "описательного
инструмента" с.в.

## Определение 1

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится по распределению*** к с.в. $X$,
если 

$F_{X_{n}}(x) \overset{n \to \infty}{\longrightarrow} F_{X}(x)$, $\forall x$ --- точки непрерывности
$F_{X}(x)$.

## Лемма 1 {#lemma-5-1}

Пусть $X_{n} \overset{P}{\longrightarrow} X$. Тогда $X_{n} \overset{d}{\longrightarrow} X$

## Лемма 2 {#lemma-5-2}

Пусть $X_{n} \overset{d}{\longrightarrow} X$, $Y_{n} \overset{P}{\longrightarrow} C$

Тогда $X_{n} + Y_{n} \overset{d}{\longrightarrow} C$

### Доказательство {-}

Пусть $C = 0$, $x_{0}$ --- точка **непрерывности** $F_{X}(x)$. Тогда $\forall \varepsilon > 0$

$F_{X_{n} + Y_{n}}(x_{0}) = P(X_{n} + Y_{n} \leqslant x_{0}) = \underbrace{P(\{X_{n} + Y_{n} \leqslant x_{0}\}
\{|Y_{n}| > \varepsilon\})}_{p_{1}} + 
\underbrace{P(\{X_{n} + Y_{n} \leqslant x_{0}\} \{|Y_{n}| \leqslant \varepsilon\})}_{p_{2}}$

$$0 \leqslant p_{1} \leqslant P(|Y_{n}| > \varepsilon) \overset{n \to \infty}{\longrightarrow} 0$$

$p_{2} = P(\{X_{n} + Y_{n} \leqslant x_{0}\} \{- \varepsilon \leqslant Y_{n} \leqslant \varepsilon\})
\overset{*}{\leqslant}$

$$\begin{cases}
    X_{n} + Y_{n} \leqslant x_{0} \\
    - \varepsilon \leqslant Y_{n} \leqslant \varepsilon \\
\end{cases} \implies
\begin{cases}
    X_{n} + Y_{n} \leqslant x_{0} \\
    - \varepsilon \leqslant - Y_{n} \leqslant \varepsilon \\
\end{cases} \implies X_{n} \leqslant x_{0} + \varepsilon$$

$\overset{*}{\leqslant} P(X_{n} \leqslant x_{0} + \varepsilon) = F_{X_{n}} (x_{0} + \varepsilon)$

$p_{2} \geqslant P(\{\varepsilon + X_{n} \leqslant x_{0}\} \{- \varepsilon \leqslant Y_{n} \leqslant 
\varepsilon \}) \overset{*}{\geqslant}$

$$P(AB) = P(A) - P(A \backslash B) \geqslant P(A) - P(\overline{B})$$

$\overset{*}{\geqslant} P(\varepsilon + X_{n} \leqslant x_{0}) - P(|Y_{n} > \varepsilon) = 
F_{X_{n}}(x_{0} - \varepsilon) - P(|Y_{n}| > \varepsilon)$

Т.о. $F_{X_{n}}(x_{0} - \varepsilon) - P(|Y_{n}| > \varepsilon) \leqslant 
F_{X_{n} + Y_{n}} (x_{0}) \leqslant F_{X_{n}} (x_{0} + \varepsilon) + p_{1}$

Выберем $\varepsilon > 0$ так, чтобы $(x_{0} - \varepsilon; x_{0} + \varepsilon)$ было областью
непрерывности  $F_{X}(x)$

Тогда $\lim\limits_{n \to \infty} F_{X_{n}} (x_{0} \pm \varepsilon) = F_{X}(x_{0} \pm \varepsilon)$

$$F_{X_{n}} (x_{0} - \varepsilon) \leqslant \lim\limits_{\overline{n \to \infty}} F_{X_{n} + Y_{n}}
(x_{0}) \leqslant \overline{\lim\limits_{n \to \infty}} F_{X_{n} + Y_{n}} (x_{0}) \leqslant
F_{X}(x_{0} + \varepsilon)$$

Возьмем предел по $\varepsilon \to 0$. В силу непрерывности $F_{X}(x)$ в $x_{0}$ 
$\lim\limits_{\varepsilon \to 0} F_{X}(x_{0} \pm \varepsilon) = F_{X}(x_{0})$

Откуда $F_{X}(x_{0}) = \lim\limits_{\varepsilon \to 0} F_{X_{n} + Y_{n}}(x_{0})$, т.е.

$$X_{n} + Y_{n} \overset{d}{\longrightarrow} X$$

Пусть $C \neq 0$. Тогда $Y_{n} - C = \tilde{Y} \overset{P}{\longrightarrow} 0$,

$X_{n} + C = \tilde{X_{n}} \overset{d}{\longrightarrow} X + C = \tilde{X}$

Получаем $X_{n} + Y_{n} = \tilde{X_{n}} + \tilde{Y_{n}} \overset{d}{\longrightarrow} \tilde{X} = X + C$.
$\blacksquare$

## Доказательство леммы 5.1

$X_{n} = (X_{n} - X) + X$, где $X \overset{d}{\longrightarrow} X, X_{n} - X \overset{P}{\longrightarrow}
0$. В силу [Леммы 5.2](#lemma-5-2) $X_{n} = (X_{n} - X) + X \overset{d}{\longrightarrow} X + 0 = X$.
$\blacksquare$

## Замечание {-}

Из *теории преобразования Фурье* следует, что $X_{n} \overset{d}{\longrightarrow} X$ тогда и только тогда, 
когда $\psi_{X_{n}}(\lambda) \overset{n \to \infty}{\longrightarrow} \psi_{X}(\lambda), \lambda \in \R$

## Определение 2

Последовательность с.в. $\{X_{n}\}_{n \in \N}$ называется ***асимптотически нормальной***, если

$X_{n} \overset{d}{\longrightarrow} X$, где $X \sim N(m; \sigma^{2})$

## Теорема 1 (Центральная предельная) {#theorem-5-1}

Пусть $\{X_{n}\}_{n \in \N}$ --- последовательность *независимых* и *одинаково распределенных* с.в.,
причем 

$$M[X_{1}] = m_{X}, D[X_{1}] = \sigma_{X}^{2}$$

Тогда 

$$S_{n} = \frac{\sum\limits_{k = 1}^{n} X_{n} - nm_{X}}{\sigma_{X} \sqrt{n}} \overset{d}{\longrightarrow}
N(0; 1)$$

### Доказательство {-}

Обозначим через $Y_{k} = \frac{X_{k} - m_{X}}{\sigma_{X} \sqrt{n}}$

Тогда $\sum\limits_{k = 1}^{n} Y_{k}$, где $Y_{1}, \dots, Y_{n}$ --- *независимые* с.в.

В силу [Леммы 1.5](#lemma-1-5)

$$\psi_{S_{n}} (\lambda) = \psi_{Y} (\lambda, \lambda, \dots, \lambda) =
\prod\limits_{k = 1}^{n} \psi_{Y_{k}}(\lambda) = \psi_{Y_{1}}^{n}(\lambda) = \psi^{n}
(\frac{\lambda}{\sigma_{X} \sqrt{n}})$$ где
$\psi(\lambda)$ --- **характеристическая функция** $X_{k} - m_{X}$.

$\psi(0) = M[e^{i0(X_{k} - m_{X})}] = M[1] = 1$

$\psi'(0) = M[i(X_{k} - m_{X})e^{i0(X_{k} - m_{X})}] = M[X_{k} - m_{X}] i = 0$

$\psi''(0) = - M[(X_{k} - m_{X})^{2} e^{i0(X_{k} - m_{X})}] = - M[(X_{k} - m_{X})^{2}] = - \sigma_{X}^{2}$

Тогда согласно *формуле Тейлора*

$\psi(\lambda) = \psi(0) + \psi'(0) \lambda + \psi''(0) \frac{\lambda^{2}}{2} + o(\lambda^{2}) = 
1 - \frac{\sigma_{X}^{2}}{2} \lambda^{2} + o(\lambda^{2})$

Рассмотрим $\ln\psi_{S_{n}}(\lambda) = n \ln\psi(\frac{\lambda}{\sigma_{X} \sqrt{n}}) =
n \ln(1 - \frac{\lambda^{2}}{2n} + o(\frac{\lambda^{2}}{\sigma_{X}^{2}n})) = 
n(- \frac{\lambda^{2}}{2n} + o(\frac{\lambda^{2}}{\sigma_{X}^{2}n}) + o(-
\frac{\lambda^{2}}{2n} + o(\frac{\lambda^{2}}{\sigma_{X}^{2}n}))) = 
n(- \frac{\lambda^{2}}{2n} + o(\frac{\lambda^{2}}{2n})) = - \frac{\lambda^{2}}{2n} +
\frac{o(\frac{\lambda^{2}}{\sigma_{X}^{2}n})}{\frac{\lambda^{2}}{\sigma_{X}^{2}n}} \cdot
\frac{\lambda^{2}}{2} \overset{n \to \infty}{\longrightarrow} - \frac{\lambda^{2}}{2}$,

$$\psi_{S_{n}}(\lambda) \overset{n \to \infty}{\longrightarrow} e^{- \frac{\lambda^{2}}{2}}$$

где $\psi_{Y}(\lambda) = e^{- \frac{\lambda^{2}}{2}}$ по определению является *характеристической функцией 
$Y \sim N(0; 1)$

Тогда $S_{n} \overset{d}{\longrightarrow} Y \sim N(0; 1)$. $\blacksquare$

## Следствие 1 (Теорема Муавра - Лапласа) {#cons-5-1}

Пусть $X_{n} \sim Bi(n; p)$

Тогда 

$$\frac{X_{n} - np}{\sqrt{np(1 - p)}} \overset{d}{\longrightarrow} N(0; 1)$$

### Доказательство {-}

Т.к. $X_{n} \sim Bi(n; p)$, то существуют **независимые** $\tilde{X_{1}}, \dots, \tilde{X_{n}} \sim
Be(p)$ такие, что

$$X_{n} = \sum\limits_{k = 1}^{n} \tilde{X_{k}}, M[\tilde{X_{k}}] = p = m_{X}, D[\tilde{X_{k}}] =
p(1 - p) = \sigma_{X}^{2}$$

Тогда  в силу [Теоремы 1](#theorem-5-1):

$\frac{\sum\limits_{k = 1}^{n} \tilde{X_{k}} - nm_{X}}{\sigma_{X}\sqrt{n}} \overset{d}{\longrightarrow}
N(0; 1)$. $\blacksquare$

## Пример 1

Вычислить вероятность того, что при $n = 1000$  подбрасываниях монета упадет *"орлом"* от $400$ до $600$
раз.

Пусть $X$ --- число выпавших *"орлов"*. Тогда $X \sim Bi(1000; \frac{1}{2})$. По *формуле Бернулли*

$$P(X \in [400; 600]) = \sum\limits_{k = 400}^{600} C_{1000}^{k} \frac{1}{2^{1000}}$$

Оценим данную величину с помощью *ЦПТ*.

В силу [Теоремы 1](#theorem-5-1) и [Следствия](#cons-5-1)

<!-- тут может быть ошибка: сходимость не по 'n -> \infty', а по 'd' -->
$$\frac{X - n \frac{1}{2}}{\sqrt{n \frac{1}{2}(1 - \frac{1}{2})}} \overset{d}{\longrightarrow} 
N(0; 1)$$

Тогда 

$P(400 \leqslant X \leqslant 600) = P(\frac{400 - 500}{\sqrt{250}} \leqslant 
\frac{X - nm_{X}}{\sigma_{X}\sqrt{n}} \leqslant \frac{600 - 500}{\sqrt{250}}) \approx
\Phi_{0}(\frac{100}{5\sqrt{10}}) - \Phi_{0}(- \frac{100}{5\sqrt{10}}) = 2 \Phi_{0}(2\sqrt{10}) \approx 1$.

## Теорема 2 (Ляпунова) {#theorem-5-2}

Пусть $\{X_{n}\}_{n \in \N}$ --- последовательность *независимых* с.в., $M[X_{n}] = m_{X_{n}}, 
D[X_{n}] = \sigma_{X_{n}}^{2}, M[|X_{n} - m_{X_{n}}|^{3}] = C_{n}^{3} < \infty$

При этом $\frac{(\sum\limits_{k = 1}^{n} C_{k}^{3})^{\frac{1}{3}}}{
(\sum\limits_{k = 1}^{n} \sigma_{X_{k}}^{2})^{\frac{1}{2}}} \overset{n \to \infty}{\longrightarrow} 0$ 
***(Условие Ляпунова)***

Тогда $\frac{(\sum\limits_{k = 1}^{n} X_{k} - M[\sum\limits_{k = 1}^{n} X_{k}]}{
\sqrt{D[\sum\limits_{k = 1}^{n} X_{k}]}} \underset{n \to \infty}{\overset{d}{\longrightarrow}}
\sim N(0; 1)$

## Замечание {-}

Для *аппроксимации* точности использования ЦПТ используется **неравенство Берри-Эссеена**

## Теорема 3 (Неравенство Берри-Эссеена) {#theorem-5-3}

Пусть $\{X_{n}\}_{n \in \N}$ --- последовательность *независимых* и *одинаково распределенных* с.в.

$$M[X_{n}] = m_{X}, D[X_{n}] = \sigma_{X}^{2}, M[|X_{n} - m_{X}|^{3}] = \rho < \infty$$

Тогда $\forall x \in \R$ и $n \in \N$

$$|P(\frac{\sum\limits_{k = 1}^{n} X_{k} - n m_{X}}{\sigma_{X}\sqrt{n}}) -
(\frac{1}{2} + \Phi_{0}(x))| \leqslant \frac{C_{0}\rho}{\sigma_{X}^{3}\sqrt{n}}$$

## Замечание {-}

Точное значение константы $C$ **неизвестно**.

По текущим данным *(2010 г.)* $C_{0} \leqslant 0.4784$

## Пример 

Оценим точность решения в предыдущем примере: 
$\sigma_{X}^{2} = \frac{1}{4}, n = 1000, m_{X} = \frac{1}{2}, X_{n} \sim 
\begin{pmatrix}
    0 & 1 \\
    \frac{1}{2} & \frac{1}{2} \\
\end{pmatrix}$

$\rho = M[|X_{n} - m_{X}|^{3}] = |0 - \frac{1}{2}|^{3} \cdot \frac{1}{2} + |1 - \frac{1}{2}|^{3} \cdot
\frac{1}{2} = \frac{1}{8}$

Тогда погрешность составит для двухстороннего нер-ва:

$\frac{2C_{0}\rho}{\sigma_{X}^{3}\sqrt{n}} = \frac{2 \cdot 0.4784 \cdot \frac{1}{8}}
{\frac{1}{64} \cdot \sqrt{1000}} \approx 0.03$

\pagebreak

# Выборка и ее характеристики 

## Определение 1 

***Выборкой*** называется $Z_{n} = (X_{1}, \dots, X_{n})^{T}$ *независимый вектор*
с.в. Если все $X_{1}, \dots, X_{n}$ --- *одинаково распределены*, а $F(x)$ --- *функция
распределения*, то говорят, что $Z_{n}$ --- ***однородная*** выборка, порожденная распределением $F(x)$

## Определение 2

***Реализацией выборки*** $Z_{n} \in \R^{n}$ называется *неслучайный* вектор $z_{n} = Z_{n}(\omega)$,
состоящий из реализаций элементов выборки $X_{k}, k = \overline{1, n}$.

## Определение 3

Множество $S$ всех возможных реализаций выборки $Z_{n}$ называют ***выборочным пространством***

## Замечание {-}

Обычно распределение, порождающее выборку, известно неточно.

$$F_{X} = F_{X}(x; \theta)$$

Задача состоит в построении оценки $\theta$ по элементам выборки.

## Определение 4

С.в. $\phi(Z_{n})$, где $\phi : S \to \R$ --- *измерима*, называется ***статистикой***.

## Определение 5

***k-ой порядковой статистикой*** называется k-е по величине значение элемента выборки 
$Z_{n} = (X_{1}, \dots, X_{n})^{T}$ и обозначается $X^{(k)}$

## Замечание {-}

$X^{(k)}$ является функцией от всей выборки, т.к. при различных $\omega \in \Omega$ $X^{(k)}$ будет 
совпадать по значению с разными $X_{i}$.

## Определение 6

Набор порядковых статистик $X^{(1)}, \dots, X^{(n)}$ называется ***вариационным рядом***.

## Определение 7

$X^{(1)} = \min_{k = \overline{1, n}} X_{k}, X^{(n)} = \max_{k = \overline{1, n}} X_{k}$.

## Лемма 1 {#lemma-6-1}

Пусть однородная выборка $Z_{n}$ порождена распределением $F(x)$. Тогда функция распределения
$X^{(k)}$ имеет вид:

$$F_{(k)}(x) = P(X^{(k)} \leqslant x) = \sum\limits_{i = k}^{n} C_{n}^{i} (F(x))^{i}(1 - F(x))^{n - i}$$

### Доказательство {-}

Рассмотрим с.в. $Y$, равную числу элементов выборки, не превосходящих $x$. Тогда $Y \sim Bi(n; F(x))$.

$F_{(k)}(x) = P(X^{(k)} \leqslant x) = P(Y \geqslant k) = 
\sum\limits_{i = k}^{n} C_{n}^{i} (F(x))^{i}(1 - F(x))^{n - i}$. $\blacksquare$

## Следствие 1 {#cons-6-1}

$F_{(1)}(x) = 1 - (1 - F(x))^{n}$,

$F_{(n)}(x) = (F)^{n}$.

## Определение 8

***Выборочной квантилью*** уровня $\alpha \in (0; 1)$ называется *порядковая статистика* 
$X^{([n\alpha] + 1)}$

## Теорема 1 (Мостеллера) {#theorem-6-1}

Пусть $X$ --- *абсолютно непрерывная* с.в., $x_{\alpha}$ --- *точка гладкости* $f_{X}(x), 
f_{X}(x_{\alpha}) > 0$

Тогда $(X^{([n\alpha] + 1)} - x_{\alpha}) \sqrt{\frac{n f_{X}^{2} (x_{\alpha})}{p(1 - p)}}
\overset{d}{\longrightarrow} N(0; 1)$

## Определение 9

***Выборочной функцией распределения*** называется статистика $\hat{F_{n}}(x)$:

$$\hat{F_{n}}(x) =
\begin{cases}
    \frac{1}{n} \max\{k = \overline{1, n} : X^{(k)} \leqslant x\}, & x \geqslant X^{(1)}, \\
    0, & x < X^{(1)} \\
\end{cases}$$

## Замечание {-}

Фактически $\hat{F_{n}}(x)$ --- **частота события** $\{X \leqslant x\}$, которая используется 
для оценки вероятности $F(x) = P(X \leqslant x)$.

```{r, echo=FALSE}
# check 'sampling_function.R' for test
library(ggplot2)
library(latex2exp)
plotter <- function(data)
{
    # first and last 'x' on graph (for pretty look in pdf)
    first <- 0
    last  <- 0
    dash_data <- data.frame() # vertical dashed lines
    dash_data_tmp <- dash_data
    for(i in 1:(length(data[, 1])-1))
    {
        if(data[i, 2] != data[i + 1, 2])
        {
            dash_data_tmp <- dash_data
            dash_data <- rbind(dash_data_tmp, data.frame("x"=c(data[i, 1], data[i, 1]), "f"=c(data[i, 2], data[i+1, 2])))
            last <- i
            first <- last - length(dash_data[, 1]) / 2
        }
    }
    last <- last + 1
    p <- ggplot(data.frame("x"=c(data[first, 1], dash_data[1, 1]), "f"=c(data[first, 2], dash_data[1, 2])), mapping=aes(x=x, y=f)) + geom_vline(xintercept=0) + geom_hline(yintercept=0) + geom_line() + geom_point(data.frame("x"=c(dash_data[1, 1]), "f"=c(dash_data[1, 2])), mapping=aes(x=x, y=f), shape=21, size=3, fill="white")
    j <- 1
    m <- 2
    while(j <= length(dash_data[, 1]))
    {
        # data_tmp <- data.frame("x"=c(dash_data[j, 1], dash_data[j+1, 1]), "f"=c(dash_data[j, 2], dash_data[j+1, 2]))
        data_tmp <- data.frame("x"=c(dash_data[j, 1], dash_data[j+1, 1]), "f"=c(0, dash_data[j+1, 2]))
        p <- p + geom_line(data_tmp, mapping=aes(x=x, y=f), linetype="dashed", colour="grey")
        if(m+1 != length(dash_data[, 1])+1)
        {
            data_tmp <- data.frame("x"=c(dash_data[m, 1], dash_data[m+1, 1]), "f"=c(dash_data[m, 2], dash_data[m+1, 2]))
            p <- p + geom_line(data_tmp, mapping=aes(x=x, y=f)) + geom_point(data.frame("x"=c(dash_data[m+1, 1]), "f"=c(dash_data[m+1, 2])), mapping=aes(x=x, y=f), shape=21, size=3, fill="white")
            m <- m + 2
        }
        j <- j + 2
    }
    p <- p + geom_line(data.frame("x"=c(dash_data[length(dash_data[,1]), 1], data[last, 1]), "f"=c(dash_data[length(dash_data[,1]), 2], data[last, 2])), mapping=aes(x=x, y=f))
    p <- p + scale_x_continuous(breaks=c(dash_data[1, 1], dash_data[3, 1], dash_data[5, 1], dash_data[length(dash_data[, 1])-2, 1], dash_data[length(dash_data[, 1]), 1]), labels=c(TeX("$X^{(1)}$"), TeX("$X^{(2)}$"), TeX("$X^{(3)}$"), TeX("$X^{(n-1)}$"), TeX("$X^{(n)}$")))
    p <- p + scale_y_continuous(breaks=c(dash_data[2, 2], dash_data[4, 2], dash_data[length(dash_data[, 1])-1, 2], 1), labels=c(TeX("$\\frac{1}{n}$"), TeX("$\\frac{2}{n}$"), TeX("$\\frac{n - 1}{n}$"), TeX("$1$"))) 
    p <- p + ggtitle("") + xlab(label="x") + ylab(label=TeX("$\\hat{F}_{n}(x)$")) + theme(axis.text=element_text(colour="red"),
                                                                                          axis.title=element_text(colour="blue"))
    p
}

# if x >= X^{(1)}
flid <- function(n, data, x)
{
    max <- 0
    for(i in 1:length(data))
    {
        if(data[i] <= x && i > max)
        {
            max <- i
        }
    }
    return (1 / n * max)
}

por_stat <- function(n, x, sort_sample)
{
    X <- c()
    Y <- c()
    for(i in 1:length(x))
    {
        if(x[i] >= min(sort_sample))
        {
            X[i] <- x[i]
            Y[i] <- flid(n, sort_sample, x[i])
        } else 
        {
            X[i] <- x[i]
            Y[i] <- 0
        }
    }
    return (data.frame("x"=X, "f"=Y))
}

# 7
set.seed(7)
n <- 21 
x <- seq(-10, 10, 1)
y <- rnorm(n, 0, 1)
data <- por_stat(n, x, sort(y))
plotter(data)
```

<!-- ![](./files/freq-prob.png) -->

## Свойства $\hat{F_{n}}(x)$ 

1. \fbox{$n \cdot \hat{F_{n}}(x) \sim Bi(n; F(x))$}

2. \fbox{$M[\hat{F_{n}}(x)] = F(x)$}

3. \fbox{$\sup\limits_{x \in \R}|\hat{F_{n}}(x) - F(x)| \overset{\text{п.н.}}{\longrightarrow} 0$}
***(Теорема Гливенко - Кантелли)***

4. \fbox{$M[(\hat{F_{n}}(x) - F(x))^{2}] = \frac{F(x)(1 - F(x))} 
{n} \leqslant \frac{1}{4n}$}

5. \fbox{$|\hat{F_{n}}(x) - F(x)| \overset{\text{с.к.}}{\longrightarrow} 0$}

6. \fbox{$\frac{\hat{F_{n}}(x) - F(x)}{\sqrt{F(x)(1 - F(x))}} \sqrt{n}
\overset{d}{\longrightarrow} N(0; 1)$}
(Следует из теоремы [Муавра - Лапласа](#cons-5-1))

## Замечание {-}

$\hat{F_{n}}(x)$ при увеличении $n$ *равномерно приближается* к $F(x)$, при этом точность приближения можно
оценить при помощи свойств **4** и **6**.

\underline{Гистограмма}. На основе реализации вариационного ряда построим разбиение $\R$
$- \infty = t_{0} < t_{1} < t_{2} < \dots < t_{l} < t_{l + 1} = + \infty$,

$t_{1} \leqslant x^{(1)}, t_{l} > x{(n)}$.

Как правило, длина интервалов разбиения выбирается *одинаковой*:

$$h_{k} = t_{k + 1} - t_{k} = \frac{t_{l} - t_{1}}{l - 1}, k = \overline{1, l - 1}$$

Вычислим частоту попадания элементов выборки в k-й интервал:

$\hat{p_{k}} = \frac{n_{k}}{n}$, где $n_{k}$ --- число элементов выборки, попавших в 
$[t_{k};  t_{k + 1}), k = \overline{0, l}$

Заметим, что $\hat{p_{0}} = \hat{p_{l}} = 0$.

## Определение 10

***Гистограммой*** называется функция:

$$\hat{f_{n}}(x) =
\begin{cases}
    0, & x \in (t_{0}; t_{1}) \cup [t_{l}; t_{l + 1}), \\
    \hat{p_{k}}, & x \in (t_{k}; t_{k + 1}), k = \overline{1, l - 1} \\
\end{cases}$$

```{r, echo=FALSE}
# check 'hist.R' for tests and debug
library(ggplot2)
library(latex2exp)
histo <- function(data)
{
    p <- ggplot() + geom_vline(xintercept=0) + geom_hline(yintercept=0)
    for(i in 1:(length(data[, 1])-1))
    {
        p <- p + geom_line(data.frame("x"=c(data[i, 1], data[i+1, 1]), "f"=c(data[i, 2], data[i, 2])), mapping=aes(x=x, y=f)) 
    }
    p <- p + geom_line(data.frame("x"=c(data[1, 1], data[1, 1]), "f"=c(data[1, 2], 0)), mapping=aes(x=x, y=f), linetype="dashed", colour="grey")
    for(i in 2:(length(data[, 1])-1))
    {
        if(data[i, 2] > data[i-1, 2])
        {
            p <- p + geom_line(data.frame("x"=c(data[i, 1], data[i, 1]), "f"=c(data[i, 2], 0)), mapping=aes(x=x, y=f), linetype="dashed", colour="grey")
        } else # if current point value is less then previous
        {
            p <- p + geom_line(data.frame("x"=c(data[i, 1], data[i, 1]), "f"=c(data[i-1, 2], 0)), mapping=aes(x=x, y=f), linetype="dashed", colour="grey")
        }
    }
    p <- p + geom_line(data.frame("x"=c(data[length(data[, 1]), 1], data[length(data[, 1]), 1]), "f"=c(data[length(data[, 1])-1, 2], 0)), mapping=aes(x=x, y=f), linetype="dashed", colour="grey")
    p <- p + scale_x_continuous(breaks=c(data[seq(1, 5, 1), 1]), labels=TeX(sprintf(r'($t_{%d}$)', 1:5)))

    p <- p + ggtitle("") + xlab(label="x") + ylab(label=TeX("$\\hat{f}_{n}(x)$")) + theme(axis.text=element_text(colour="red"),
                                                                                          axis.title=element_text(colour="blue"))
    p <- p + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())
    p
}

data <- data.frame("x"=c(seq(-0.7, 0.5, 0.3)), "f"=0)
set.seed(0)
for(i in 1:length(data[, 1]))
{
    data[i, 2] <- abs(runif(1, min=0, max=1))
}
data[length(data[, 1]), 2] <- 0 # for expl: in 5 points we have 4 bins
histo(data)
```

## Замечание {-}

Если плотность вероятности $f_{X}(x)$ *непрерывна* и *ограничена*, а число разрядов гистограммы
$l_{n}$ *удовлетворяет условию*: $l_{n} \longrightarrow + \infty, \frac{n}{l_{n}} \longrightarrow +
\infty$,

то

$$\hat{f_{n}}(x) \overset{P}{\longrightarrow} f_{X}(x)$$

Т.е. гистограмма является статистической аппроксимацией функции плотности вероятности.

## \underline{Выборочные моменты} {-}

## Определение 1

***Выборочным начальным*** и ***центральным моментами*** называется соответственно статистики:

$\hat{\nu_{r}(n)} = \frac{1}{n} \sum\limits_{k = 1}^{n} X_{k}^{r}$ 

И

$\hat{\mu_{r}}(n) = \frac{1}{n} \sum\limits_{k = 1}^{n} (X_{k} - \hat\nu_{1}(n))^{r}$

## Определение 2

***Выборочным средним*** и ***выборочной дисперсией*** называются соответственно статистики:

$\overline{X_{n}} = \hat{\nu_{1}(n)} = \frac{1}{n} \sum\limits_{k = 1}^{n} X_{k}$ 

И

$\hat{d_{X}}(n) = \hat{S}^{2}(n) = \hat{\mu_{2}}(n) = 
\frac{1}{n} \sum\limits_{k = 1}^{n} (X_{k} - \overline{X})^{2}$

## Определение 3

Пусть $Z_{n} = (X_{1}, \dots, X_{n})^{T}$ и $V_{n} = (Y_{1}, \dots, Y_{n})^{T}$ ---
выборки, порожденные распределениями $F_{X}$ и $F_{Y}$ соответственно. Тогда
***выборочным коэффициентом корреляции*** называется:

$$\hat{r_{XY}} = \frac{\sum\limits_{k = 1}^{n} (X_{k} - \overline{X_{n}})(Y_{k} - \overline{Y_{k}})}
{n\sqrt{\hat{d_{X}} \cdot \hat{d_{Y}}}}$$

## Свойства выборочных моментов 

1. \fbox{$M[\hat{\nu_{r}}(n)] = \nu_{r}, r \in \N$}

**\underline{Доказательство}**

$M[\hat{\nu_{r}}(n)] = \frac{1}{n} \sum\limits_{k = 1}^{n} M[X_{k}^{r}] = \frac{1}{n} n \nu_{r}$.
$\blacksquare$

2. \fbox{Если $M[X^{r}] < \infty$, то $\hat{\nu_{r}}(n) \overset{\text{п.н.}}{\longrightarrow} \nu_{r}$}

**\underline{Доказательство}**

3. \fbox{Если $M[X^{r}] < \infty$, то $\hat{\mu_{r}}(n) \overset{\text{п.н.}}{\longrightarrow} \mu_{r}$}

**\underline{Доказательство}**

$\hat{\mu_{r}}(n) = \frac{1}{n} \sum\limits_{k = 1}^{n} (X_{k} - \overline{X_{n}})^{r} = 
\frac{1}{n} \sum\limits_{k = 1}^{n} \sum\limits_{i = 0}^{r} C_{r}^{i} X_{k}^{i}( - \hat{X_{n}})^{r - i}=
\sum\limits_{i = 0}^{r} C_{r}^{i} ( - \hat{X_{n}})^{r - i} (\frac{1}{n} \sum\limits_{k = 1}^{n}
X_{k}^{i}) = \sum\limits_{i = 0}^{r} C_{r}^{i} ( - \hat{X_{n}})^{r - i} \hat{\nu_{i}}(n)
\underset{\text{св-во (2)}}{\overset{\text{п.н.}}{\longrightarrow}}
\sum\limits_{i = 0}^{r} C_{r}^{i} (- \nu_{1})^{r - i} \hat{\nu_{i}} = 
\dots = \mu_{i}$. $\blacksquare$

4. \fbox{$D[\overline{X_{n}}] = \frac{1}{n}D[X]$}

**\underline{Доказательство}**

С учетом *независимости* $X_{1}, \dots, X_{n}$

$D[X\overline{X_{n}}] = D[\frac{1}{n} \sum\limits_{k = 1}^{n} X_{k}] = \frac{1}{n^{2}}
\sum\limits_{k = 1}^{n} D[X_{k}] = \frac{n D[X]}{n^{2}}$. $\blacksquare$

5. \fbox{$M[\hat{d_{X}}(n)] = \frac{n - 1}{n}D[X]$}

**\underline{Доказательство}**

$M[\hat{d_{X}}(n)] = \frac{1}{n} \sum\limits_{k = 1}{n} M[(X_{k} - \overline{X_{n}})^{2}] =
M[(X_{1} - \overline{X_{n}})^{2}] = D[X_{1} - \overline{X_{n}})^{2}] = 
D[\frac{n - 1}{n} X_{1} - \sum\limits_{k = 2}{n} \frac{1}{n}\overline{X_{k}}] 
\overset{\text{н / з}}{=} 
\frac{n - 1}{n}D[X_{1}] + \frac{1}{n^{2}} \sum\limits_{k = 2}^{n} D[X_{k}] =
D[X] \frac{n^{2} -2n + 1 + n - 1}{n^{2}} = D[X] \cdot \frac{n - 1}{n}$. $\blacksquare$

6. \fbox{$\frac{\overline{X_{n}} - m_{X}}{\sigma_{X}} \sqrt{n} \overset{d}{\longrightarrow} 
N(0; 1)$}

**\underline{Доказательство}**

Т.к. $X_{1}, \dots, X_{n}$ *независимые*, $M[X_{k}] = m_{k}, D[X_{k}] = \sigma_{X}^{2}, k \in \N$, то
в силу [Теоремы 5.1](#theorem-5-1):

$$\frac{\sum\limits_{k = 1}^{n} X_{k} - n m_{X}}{\sigma_{X} \sqrt{n}} = 
\frac{\frac{1}{n} \sum\limits_{k = 1}^{n} X_{k} - m_{X}}{\frac{\sigma_{X}}{\sqrt{n}}}
\overset{d}{\longrightarrow} N(0; 1)$$
\hfill $\blacksquare$

7. \fbox{$\frac{\hat{d_{X}}(n) - \sigma_{X}^{2}}{\sqrt{\mu_{4} - \mu_{2}}} \sqrt{n}
\overset{d}{\longrightarrow} N(0; 1)$}

\pagebreak

# Основные распределения в статистике

## \underline{Точечные оценки} {-}

## Определение 1

Пусть $X_{1}, \dots, X_{n}$ --- *независимые* с.в.

Тогда

$$Y = \sum\limits_{k = 1}^{n} X_{k}^{2} \sim \chi^{2}(n)$$

имеет ***распределение хи-квадрат*** с $n$ степенями свободы.

## Свойства распределения $\chi^{2}(n)$

1. $Y$ имеет плотность вероятности

$$f_{Y}(x; n) = 
\begin{cases}
    \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} x^{\frac{n}{2} - 1} e^{- \frac{x}{2}}, & x > 0 \\
    0, & x \leqslant 0 \\
\end{cases}$$

где $\Gamma(z) = \int\limits_{0}^{+ \infty} y^{z - 1} e^{-y} dy$ --- гамма-функция.

```{r, echo=FALSE}
library(ggplot2)
library(latex2exp)
p <- ggplot(data.frame(x = c(0, 15)), aes(x=x)) + stat_function(fun=dchisq, args=list(df=1), aes(colour="n = 1"))
p <- p + stat_function(fun=dchisq, args=list(df=2), aes(colour="n = 2"))
p <- p + stat_function(fun=dchisq, args=list(df=6), aes(colour="n > 2")) # n > 2
p <- p + scale_colour_manual("", values=c("green", "purple", "red"))
p <- p + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
p <- p + ggtitle("") + xlab(label="x") + ylab(label=TeX("$f_{Y}(x, n)$")) + theme(axis.text=element_text(colour="red"),
                                                                                      axis.title=element_text(colour="blue"))
p
```

2. $Y$ имеет характеристическую функцию

$$\psi_{Y}(\lambda) = (1 - 2\lambda i)^{\frac{n}{2}}$$

3. $Y \sim \chi^{2}(n), M[Y] = n, D[Y] = 2n$

4. Пусть *н / з* $Y_{1} \sim \chi^{2}(n_{1}), \dots, Y_{k} \sim \chi^{2}(n_{k})$. Тогда

$$\sum\limits_{k = 1}^{n} Y_{i} \sim \chi^{2}(\sum\limits_{k = 1}^{n} n_{i})$$

5. $$\frac{Y - n}{\sqrt{2n}} \overset{d}{\longrightarrow} N(0; 1)$$

6. Пусть $Z_{n}$ *порожденная распределением* $N(m_{X}, \sigma_{X}^{2})$. Тогда если 

$\hat{d_{X}} = \frac{1}{n} \sum\limits_{k = 1}^{n}(X_{k} - \overline{X_{n}})^{2}$, 

то $\frac{n \hat{d_{X}}}{\sigma_{X}^{2}} \sim \chi^{2}(n - 1)$.

***(Доказательство приведено далее в теореме Фишера)***

## Определение 2 {#def-7-2}

Пусть $X_{0}, X_{1}, \dots, X_{n} \sim N(0; 1)$ --- *независимые* с.в.. Тогда с.в.

$$Y = \frac{X_{0}}{\sqrt{\frac{1}{n} \sum\limits_{k = 1}^{n} X_{k}^{2}}} \sim t(n)$$

имеем ***распределение Стьюдента*** с $n$ степенями свободы.

## Свойства распределения $t(n)$

1. $Y$ имеет плотность вероятности

$$f_{x; n} = \frac{\Gamma(\frac{n + 1}{2})}{\sqrt{\pi n} \Gamma(\frac{n}{2})} (1 + \frac{x^{2}}{n})^
{- \frac{n + 1}{2}}$$

<!-- add PICTURE OF DISTRIBUTION

-->

2. $Y \sim t(n), M[Y] = 0, n \geqslant 2$

$$D[Y] = \frac{n}{n - 2}, n > 2$$

3. $t(1) = C$ --- *распределение Коши*

4. $Y \sim t(n), Y \overset{d}{\longrightarrow} N(0; 1)$

5. Пусть $Z_{n}$ порождена распределением $N(m_{X}; \sigma_{X}^{2})$. Тогда

$$\frac{\overline{X_{n}} - m_{X}}{\sqrt{\hat{d_{X}}}} \cdot \sqrt{n - 1} \sim t(n - 1)$$

***(Доказательство приведено далее в теореме Фишера)***

## Определение 3

Пусть $X \sim \chi^{2}(n), Y \sim \chi^{2}(m)$ --- *независимые* с.в. Тогда с.в. $V = \frac{X m}{Y n} \sim
F(n; m)$ имеет ***Распределение Фишера*** с $n$ и $m$ степенями свободы.

## Свойства распределения $F(n; m)$

1. $V$ имеет плотность вероятности 

$$f_{V}(x, n, m) = 
\begin{cases}
    \frac{\Gamma(\frac{n + m}{2})}{\Gamma(\frac{n}{2}) \cdot \Gamma(\frac{m}{2})} n^{\frac{n}{2}}
m^{\frac{n}{2}} \frac{x^{\frac{n}{2} - 1}}{(m + nx)^{\frac{n + m}{2}}}, & x > 0 \\
    0, & x \leqslant 0 \\
\end{cases}$$

<!-- add PICTURE OF DISTRIBUTION

-->

2. $V \sim F(n; m), M[V] = \frac{m}{m - 2}, m > 2$, 

$$D[V] = \frac{2m^{2}(m + n - 2)}{n(m - 2)^{2}(m - 4)}, m > 4$$

3. Пусть $Z_{n} = (X_{1}, \dots, X_{n})^{T}$ и $W_{m} = (Y_{1}, \dots, Y_{m})^{T}$ --- 
*однородные* выборки, порожденные распределениями $N(m_{X}; \sigma^{2})$ и $N(m_{Y}; \sigma^{2})$.

Тогда если $Z_{n}$ и $W_{n}$ **независимы**,

$$V = \frac{\frac{1}{n - 1} \sum\limits_{k = 1}{n}(X_{k} - \overline{X_{n}})^{2}}
{\frac{1}{m - 1} \sum\limits_{k = 1}{n}(Y_{k} - \overline{Y_{m}})^{2}} \sim F(n - 1; m - 1)$$

***(Данный факт следует из свойства **6** распределения $\chi^{2}(n)$)***

## Определение 4

***Параметром $\theta \in \R^{n}$ распределения*** с.в. $X$ называется любая числовая характеристика,
входящая в $F_{x}(x, \theta)$ явно.

## Определение 5

***Точечной оценкой*** неизвестного параметра $\theta$ называется произвольная стастистика
$\hat{\theta}(Z_{n})$.

## Замечание {-}

Оценка $\hat{\theta}(Z_{n})$ является с.в.

\underline{На практике используется ее реализация}.

## Определение 6

***Оценка*** $\hat{\theta}(Z_{n})$ называется ***несмещенной***, если
$$M[\hat{\theta}] = \theta$$

## Определение 7

Оценка $\hat{\theta}(Z_{n})$ называется ***состоятельной***, если
$$\hat{\theta} \overset{P}{\longrightarrow} \theta$$

## Определение 8

Оценка $\hat{\theta}(Z_{n})$ называется ***сильно состоятельной***, если
$$\hat{\theta} \overset{\text{п.н.}}{\longrightarrow} \theta$$

## Определение 9

Оценка $\hat{\theta}(Z_{n})$ называется ***с.к. состоятельной***, если
$$\hat{\theta} \overset{\text{с.к.}}{\longrightarrow} \theta$$

## Замечание {-}

1. Из свойств $\overline{X_{n}}$ следует, что $\overline{X_{n}}$ --- **несмещенная** и
**сильносостоятельная** оценка $m_{X}$.

2. Из свойств $\hat{d_{X}}$ следует, что $\hat{d_{X}}$ --- **смещенная** и **сильносостоятельная** 
оценка $m_{X}$.

## Определение 10

*Несмещнная* оценка $\hat{\theta}^{*}(Z_{n})$ называется **эффективной**, если $\forall \hat{\theta}(Z_{n})$
--- *несмещенной* оценки верно, что

$$D[\hat{\theta}^{*}(Z_{n})] \leqslant D[\hat{\theta}(Z_{n})]$$

## Пример 

Пусть $M[\overline{X}] < \infty$. Тогда $\overline{X}$ --- *сильно состоятельная* оценка $m_{X}$. Если
$D[\overline{X}] < \infty$, то $\overline{X}$ --- *с.к.-состоятельная* оценка $m_{X}$

***(Доказательство следует из ЗБЧ)***

## Теорема 1 {#theorem-7-1}

Пусть $\hat{\theta_{1}}, \hat{\theta_{2}}$, --- *с.к.-оптимальные* оценки параметра $\theta$. Тогда 

$$\hat{\theta_{1}} = \hat{\theta_{2}}$$

### Доказательство {-}

Т.к. $\hat{\theta_{1}}$ и $\hat{\theta_{2}}$ **оптимальны**, то $D[\hat{\theta_{1}}] = D[\hat{\theta_{2}}] =
d$. 

Пусть $\hat{\theta_{3}} = \frac{1}{2} (\hat{\theta_{1}} + \hat{\theta_{2}})$. Тогда

$D[\hat{\theta_{3}}] = \frac{1}{4} D[\hat{\theta_{1}} + \hat{\theta_{2}}] = \frac{1}{4}
(D[\hat{\theta_{1}}] + [D\hat{\theta_{2}}] + 2\mathrm{cov}(\hat{\theta_{1}}, \hat{\theta_{2}})) =
\frac{1}{2}(d + \mathrm{cov}(\hat{\theta_{1}}, \hat{\theta_{2}})) \leqslant 
\frac{1}{2}(d + |\mathrm{cov}(\hat{\theta_{1}}, \hat{\theta_{2}})|)
\leqslant \frac{1}{2}(d + \sqrt{D[\hat{\theta_{1}}] \cdot D[\hat{\theta_{2}}]}) = d$

Тогда *в силу оптимальности* $\hat{\theta_{1}}$ и $\hat{\theta_{2}}$ 

$$D[\hat{\theta_{3}}] = d$$

$d = D[\hat{\theta_{3}}] = \frac{1}{2}(d + \mathrm{cov}(\hat{\theta_{1}}, \hat{\theta_{2}}))$, 
$\mathrm{cov}(\hat{\theta_{1}}, \hat{\theta_{2}}) = d = \sqrt{D[\hat{\theta_{1}}] \cdot D[\hat{\theta_{2}}]}$,

т.е. в *нераввенстве Коши-Буняковского* достигается *равенство*. Следовательно,

$$\hat{\theta_{1}} = \alpha \hat{\theta_{2}} + \beta, \alpha > 0$$

$$\begin{cases}
    M[\hat{\theta_{1}}] = \alpha M[\hat{\theta_{2}}] + \beta \\
    D[\hat{\theta_{1}}] = \alpha^{2} D[\hat{\theta_{2}}] \\
\end{cases} \implies
\begin{cases}
    \theta = \alpha \theta + \beta \\
    d = \alpha^{2} d \\
\end{cases} \implies
\begin{cases}
    \beta = 0 \\
    \alpha = 1 \\
\end{cases}$$ 

Окончательно, $\hat{\theta_{1}} = \hat{\theta_{2}}$. $\blacksquare$

\pagebreak

# Эффективные оценки

Обозначим через $f(x; \theta)$ плотность вероятности с.в. $X$, порождающей **выборку** в *абсолютно
непрерывном случае* или **функцию** в *дискретном случае*. В силу *критерия независимости* функция

$$L(z_{n}; \theta) = \prod\limits_{k = 1}^{n} f(x_{k}; \theta)$$

является *плотностью вероятности* с.в. $Z_{n}$.

## Определение 1

$L(z_{n}; \theta)$ при фиксированном $z_{n} \in S$ и переменной $\theta$ называется ***функцией 
правдоподбия***.

## Замечание {-}

Далее будем полагать, что $\theta \in \R^{1}$. 

## Определение 2

Распределение с.в. $X$ называется ***регулярным***, если

1. $\sqrt{f(x;\theta)}$ --- **дифференцируема** по $\theta$ *почти для всех* $x$.

2. $i(\theta) = \int\limits_{- \infty}^{+ \infty}(\frac{\partial \ln f(x; \theta)}{\partial \theta})^{2}
f(x; \theta) dx$
**конечна**, **непрерывна** по $\theta$ и **положительна**.

## Замечание {-}

Далее будем предполагать, что выборка $Z_{n}$ порождена *регулярным распределением*.

## Определение 3

Случайная величина

$$U(Z_{n}; \theta) = \frac{\partial \ln L(Z_{n}; \theta)}{\partial \theta} = 
\sum\limits_{k = 1}^{n} \frac{\partial \ln f(x; \theta)}{\partial \theta}$$

называется ***вкладом выборки $Z_{n}$***

## Лемма 1 {#lemma-8-1}

Пусть распределение *регулярное*. Тогда 

$$M[U(Z_{n}; \theta)] = 0$$

### Доказательство {-}

В силу *условия нормировки*

$$\int\limits_{\R^{n}} L(z_{n}; \theta)d_{X_{1}}, \dots, d_{X_{n}} = 1$$

С учетом *условий регулярности*

$0 = \frac{\partial}{\partial \theta} \int\limits_{\R^{n}} L(z_{n}; \theta)d_{X_{1}}, \dots, d_{X_{n}} =
\int\limits_{\R^{n}} \frac{\partial L(z_{n}; \theta)}{\partial \theta}d_{X_{1}}, \dots, d_{X_{n}} =
\int\limits_{\R^{n}} \frac{\partial \ln L(z_{n}; \theta)}{\partial \theta}
L(z_{n}; \theta)d_{X_{1}}, \dots, d_{X_{n}} = M[U(Z_{n}; \theta)]$. $\blacksquare$

## Определение 4

***Информацией Фишера*** о параметре $\theta$, ***содержащейся в выборке $Z_{n}$***, называют

$$I_{n}(\theta) = D[U(Z_{n}; \theta)] \overset{\text{регул.}}{=} M[U^{2}(Z_{n}; \theta)]$$

$i(\theta) = M[(\frac{\partial \ln f(x; \theta)}{\partial \theta})^{2}]$ называется ***количеством
информации Фишера, содержащимся в одном наблюдении***.

## Замечание {-}

Из определения $U(Z_{n}; \theta)$ и *независимости* элементов выборки следует, что
$I_{n}(\theta) = n \cdot i(\theta)$, т.е. количество информации **вырастает пропорционально объему
выборки**.

## Лемма 2 {#lemma-8-2}

Пусть $f(x; \theta)$ *дважды непрерывно дифференцируема* по $\theta$. Тогда

$$i(\theta) = - M[\frac{\partial^{2} \ln f(X_{1}; \theta)}{\partial \theta^{2}}]$$

### Доказательство {-}

$U(X_{1}; \theta) = \frac{\partial \ln f(X_{1}; \theta)}{\partial \theta}$. С учетом [Леммы 1](#lemma-8-1)

$0 = \frac{\partial}{\partial \theta} M[U(X_{1}; \theta)] = \frac{\partial}{\partial \theta}
\int\limits_{- \infty}^{+ \infty} \frac{\partial \ln f(x; \theta)}{\partial \theta} f(x; \theta) dx =
\int\limits_{- \infty}^{+ \infty} \frac{\partial^{2} \ln f(x; \theta)}{\partial \theta^{2}} f(x; \theta)dx +
\int\limits_{- \infty}^{+ \infty} \frac{\partial \ln f(x; \theta)}{\partial \theta} \cdot
\frac{\partial f(x; \theta)}{\partial \theta} dx = 
\int\limits_{- \infty}^{+ \infty} \frac{\partial^{2} \ln f(x; \theta)}{\partial \theta^{2}} f(x; \theta)dx +
\int\limits_{- \infty}^{+ \infty} (\frac{\partial \ln f(x; \theta)}{\partial \theta})^{2} f(x; \theta)dx =
M[\frac{\partial^{2} \ln f(x; \theta)}{\partial \theta^{2}}] + i(\theta)$. $\blacksquare$

## Пример 1

Пусть $X \sim N(\theta; \sigma^{2})$.

$$f(x; \theta) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\{- \frac{(x - \theta)^{2}}{2 \sigma^{2}}\}$$

$$U(X_{1}; \theta) = \frac{\partial}{\partial \theta}(- \frac{1}{2} \ln(2\pi \sigma^{2}) - 
\frac{(x - \theta)^{2}}{2 \sigma^{2}}) = \frac{X_{T} \theta}{\sigma^{2}}$$

$$\frac{\partial^{2} \ln f(x; \theta)}{\partial \theta^{2}} = - \frac{1}{\sigma^{2}}$$

С учетом [Леммы 2](#lemma-8-2)

$$i(\theta) = -M[- \frac{1}{\sigma^{2}}] = \frac{1}{\sigma^{2}}$$

## Пример 2

Рассмотрим нерегулярную модель.

$$X \sim R(0; \theta)$$

Здесь из множества $\int\limits_{0}^{\theta} \frac{1}{\theta} dx = 1$ не следует, что
$\int\limits_{0}^{\theta} \frac{\partial}{\partial \theta}(\frac{1}{\theta}) dx = 0$, т.к. при
диффференцировании по $\theta$ появляется еще одно слагаемое:

$\frac{\partial}{\partial \theta} \int\limits_{0}^{\theta}\frac{1}{\theta} dx = 
\frac{1}{\theta} + \int\limits_{0}^{\theta} \frac{\partial}{\partial \theta}(\frac{1}{\theta})dx$

## Теорема 1 (Неравенство Рао-Крамера) {#theorem-8-1} 
 Пусть распределение $F(x; \theta)$, порождающее выборку $Z_{n}$ *регулярно*. Тогда для любой
 несмещенной оценки $\hat{\theta}$ верно неравенство:

$$D[\hat{\theta}(Z_{n})] \geqslant \frac{1}{I_{n}(\theta)} = \frac{1}{n i(\theta)}$$

При этом равенство достигается лишь в том случае, если 

$\hat{\theta}(Z_{n}) - \theta = a(\theta) \cdot U(Z_{n}; \theta)$, где

$a(\theta)$ --- некоторая функция от $\theta$

### Доказательство {-}

В силу *несмещенности* $\hat{\theta}$

$$M[\hat{\theta}] = \int\limits_{\R^{n}} (z_{n})L(z_{n}; \theta)dx_{1}, \dots, dx_{n} = \theta$$

В силу *регулярности* и [Леммы 1](#lemma-8-1)

$1 = \frac{\partial}{\partial \theta} (\theta) = \frac{\partial}{\partial \theta} M[\hat{\theta}] = 
\int\limits_{\R^{n}} \hat{\theta}(z_{n}) \frac{\partial L(z_{n}; \theta)}{\partial \theta}
dx_{1}, \dots, dx_{n} = 
\int\limits_{\R^{n}} \hat{\theta}(z_{n}) \frac{\partial \ln L(z_{n}; \theta)}{\partial \theta} \cdot
L(z_{n}; \theta) dx_{1}, \dots, dx_{n} = M[\hat{\theta} U(Z_{n}; \theta)] = 
M[(\hat{\theta} - \theta)(U(Z_{n}; \theta) - \theta)] + \theta \cdot M[U(Z_{n}; \theta)] =
\mathrm{cov}(\hat{\theta}, U(Z_{n}; \theta))$

Откуда с учетом *неравенства Коши-Буняковского* 

$$1^{2} \leqslant D[\hat{\theta}] \cdot D[U(Z_{n}; \theta)] = D[\hat{\theta}] \cdot I_{n}(\theta)$$

Причем *равенство достигается* в том и только том случае, когда $\hat{\theta} = a(\theta)U(Z_{n}; \theta) +
b(\theta)$

Но с учетом [Леммы 1](#lemma-8-1) $b(\theta) = 0$.

## Определение 5

Оценка $\hat{\theta}^{*}(Z_{n})$, *для которой достигается равенство в неравенстве Рао-Крамера* называется
***эффективной***

## Замечание {-}

В силу [Теоремы 1](#theorem-8-1) эффективная оценка является *оптимальной*. А с учетом 
[Теоремы 7.1](#theorem-7-1) эффективная оценка **единственна**

## Пример 3

Пусть $X \sim N(\theta; \sigma^{2})$

$U(Z_{n}; \theta) = \sum\limits_{k = 1}^{n} \frac{\partial \ln f(X_{k}; \theta)}{\partial \theta} = 
\sum\limits_{k = 1}^{n} \frac{X_{k} - \theta}{\sigma^{2}} = \frac{n}{\sigma^{2}} (\overline{X_{n}} -
\theta)$

Т.о. $a(\theta) = \frac{\sigma^{2}}{n}$. Тогда $a(\theta) U(Z_{n}; \theta) = \overline{X_{n}} - \theta$, 
откуда следует, что $\overline{X_{n}}$ --- *эффективная оценка*.

\pagebreak

# Методы построения точеченых оценок

## \underline{Метод максимального правдоподобия} {-}

## Определение 1

***Оценкой максимального правдоподобия*** $\theta$ называют 

$$\hat{\theta} = \underset{\theta}{argmax} L(Z_{n};\theta)$$

## Замечание {-}

1. В силу *монотонности* функции $\ln x$ справедливо представление:

$$\hat{\theta} = \underset{\theta}{argmax} L(Z_{n}; \theta) = \underset{\theta}{argmax} 
\ln L(Z_{n}; \theta)$$

2. Если $L(Z_{n}; \theta)$  --- **гладкая** и **максимум** по $\theta$ достигается **внутри
множества** возможных значений $\theta$, то $\theta$ можно вычислить из *уравнения правдоподбия*

$$U(Z_{n}; \theta) = \frac{\partial \ln L(z_{n}; \theta)}{\partial \theta} = 0$$

3. Из [Теоремы 8.1](#theorem-8-1) следует, что $\hat{\theta}$ **будет также эффективной оценкой**.

## Пример 1

Рассмотрим случайную величину $X \sim N(m; \sigma^{2})$ с неизвестными $m$ и $\sigma^{2}$:

$$\theta = (m; \sigma^{2})^{T}$$

$$L(Z_{n}; \theta) = \prod\limits_{k = 1}^{n} \frac{1}{\sqrt{2\pi \theta_{2}}} 
e^{- \frac{(X_{k} - \theta_{1})^{2}}{2 \theta_{2}}}$$

$$\ln L(Z_{k}; \theta) = \sum\limits_{k = 1}{n}(- \frac{1}{2} \ln(2\pi) - \frac{1}{2} \ln \theta_{2} - 
\frac{(X_{k} - \theta_{1})^{2}}{2 \theta_{2}})$$

$\begin{cases}
    \frac{\partial \ln L(Z_{n}; \theta)}{\partial \theta_{1}} = \sum\limits_{k = 1}^{n}
\frac{(X_{k} - \theta_{1})}{\theta_{2}} = \frac{1}{\theta_{2}}(n \overline{X_{n}} - n \theta_{1}) = 0 \\
    \frac{\partial \ln L(Z_{n}; \theta)}{\partial \theta_{2}} = \sum\limits_{k = 1}^{n}
(\frac{(X_{k} - \theta_{1})^{2}}{2 \theta_{2}^{2}} - \frac{1}{2 \theta_{2}})
= \frac{1}{2 \theta_{2}}(n \hat{d_{X}}(n) - n \theta_{2}) = 0 \\
\end{cases}$

$\begin{cases}
    \hat{\theta_{1}} = \overline{X_{n}} \\
    \hat{\theta_{2}} = \hat{d_{X}}(n) \\
\end{cases}$

## Пример 2

Пусть $X \sim R(\theta_{1}; \theta_{2})$. В этом случае $L(Z_{n}; \theta)$ **не является** непрерывной:

$$L(Z_{n}; \theta) = \prod\limits_{k = 1}^{n} f_{X} (X_{k}; \theta) =
\begin{cases}
    0, & \exists k = \overline{1, n} : X_{k} \notin [\theta_{1}; \theta_{2}] \\
    \frac{1}{(\theta_{2} - \theta_{1})^{n}}, & \forall k = \overline{1, n} : X_{k} \in [\theta_{1}; 
\theta_{2}] \\
\end{cases}$$

Тогда оценка максимального правдоподобия **не может быть вычислена из уравнений правдаподобия**. Хотя
$\hat{\theta}$ существует:

$$L(Z_{n}; \theta) = 
\begin{cases}
    0, & \min_{k = \overline{1, n}} X_{k} < \theta_{1} || \max_{k = \overline{1, n}} X_{k} > \theta_{2} \\
    \frac{1}{(\theta_{2} - \theta_{1})^{n}}, & \theta_{1} \leqslant \min_{k = \overline{1, n}} X_{k} 
\leqslant \max_{k = \overline{1, n}} X_{k} \leqslant \theta_{2} \\
\end{cases}$$

Откуда $L(Z_{n}; \theta)$ *возрастает* по $\theta_{1}$ и *убывает* по $\theta_{2}$

Тогда $\hat{\theta_{1}} = \min_{k = \overline{1, n}} X_{k} = X^{(1)}, 
\hat{\theta_{1}} = \max_{k = \overline{1, n}} X_{k} = X^{(n)}$

## Замечание {-}

Для МП-оценок выполняется **принцип инвариантности**:

Пусть $g(\theta)$ --- *биективное* отображение.

Тогда МП-оценка $\hat{g}(Z_{n}) = g(\hat{\theta}(Z_{n}))$.

Действительно, 

$\sup\limits_{\theta} L(Z_{n}; \theta) = \sup\limits_{g} L(Z_{n}; g^{-1}(g))$

Тогда $g^{-1}(\hat{g}) = \hat{\theta}$, т.е. $\hat{g} = g(\hat{\theta})$

## Пример 3

Пусть $X \sim N(\theta_{1}; \theta_{2})$. Требуется оценить $F_{X}(x_{0}) = \frac{1}{2} + 
\Phi_{0}(\frac{x_{0} - \theta_{1}}{\theta_{2}})$. Рассмотрим *биективное отображение*

$$g(\theta_{1}; \theta_{2}) = 
\begin{pmatrix}
    \frac{1}{2} + \Phi_{0}(\frac{x_{0} - \theta_{1}}{\theta_{2}}) \\
    \theta_{2} \\
\end{pmatrix}$$

Тогда 

$$\hat{g}(Z_{n}) = 
\begin{pmatrix}
    \frac{1}{2} + \Phi_{0}(\frac{x_{0} - \overline{X_{n}}}{\sqrt{\hat{d_{X}(n)}}}) \\
    \sqrt{\hat{d_{X}(n)}} \\
\end{pmatrix}$$

$X_{0} \in \R$

## Замечание {-}

Для решения уравнений правдоподобия часто используются *численные методы*

## Теорема 1 {#theorem-9-1}

Пусть распределение с.в. $X$, порождающей выборку $Z_{n}$, *регулярно*.

Функция правдоподобия $L(z_{n}; \theta)$ *имеет единственный достижимый максимум* по $\theta \forall z_{n}
\in S, n \in \N$. Тогда

1. МП-оценка $\hat{\theta}$ ***состоятельна***;

2. Если $|\frac{\partial^{k} f(x; \theta)}{\partial \theta^{k}}| \leqslant g_{k}(x), \forall \theta$, где

$\int\limits_{\R} g_{1}(x)dx < \infty, \int\limits_{\R} g_{2}(x)dx \leqslant \infty, 
\int\limits_{\R} g_{3}(x)f(x; \theta)dx \leqslant C \leqslant \infty$, а
функция $i(\theta) = \int\limits_{\R}
(\frac{\partial \ln f(x; \theta)}{\partial \theta})^{2} f(x; \theta)dx$ *конечна* и *положительна*
$\forall \theta$, то 

2.1 $M[\hat{\theta}] \to \theta$ ***(асимптотически несмещенность)***

2.2 $\hat{\theta}$ ***сильно состоятельна***;

2.3 $\sqrt{n i (\theta)}(\hat{\theta} - \theta) \overset{d}{\longrightarrow} N(0; 1)$

***Асимптотическая нормальная***

## \underline{Метод моментов} {-}

## Определение 2

Пусть $\theta = (\theta_{1}, \dots, \theta_{r})^{T}$, а для распределения $F_{X}(x; \theta)$, порождающего
выборку $Z_{n}, M[X^{r}] < \infty$

Тогда 

$$\begin{cases}
    \nu_{1}(\theta) = & \hat{\nu_{1}}(n) \\
    \vdots & \vdots \\
    \nu_{r}(\theta) = & \hat{\nu_{r}}(n) \\
\end{cases}$$

называется ***системой метода моментов***.

## Определение 3

Решение системы метода моментов 

$$\hat{\theta_{i}} = \phi_{i}(\hat{\nu_{1}}(n), \dots, \hat{\nu_{r}}(n)), i = \overline{1, r}$$

называется ***оценкой метода моментов***.

## Теорема 2 {#theorem-9-2}

Пусть функции $\phi_{1}, \dots, \phi_{r}$, определяющие оценку метода моментов *непрерывные* и 
*биективные*. Тогда оценка метода моментов ***состоятельна***.

### Доказательство {-}

Доказательство следует из **состоятельности** статистик $\hat{\nu_{i}}(n)$.

## Пример 1

$X \sim N(\theta_{1}; \theta_{2})$. Тогда $\hat{\theta_{1}} = \overline{X_{n}}, 
\hat{\theta_{2}} = \hat{d_{X}}(n)$

## Пример 2

$X \sim R(\theta_{1}; \theta_{2})$. Тогда

$$\begin{cases}
    \overline{X_{n}} = \frac{\theta_{1} + \theta_{2}}{2} \\
    \hat{\d_{X}}(n) = \frac{(\theta_{2} - \theta_{1})^{2}}{12} \\
\end{cases}
\begin{cases}
    \theta_{1} + \theta_{2} = 2 \overline{X_{n}} \\
    \theta_{2} - \theta_{1} = 2 \sqrt{\hat{d_{X}}(n)}\sqrt{3} \\
\end{cases}
\begin{cases}
    \theta_{1} = \overline{X_{n}} - \sqrt{3 \hat{d_{X}}(n)} \\
    \theta_{2} = \overline{X_{n}} + \sqrt{3 \hat{d_{X}}(n)} \\
\end{cases}$$

## Замечание {-}

Метод моментов **трудноприменим**, если *теоретические моменты не удается вычислить явно*.

\pagebreak

# Интервальные оценки

## Определение 1

Пусть выполнено условие:

$$P(\hat{\theta_{1}}(Z_{n}) \leqslant \theta \leqslant \hat{\theta_{2}}(Z_{n})) = 1 - \alpha$$

для некоторого распределения параметром $F_{x}(x; \theta)$. Тогда интервал $[\hat{\theta_{1}}(Z_{n}); 
\hat{\theta_{2}}(Z_{n})]$ --- называется ***доверительным интервалом*** или ***интервальной оценкой***
параметра $\theta$ уровня надежности $1 - \alpha$.

## Определение 2

Доверительный интервал называется ***центральным***, если верно условие:

$$P(\theta \leqslant \hat{\theta_{1}}(Z_{n})) = P(\theta \geqslant \hat{\theta_{2}}(Z_{n})) = 
\frac{\alpha}{2}$$

## Определение 3

Доверительный интервал называется ***правосторонним*** или ***левосторонним***, если выполнено условие:


<!-- тут может быть ошибка: не было после "или" альфа справа -->
$P(\theta \geqslant \hat{\theta_{2}}(Z_{n})) = \alpha$ или 
$P(\theta \leqslant \hat{\theta_{1}}(Z_{n})) = \alpha$

## Замечание {-}

1. Можно визуализировать интервальную оценку следующим образом:

<!-- add PICTURE -->

2. Интервальную оценку можно рассматривать в качестве оценки погрешности точеченой оценки 
$\hat{\theta}(Z_{n})$, если строить доверительный интервал в форме:

$$[\hat{\theta}(Z_{n}) - \varepsilon_{1}; \hat{\theta}(Z_{n}) + \varepsilon_{2}]$$

где $\varepsilon_{1}, \varepsilon_{2} > 0$.

## \underline{Построение доверительного интервала на основе центральной статистики} {-}

## Определение 4

Статистика $G(Z_{n}; \theta)$ называется ***центральной***, если $G$ *непрерывна и *строго монотонна* по 
$\theta, \forall z_{n} \in S$, а ее распределение **не зависит** от $\theta$.

Если распределение $F_{G}(Z_{n}; \theta)$ известно, то можно подобрать числа $g_{1}, g_{2}$ такие, что 
$P(g_{1} \leqslant G(Z_{n}; \theta) \leqslant g_{2}) = 1 - \alpha$.

Например, для центрального доверительного интервала $g_{1} = G_{\frac{\alpha}{2}}, 
g_{2} = G_{1 - \frac{\alpha}{2}}$ --- квантили уровней $\frac{\alpha}{2}$ и $1 - \frac{\alpha}{2}$.

Если $G(Z_{n}; \theta)$ *монотонно возрастает* по $\theta$, то можно перейти к эквивалентному 
неравенству:

$$P(G^{-1}(Z_{n}; g_{1}) \leqslant G^{-1}(Z_{n}; g_{2}) = 1 - \alpha$$

Тогда границы интервальной оценки имеют вид:

$$\hat{\theta_{1}}(Z_{n}) = G^{-1}(Z_{n}; g_{1}), \hat{\theta_{2}}(Z_{n}) = G^{-1}(Z_{n}; g_{2})$$

## Замечание {-}

Построение центральных статистик **довольно сложно**. В связи с чем зачастую использюут **
асимптотические распределения**, которые в силу ЦПТ тесно связаны с нормальным распределением.

## Определение 5

Матрица $C \in \R^{n \times n}$ называется ***ортогональной***, если $C^{T} = C^{-1}$.

## Лемма 1 {#lemma-10-1}

Пусть $C \in \R^{n \times n}$ --- *ортогональная матрица*, $C_{1j} = \frac{1}{\sqrt{n}}, j = \overline
{1, n}$. Тогда

1. $\sum\limits_{k = 1}^{n} C_{jk} \cdot C_{ik} = 0, j \neq i; i,j = \overline{1, n}$

2. $\sum\limits_{k = 1}^{n} C_{ik}^{2} = 1, i = \overline{1, n}$

3. Если $y = Cx$, то $\lVert y \rVert = \lVert x \rVert$

4. $\sum\limits_{k = 1}^{n} C_{ik} = 0, i= \overline{2, n}$

### Доказательство {-}

Пункты *1* и *2* следуют из тождества

$$C^{T} \cdot C = I$$

3. $\lVert y \rVert^{2} = (y, y) = (Cx, Cx) = (x, C^{T}Cx) = (x, x) = \lVert x \rVert^{2}$

4. Рассмотрим свойство *1* для $i = 1$. Тогда 

$$0 = \sum\limits_{k = 1}^{n} C_{1k} \cdot C_{ik} = \sum\limits_{k = 1}^{n} \frac{1}{\sqrt{n}} C_{ik} =
\frac{1}{\sqrt{n}} \sum\limits_{k = 1}^{n} C_{ik}$$

\hfill $\blacksquare$

## Теорема 1 (ФИШЕРА) {#theorem-10-1}

Пусть $Z_{n}$ --- *однородная* выборка, порожденная распределением $N(m_{X}; \sigma_{X}^{2})$. Тогда

1. $\frac{\overline{X_{n}} - m_{X}}{\sigma_{X}} \sqrt{n} \sim N(0; 1)$

2. $\frac{n \hat{d_{X}(n)}}{\sigma_{X}^{2}} \sim \chi(n - 1)$

3. $\frac{\overline{X_{n}} - m_{X}}{\sqrt{\hat{d_{X}(n)}}} \sqrt{n - 1} \sim t(n - 1)$

4. $\overline{X_{n}}$ и $\hat{d_{X}(n)}$ **независимы**

### Доказательство {-}

<!-- check here for sign in brackets i \ sigma sqrt n ... "_" -->
.1) $\frac{\overline{X_{n}} - m_{X}}{\sigma_{X}} \sqrt{n} = \sum\limits_{k = 1}^{n} \frac{1}{\sigma_{X}
\sqrt{n}} X_{k} - \frac{m_{X}\sqrt{n}}{\sigma_{X}} = 
(\frac{1}{\sigma_{X} \sqrt{n}} + \frac{1}{\sigma_{X} \sqrt{n}} + \dots + \frac{1}{\sigma_{X} \sqrt{n}})
\cdot Z_{n} - \frac{m_{X}\sqrt{n}}{\sigma_{X}}$.

Т.к. $Z_{n} \sim N(
\begin{pmatrix}
    m_{X} \\
    \vdots \\
    m_{X} \\
\end{pmatrix};
\sigma_{X}^{2} I)$, то в силу [Леммы 1.3](#lemma-1-3)

$\frac{\overline{X_{n}} - m_{X}}{\sigma_{X}} \sqrt{n} \sim N(m; \sigma^{2})$, где $m = 
(\frac{1}{\sigma_{X} \sqrt{n}} \dots \frac{1}{\sigma_{X} \sqrt{n}})
\begin{pmatrix}
    m_{X} \\
    \vdots \\
    m_{X} \\
\end{pmatrix} - 
\frac{m_{X}\sqrt{n}}{\sigma_{X}} = 0$

$$\sigma^{2} = (\frac{1}{\sigma_{X} \sqrt{n}} \dots \frac{1}{\sigma_{X} \sqrt{n}}) \sigma^{2} I \cdot
\begin{pmatrix}
    \frac{1}{\sigma_{X} \sqrt{n}} \\
    \vdots \\
    \frac{1}{\sigma_{X} \sqrt{n}} \\
\end{pmatrix} = 1$$

.2) Построим матрицу $C \in \R^{n \times n}$ так, чтобы $C_{1j} = \frac{1}{\sqrt{n}}, j = \overline{1,n}$;
остальные строки выберем так, чтобы векторы 
$\begin{pmatrix}
    C_{11} \\
    \vdots \\
    C_{1n} \\
\end{pmatrix},
\begin{pmatrix}
    C_{21} \\
    \vdots \\
    C_{2n} \\
\end{pmatrix}, \dots,
\begin{pmatrix}
    C_{n1} \\
    \vdots \\
    C_{nn} \\
\end{pmatrix} \in \R^{n}$

образовывали *ортонормированный* базис, что возможно, т.к. вектор 
$\begin{pmatrix}
    C_{11} \\
    \vdots \\
    C_{1n} \\
\end{pmatrix} = 
\begin{pmatrix}
    \frac{1}{\sqrt{n}} \\
    \vdots \\
    \frac{1}{\sqrt{n}} \\
\end{pmatrix}$ 
всегда можно дополнить до *ортонормированного* базиса.

Тогда $C$ удовлетворяет условиям [Леммы 1](#lemma-10-1).

Рассмотрим $Y = \frac{1}{\sigma_{X}} C Z_{n}$. Тогда $Y \sim N(m_{Y}; K_{Y})$, где в силу 
[Леммы 1.3](#lemma-1-3) и [Леммы 1, п.4](#lemma-10-1):

$$m_{Y} = \frac{1}{\sigma_{X}} C(\sigma_{X}^{2} I) \frac{1}{\sigma_{X}} C^{T} = CIC^{T} = CC^{T} = I$$

Т.о. $Y_{1}, \dots, Y_{n}$ --- **некоррелированные** нормальные с.в.

Тогда в силу [Леммы 1.4](#lemma-1-4) $Y_{1}, \dots, Y_{n}$ *независимы*

$\frac{n \hat{d_{X}(n)}}{\sigma_{X}^{2}} = \frac{1}{\sigma_{X}^{2}} \sum\limits_{k = 1}^{n}(X_{k} - 
\overline{X_{n}})^{2} = \frac{1}{\sigma_{X}^{2}} \sum\limits_{k = 1}^{n}(X_{k}^{2} - 
2 X_{k}\overline{X_{n}} + \overline{X_{n}}^{2}) = \frac{1}{\sigma_{X}^{2}}
(\sum\limits_{k = 1}^{n} X_{k}^{2} - 2n \overline{X_{n}}(\frac{1}{n}  \sum\limits_{k = 1}^{n} X_{k}) +
n \overline{X_{n}}^{2}) = \frac{1}{\sigma_{X}^{2}} \sum\limits_{k = 1}^{n} X_{k}^{2} - 
\frac{n}{\sigma_{X}^{2}} \overline{X_{n}}^{2} \overset{*}{=}$

$Y_{1} = \frac{1}{\sigma_{X} \sqrt{n}}(X_{1} + \dots + X_{n}) = \frac{\sqrt{n}}{\sigma_{X}} 
\overline{X_{n}}$. Тогда с учетом [Леммы 1](#lemma-10-1)

$\overset{*}{=} \frac{1}{\sigma_{X}^{2}} \lVert Z_{n} \rVert^{2} - Y_{1}^{2} = 
\frac{1}{\sigma_{X}^{2}} \lVert C Z_{n} \rVert^{2} - Y_{1}^{2} = \lVert \frac{1}{\sigma_{X}}Z_{n} 
\rVert^{2} - Y_{1}^{2} = \lVert Y \rVert^{2} - Y_{1}^{2} = \sum\limits_{k = 1}^{n} Y_{k}^{2} - 
Y_{1}^{2} = \sum\limits_{k = 2}^{n} Y_{k}^{2}$, где 

$Y_{k} \sim N(0; 1), k = \overline{2, n}$. Тогда по определению 

$$\frac{n \hat{d_{X}(n)}}{\sigma_{X}^{2}} = \sum\limits_{k = 2}^{n} Y_{k}^{2} \sim \chi^{2}(n - 1)$$

.4) Заметим, что $\overline{X_{n}} = \frac{\sigma_{X}}{\sqrt{n}} Y_{1} = \phi(Y_{1})$, где 
$Y_{1}$ и $(Y_{2}, \dots, Y_{n})^{T}$ **независимы**. Тогда **независимы** также $\phi(Y_{1})$ и
$\psi(Y_{2}, \dots, Y_{n})$, т.е. $\overline{X_{n}}$ и $\hat{d_{X}}(n)$

.3) $\frac{\overline{X_{n}} - m_{X}}{\sqrt{\hat{d_{X}(n)}}} \sqrt{n - 1} = 
\frac{\frac{\sigma_{X}}{\sqrt{n}} Y_{1} - m_{X}}{\sqrt{\frac{\sigma_{X}^{2}}{n}
\sum\limits_{k = 2}^{n} Y_{k}^{2}}}\sqrt{n - 1} = 
\frac{Y_{1} - \frac{m_{X} \sqrt{n}}{\sigma_{X}}}{\sqrt{\frac{1}{n - 1}
\sum\limits_{k = 2}^{n} Y_{k}^{2}}} \sim t(n - 1)$, т.к.

$Y_{1} - \frac{m_{X}\sqrt{n}}{\sigma_{X}} \sim N(0; 1)$ и $Y_{1}, (Y_{2}, \dots, Y_{n})^{T}$ 
**независимы**. $\blacksquare$

## Пример 3

Построить интервальную оценку $\theta$ по однородной выборке $Z_{n}$, порожденной распределением
$N(\theta; \sigma_{X}^{2})$, если $\sigma_{X}^{2}$ *неизвестна*, уровня надежности $1 - \alpha$

В силу [Теоремы Фишера](#theorem-10-1)

$$G(Z_{n}; \theta) = \frac{\overline{X_{n}} - \theta}{\sqrt{\hat{d_{X}}(n)}} \sqrt{n - 1} \sim (n - 1)$$

Тогда 

$1 - \alpha = P(t_{\frac{\alpha}{2}}(n - 1) \leqslant \frac{\overline{X_{n}} - \theta}
{sqrt{\hat{d_{X}}(n)}}\sqrt(n - 1) \leqslant t_{1 - \frac{\alpha}{2}}(n - 1)) = 
P(\overline{X_{n}} - \sqrt{\frac{\hat{d_{X}}(n)}{n - 1}} t_{1 - \frac{\alpha}{2}}(n - 1) \leqslant \theta 
\leqslant \overline{X_{n}} + \sqrt{\frac{\hat{d_{X}}(n)}{n - 1}} t_{1 - \frac{\alpha}{2}}(n - 1))$

Тогда интервальная оценка имеет вид:

$$[\overline{X_{n}} - \sqrt{\frac{\hat{d_{X}}(n)}{n - 1}} t_{1 - \frac{\alpha}{2}}(n - 1); 
\overline{X_{n}} + \sqrt{\frac{\hat{d_{X}}(n)}{n - 1}} t_{1 - \frac{\alpha}{2}}(n - 1)]$$

Заметим, что $\overline{X_{n}}$ --- *точечная оценка* $\theta$ --- лежит в середине интервала. При этом
длина интервала $2 \sqrt{\frac{\hat{d_{X}}(n)}{n - 1}} t_{1 - \frac{\alpha}{2}}(n - 1)$ **возрастает** по 
$\sigma_{X}^{2}$ и **убывает** по $\alpha$ и $n$.

## Пример 4

Пусть выборка $Z_{n}$ порождена распределением $R(0; \theta)$. Пусть

$$G(Z_{n}; \theta) = (\frac{X^{(n)}}{\theta})^{n}$$

$F_{G}(x) = P((\frac{X^{(n)}}{\theta})^{n} \leqslant x) = P(X^{(n)} \leqslant \theta \sqrt[n]{x}) =
F_{(n)}(\theta \sqrt[n]{x}) \overset{*}{=}$

в силу [Следствия 6.1](#cons-6-1) 

$\overset{*}{=} (F_{X}(\theta \sqrt[n]{x}))^{n} = 
\begin{cases}
    1^{n}, & \theta \sqrt[n]{x} \geqslant \theta, \\
    0^{n}, & 0 \leqslant 0, \\
    -(\frac{\theta \sqrt[n]{x} - 0}{\theta - 0})^{n}, & \theta \sqrt[n]{x} \leqslant (0; \theta) \\
\end{cases} = 
\begin{cases}
    1, & x \geqslant 1, \\
    0, & x \leqslant 0, \\
    x, & 0 < x < 1 \\
\end{cases} \implies G(Z_{n}; \theta) \sim R(0; 1)$

Тогда $G_{\frac{\alpha}{2}} = \frac{\alpha}{2}, G_{1 - \frac{\alpha}{2}} = 1 - \frac{\alpha}{2}$,

$$1 - \alpha = P(\frac{\alpha}{2} \leqslant (\frac{X^{(n)}}{\theta})^{n} \leqslant 1 - \frac{\alpha}{2}) =
P(X^{(n)}(1 - \frac{\alpha}{2})^{- \frac{1}{2}} \leqslant \theta \leqslant X^{(n)}(\frac{\alpha}{2})^{-
\frac{1}{n}})$$

Интервальная оценка имеет вид:

$$[X^{(n)}(1 - \frac{\alpha}{2})^{- \frac{1}{n}}; X^{(n)}(\frac{\alpha}{2})^{- \frac{1}{n}}]$$

Заметим, что точечная МП-оценка $\hat{\theta}(Z_{n}) = X^{(n)}$ **не лежит внутри** интервала.

\pagebreak

# Проверка статистических гепотез

<!-- check in this chapter z's or Z's in formulas -->

## Определение 1

***Ститистической гипотезой*** $H$ называется любое предположение относительно закона распределения с.в.
$X$, порождающей выборку $Z_{n}$. 

## Определение 2

Проверяемая гипотеза $H_{0}$ называется ***основной***. Конкурирующая с $H_{0}$ гипотеза $H_{1}$ называется
***альтернативой***.

## Определение 3

***Статистическим критерием*** или ***критерием согласия*** назыается правло, в соответствии с которым
по реализации выборки $z_{n} = Z_{n}(\omega)$ принимается или отвергается гипотеза $H_{0}$.

## Примеры

**\underline{Гипотеза о виде распределения}**

Пусть $F_{X}$ --- распределение, порождающее выборку $Z_{n}$, $F$ --- некоторая заданная функция
распределения, $\F$ --- некоторый заданный класс функций распределения.

Например, $F(x) = \frac{1}{2} + \Phi(x - 1), \F = \{F(x) = \frac{1}{2} + \Phi(\frac{x - m}{\sigma}):
m \in \R, \sigma > 0\}$

Тогда можно рассмотреть

$H_{0}: F_{X}(x) = F(x), x \in \R$ или $H_{0}:F_{X} \in \F$

**\underline{Гипотеза однородности}**

Пусть задано $k$ выборок $Z_{n_{1}}^{1}, \dots, Z_{n_{k}}^{k}$.

Требуется определить, образуют ли они единую однородную выборку. 

Если $F_{i}(x)$ --- функция распределения, порождающая выборку $Z_{n_{i}}^{i}$, то 

$$H_{0} : F_{1}(x) = \dots = F_{k}(x)$$

**\underline{Гипотеза независимости}**

Пусть $Z_{n}^{1}, Z_{n}^{2}$ --- *однородные* выборки, порождаемые с.в. $X$ и $Y$ соответственно с 
функциями распределения $F_{X}$ и $F_{Y}$.

Через $F_{X, Y}(x, y) = F_{X}(x) \cdot F_{Y}(y); x,y \in \R$

## Замечание {-}

Для проверки гипотезы $H_{0}$ используется специальные статистики $T(Z_{n})$, характеризующие
отклонение наблюдения от теоретического предположения.

## Определение 4

Обозначим через $G$ ***множество всех возможных значений статистики $T$***:

$$G = \{t : t = T(z_{n}), z_{n} \in S\}$$

## Определение 5

Построим разбиение $G$ на ***доверительную область $G_{\circ\alpha}$ и критическую область 
$G_{| \alpha}$*** так, чтобы

$$G_{\circ\alpha} \cup G_{| \alpha} = G$$

$$G_{\circ\alpha} \cap G_{| \alpha} = \varnothing$$

$P(T(z_{n}) \in G_{| \alpha} | H_{0}) \leqslant \alpha, P(T(Z_{n}) \in G_{\circ\alpha} | H_{0}) \geqslant 
1 - \alpha$ для некоторого $\alpha \in (0; 1)$.

Если $H_{0}$ принимается в случае, когда $T(Z_{n}) \in G_{| \alpha}$, то $\alpha$ называется
***уровнем значимости критерия***.

## Определение 6

***Ошибкой \underline{первого} рода*** называется событие, когда $H_{0}$ *верна*, но она *отвергается*

$T(z_{n}) \in G_{| \alpha}$, $H_{0}$ **верно**.

## Определение 7

***Ошибкой \underline{второго} рода*** называется событие, когда $H_{0}$ *неверно*, но она *принимается*

$T(z_{n}) \in G_{\circ\alpha}$, $H_{0}$ **неверно**.

## Замечание {-}

1.  Если $H_{0}$ *принимается*, то это не значит, что $H_{0}$ верно. Это значит, что наблюдения
$z_{n} = Z_{n}(\omega)$ ***согласуется*** с гипотезой $H_{0}$.

2. Для построения доверитиельной и критической областей необходимо знать

$$Law(T(Z_{n}) | H_{0})$$

а также задать уровень значимости $\alpha$

3. Структура критической области $G_{| \alpha}$ определяется алтернативой $H_{1}$

## Определение 8

Пусть $\F$ --- *множество всех распределений* $T(Z_{n})$, *удовлетворяющих альтернативе* $H_{1}$.

Тогда ***функцией мощности критерия*** $W: \F \to [0; 1]$ называется

$$W(F) = P(T(Z_{n}) \in G_{| \alpha} | Law(T(Z_{n})) = F_)$$

Значение $W(F)$ называется ***мощностью критерия при альтернативе $F$***.

## Определение 9

Критерий называется ***несмещенным***, если $W(F) > \alpha, \forall F \in \F$.

## Замечание {-} 

1. Для построения $W(F)$ необходимо знать $Law(T(Z_{n}) | F)$, что не всегда доступно.

2. Качество критерия определяется функцией мощности: чем выше мощность, тем чаще критерий отвергает 
$H_{0}$, если она *неверна*.

## \underline{Критерий согласия Колмогорова} {-}

$H_{0} : F_{X}(x) = F(x), x \in \R$

$H_{1} : F_{X}(x_{0}) \neq F(x_{0})$ для некоторого $x_{0} \in \R$

$$T(Z_{n}) = \sup\limits_{x \in \R}|\hat{F_{n}}(x) - F(x)|$$

Данный критерий применяется для *непрерывных* $F(x)$.

$Law(T(Z_{n}) | H_{0})$ известно. Для *малых* $n \in \N$ соответствующие квантили берутся из таблиц. 
При больших $n \in \N$ исользуют *асимптотическую аппроксимацию*:

$$F_{\sqrt{n} T(Z_{n}) | H_{0}} (x) \overset{n \to \infty}{\longrightarrow} K(x) = 
\begin{cases}
    \sum\limits_{k = - \infty}^{+ \infty} (-1)^{k} e^{-2k^{2}x^{2}}, & x > 0 \\
    0, & x \leqslant 0 \\
\end{cases}$$

$K(x)$ --- ***функция распределения Колмогорова***

## \underline{Критерий согласия хи-квадрат Пирсона} {-}

Пусть для выборки $Z_{n}$ построено разбиение $\R$:

$- \infty = t_{0} < t_{1} < t_{2} < \dots < t_{l} < t_{l + 1} = + \infty$, где 

$t_{1} \leqslant X^{(1)}, t_{l} > X^{(n)}$.

На построенном разбиении построим гистограмму:

$$\hat{f_{n}}(x) = 
\begin{cases}
    0, & x \in (t_{0}, t_{1}) \cup [t_{l}; t_{l + 1}), \\
    \hat{p_{k}}, & x \in [t_{k}; t_{k + 1}), k = \overline{1, l - 1} \\
\end{cases}$$

Пусть $p_{k} = P(X \in [t_{k}; t_{k + 1}) |H_{0}) = F(t_{k + 1}) - F(t_{k}), k = \overline{0, l}$.

*Статистика критерия Пирсона* имеет вид:

$$T_{\chi^{2}}(Z_{n}) = n \cdot \sum\limits_{k = 0}^{l} \frac{(\hat{p_{k}} - p_{k})^{2}}
{p_{k}}$$

где $\hat{p_{0}} = \hat{p_{l}} = 0$.

## Теорема 1 {#theorem-11-1}

Пусть $p_{k} \in (0; 1), k = \overline{0, l}$. 

Тогда 

$$Law(T_{\chi^{2}}(Z_{n}) | H_{0}) \overset{n \to \infty}{\longrightarrow} \chi^{2}(l)$$

## Замечание {-}

1. **Высокая точность** приближения законом $\chi^{2}(l)$ **достигается** при $l \geqslant 5$ и 
$n \geqslant 50$.

2. Если у $F(x; \theta_{1}, \dots, \theta_{5})$ теоретической функции распределения
есть $s$ *неизвестных параметров*, то 

$$Law(T_{\chi^{2}}(Z_{n}) | H_{0}) \overset{n \to \infty}{\longrightarrow} \chi^{2}(l - s)$$

## \underline{Проверка гипотезы о значении параметра} {-}

Рассматривается выборка $Z_{n}$, порожденная распределением $N(m_{X}; \sigma_{X}^{2})$

Рассмотрим $H_{0}: m_{X} = m_{0}$ против

$H_{1} : m_{X} \neq m_{0}, H_{2} : m_{X} > m_{0}, H_{3} : m_{X} < m_{0}$

**\underline{1 случай: $G_{X}^{2}$ известна}** 

$$T(Z_{n}) = \frac{(\overline{X_{n}} - m_{0})}{\sigma_{X}} \sqrt{n}, Law(T(Z_{n}) | H_{0}) = N(0; 1)$$

$$G_{1\alpha} = (- \infty; u_{\frac{\alpha}{2}}) \cup (u_{1 -\frac{\alpha}{2}}; + \infty)$$

$$G_{2\alpha} = (u_{1 -\alpha}; + \infty), G_{3\alpha} = (- \infty; U_{\alpha})$$

**\underline{2 случай: $G_{X}^{2}$ неизвестна}** 

$$T(Z_{n}) = \frac{(\overline{X_{n}} - m_{0})}{\sqrt{\hat{d_{X}(n)}}} \sqrt{n - 1}, 
Law(T(Z_{n}) | H_{0}) = t(n - 1)$$

$$G_{1\alpha} = (- \infty; t_{\frac{\alpha}{2}}(n - 1)) \cup (t_{1 -\frac{\alpha}{2}}(n - 1);+ \infty)$$

$$G_{2\alpha} = (t_{1 -\alpha}(n - 1); + \infty), G_{3\alpha} = (- \infty; t_{\alpha}(n - 1))$$

Рассмотрим $H_{0}: \sigma_{X}^{2} = \sigma_{X}^{2}$ против

$H_{1} : \sigma_{X}^{2} \neq \sigma_{0}^{2}$

**\underline{1 случай: $m_{X}$ известна}** 

$$T(Z_{n}) = \frac{\sum\limits_{k = 1}^{n}(X_{k} - m_{X})}{\sigma_{0}^{2}},
Law(T(Z_{n}) | H_{0}) = \chi^{2}(n)$$

**\underline{2 случай: $m_{X}$ неизвестна}** 

$$T(Z_{n}) = \frac{n \hat{d_{X}}(n)}{\sigma_{0}^{2}},
Law(T(Z_{n}) | H_{0}) = \chi^{2}(n - 1)$$

\pagebreak

# Метод наименьших квадратов 

В рамках регрессионного анализа рассматривается задача восстановления зависимости $Y = \phi(x)$ 
по набору наблюдений $Y_{1}, \dots, Y_{n}$, которые предполагаются **защищенными**.

<!-- add PICTURE -->

## Определение 1

***Линейной регрессионной моделью*** называется класс линейных по набору неизвестных параметров
$\theta \in \R^{s}$ функций:

$$\phi(x; \theta) = \theta_{1}\phi_{1}(x) + \dots + \theta_{s}\phi_{s}(x)$$

## Определение 2

***Схемой Гаусса-Маркова*** называется модель наблюдения линейной регрессионной модели при 
наличии случайных ошибок наблюдения:

$$Y_{k} = \theta_{1}\phi_{1}(x) + \dots + \theta_{s}\phi_{s}(x_{k}) + \varepsilon_{k}, 
k = \overline{1, n}$$

ИЛИ 

$Y = X\theta + E$, где $X = 
\begin{pmatrix}
    \phi_{1}(x_{1}) & \dots & \phi_{s}(x_{1}) \\
    \vdots & \ddots & \vdots \\
    \phi_{1}(x_{n}) & \dots & \phi_{s}(x_{n}) \\
\end{pmatrix} \in \R^{n \times s}$

$$\theta = (\theta_{1}, \dots, \theta_{s})^{T} \in \R^{s}, Y = (Y_{1}, \dots, Y_{s})^{T} \in 
\R^{n}$$

$$E = (\varepsilon_{1}, \dots, \varepsilon_{n})^{T} \in \R^{n}$$

где $X$ называется ***регрессионной матрицей***. Предполагается, что 

$$M[E] = 0, K_{E} = \sigma^{2} \cdot I$$

## Определение 3

***МНК-оценкой вектора $\theta$*** называется 

$$\hat{\theta}(Y) = \underset{\theta}{argmin}(Y - X\theta)^{T}(Y - X\theta) = 
\underset{\theta}{argmin}(\sum\limits_{k = 1}^{n}(Y_{k} - \theta_{1}\phi_{1}(x_{k}) - \dots - 
\theta_{s}\phi_{s}(x_{k}))^{2})$$

## Теорема 1 (Гаусса-Маркова) {#theorem-12-1}

Пусть матрица $X \in \R^{n \times s}$ такая, что $det(X^{T}X) \neq 0$

Тогда

1. МНК-оценка $\hat{\theta}(Y)$ существует, ***единственна*** и определяется соотношением:

$$\hat{\theta}(Y) = (X^{T}X)^{-1}X^{T}Y$$

2. МНК-оценка $\hat{\theta}(Y)$ ***несмещенная*** обладает **наименьшей дисперсией** в классе
линейных по $Y$ и *несмещенных* оценок по координатам.

3. Ковариационная матрица $\hat{\theta}$ имеет вид

$$K_{\hat{\theta}} = \sigma^{2} (X^{T}X)^{-1}$$

### Доказательство {-}

1. $$J(\theta) = (Y - X\theta)^{T}(Y - X\theta) = Y^{T}Y - 2Y^{T}X\theta + 
\theta^{T}X^{T}X\theta \longrightarrow \min\limits_{\theta}$$

$J(\theta)$ --- квадратичная функция. При этом $H_{J} = 2X^{T}X$ --- *матрица Гессе* **невырождена**
и **положительно определена**:

Пусть $x \in \R^{s} \backslash\{0\}$. Тогда $x^{T}(2X^{T}X)x = 2(Xx, Xx) > 0$.

Т.о. $J(\theta)$ имеем *единственный экстремум* --- точку **минимума**, которая может быть найдена
из необходимых условий:

$$\nabla_{\theta}J(\theta) = -(2Y^{T}X)^{T} + 2X^{T} \cdot X\theta = 0$$

$$\hat{\theta} = (X^{T}X)^{-1} X^{T} Y$$

2. $M[\hat{\theta}] = M[(X^{T}X)^{-1}X^{T}Y] = (X^{T}X)^{-1} X^{T}M[X\theta + E] =
(X^{T}X)^{-1}X^{T}X\theta + (X^{T}X)^{-1}X^{T} \cancelto{0}{M[E]} = 0$

Рассмотрим произвольную несмещенную линейную оценку $\tilde{\theta} = AY$, где 
$A \in \R^{s \times n}$

Тогда $\forall \theta \in \R^{s}$

$$\theta = M[\tilde{\theta}] = M[AX\theta + AE] = AX\theta + AM[E] = AX\theta$$

Откуда $AX = I$

$K_{\tilde{\theta}} = M[(\tilde{\theta} - \theta)(\tilde{\theta} - \theta)^{T}] = M[(AY - \theta)(AY - \theta
)^{T}] = [(\cancel{AX\theta} - AX - \cancel{\theta})(\cancel{AX\theta} - AE - \cancel{\theta})^{T}] 
\overset{AX = I}{=} M[AEE^{T}A^{T}] = AK_{E}A^{T} = \sigma^{2}AA^{T}$ \hfill ($\ast$)

$AA^{T} = (A - ((X^{T}X)^{-1}X^{T}) + ((X^{T}X)^{-1}X^{T}))(A - (X^{T}X)^{-1}X^{T} + (X^{T}X)^{-1}X^{T})^{T}
= (A - (X^{T}X)^{-1}X^{T})(A -(X^{T}X)^{-1}X^{T})^{T} + (A -(X^{T}X)^{-1}X^{T})X(X^{T}X)^{-1} +
(X^{T}X)^{-1}X^{T}(A - (X^{T}X)^{-1}X^{T})^{T}  + (X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} =
(A - (X^{T}X)^{-1}X^{T})(A -(X^{T}X)^{-1}X^{T})^{T} + AX(X^{T}X)^{-1} - 
(X^{T}X)^{-1}X^{T}X(X{^T}X)^{-1} + (X^{T}X)^{-1}X^{T}A^{T} - (X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} + 
(X^{T}X)^{-1} \overset{AX = I}{=} (A - (X^{T}X)^{-1}X^{T})(A-(X^{T}X)^{-1}X^{T})^{T} + (X^{T}X)^{-1}$

т.к. $(A - (X^{T}X)^{-1}X^{T})(A - (X^{T}X)^{-1}X^{T})^{T} \geqslant 0$, то все ее диагональные элементы,
которые и определяют $D[\theta_{k}], k = \overline{1, s}$, **неотрицательны**. Тогда *минимальное* значение
$D[\theta_{k}], \forall k = \overline{1, s}$ достигается, если 

$(A - (X^{T}X)^{-1}X^{T})(A - (X^{T}X)^{-1}X^{T})^{T} = 0$, т.е.

$$A = (X^{T}X)^{-1}X^{T}$$

ИЛИ

$$\hat{\theta} = \hat{\theta}$$

3. Если положить $A = (X^{T}X)^{-1}X^{T}$ то из ($\ast$) следует, что 

$K_{\hat{\theta}} = \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} = \sigma^{2}(X^{T}X)^{-1}$. $\blacksquare$

## Замечание {-}

[Теорема 1](#theorem-12-1) гарантирует свойства МНК-оценки для любых коррелированных шумов.

## \underline{Нормальная регресcия} {-}

## Определение 4

Схема Гаусса-Маркова называется ***нормальной регрессией***, если $E \sim N(0; \sigma^{2}I)$.

## Лемма 1 {#lemma-12-1}

В нормальной регрессии 

$$\hat{\theta} \sim N(\theta; \sigma^{2}(X^{T}X)^{-1})$$

### Доказательство {-}

Доказательство следует из [Леммы 1.3](#lemma-1-3) и [Теоремы 1 п.2, п.3](#theorem-12-1). $\blacksquare$

## Лемма 2 {#lemma-12-2}

В нормальной регрессии МНК-оценка и МП-оценка ***совпадают***

### Доказательство {-}

В силу [Леммы 1.3](#lemma-1-3) $Y \sim N(X\theta; \sigma^{2}I)$

Тогда функция правдоподобия по определению имеет вид

$$L(y; \theta) = f_{Y}(y; \theta) = \frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\det(\sigma^{2}I)}} 
\exp\{- \frac{1}{2\sigma^{2}}(y - X\theta)^{T}(y - X\theta)\}$$

откуда $\underset{\theta}{argmax} L(Y; \theta) = \underset{\theta}{argmin}(Y - X\theta)^{T}(Y - X\theta)
= \hat{\theta}$. $\blacksquare$

## Следствие 1

В нормальной регрессии МНК-оценка $\hat{\theta}$ ***эффективна***, т.е. *оптимальна* в классе всех
*несмещенных* оценок.

### Доказательство {-}

Доказательство следует из [Леммы 2](#lemma-12-2) и [Теоремы 8.1](#theorem-8-1). $\blacksquare$

## Лемма 3 {#lemma-12-3}

В нормальной регрессии МП-оценка $\sigma^{2}$ имеет вид

$$\hat{\sigma^{2}} = \frac{1}{n} \lVert Y - X \hat{\theta} \rVert$$

### Доказательство {-}

$\ln L(Y; \theta; \sigma^{2}) = - \frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln \sigma^{2} -
\frac{(Y - X\theta)^{T}(Y - X\theta)}{2\sigma^{2}}$,

$$\begin{cases}
    \frac{\partial \ln L(Y;\theta;\sigma^{2})}{\partial \sigma^{2}} = - \frac{n}{2\sigma^{2}} + 
\frac{(Y - X\theta)^{T}(Y - X\theta)}{2(\sigma^{2})^{2}} = 0, \\
   \nabla_{\theta}\ln L(Y;\theta;\sigma^{2}) = - \frac{1}{2\sigma^{2}} (-
2X^{T}Y + 2X^{T}X\theta) = 0 \\
\end{cases}$$

Откуда $\hat{\theta} = (X^{T}X)^{-1}X^{T}Y$ --- МНК-оценка,

$$\hat{\sigma^{2}} = \frac{1}{n}(Y - X\hat{\theta})^{T}(Y - X\hat{\theta})$$
\hfill $\blacksquare$

## Определение 5

***Вектором остатков*** называется

$$\hat{E} = Y - X\hat{\theta}$$

## Лемма 4 {#lemma-12-4}

В нормальной регрессии $\hat{E} \sim N(0; \sigma^{2}(I - X(X^{T}X)^{-1}X^{T}))$

### Доказательство {-}

$\hat{E} = Y - X\hat{\theta} = X\theta + E - X(X^{T}X)^{-1}X^{T}(X\theta + E) = 
X\theta + E - X(X^{T}X)^{-1}X^{T}X\theta - X(X^{T}X)^{-1}X^{T}E = 
(I - X(X^{T}X)^{-1}X^{T})E$.

Тогда в силу [Леммы 1.3](#lemma-1-3) $m_{\hat{E}} = 0$

$K_{\hat{E}} = (I - X(X^{T}X)^{-1}X^{T})\sigma^{2}I(X-X(X^{T}X)^{-1}X^{T})^{T} = 
\sigma^{2}(I - 2X(X^{T}X)^{-1}X^{T} + X(X^{T})^{-1}X^{T}X(X^{T}X)^{-1}X^{T}) = \sigma^{2}
(I - (X(X^{T}X)^{-1}X^{T}))$. $\blacksquare$

## Лемма 5 {#lemma-12-5}

В нормальной регрессии $\hat{\theta}$ и $\hat{E}$  **независимы**

### Доказательство {-}

В силу [Леммы 1.5](#lemma-1-5) достаточно показать, что $\hat{\theta}$ и $\hat{E}$ *некоррелированы*.

$K_{\hat{\theta}\hat{E}} = \mathrm{cov}(\hat{\theta}, Y - X\hat{\theta}) = \mathrm{cov}(\hat{\theta},
Y) - \mathrm{cov}(\hat{\theta},\hat{\theta}) \cdot X^{T} \overset{*}{=}$

с учетом [Леммы 1](#lemma-12-1) и [Теоремы 1](#theorem-12-1) 

$\overset{*}{=} \mathrm{cov}((X^{T}X)^{-1}X^{T}Y, Y) - \sigma^{2}(X^{T}X)^{-1}X^{T} = 
(X^{T}X)^{-1}X^{T} \mathrm{cov}(X\theta + E, X\theta + E) - \sigma^{2}(X^{T}X)^{-1}X^{T} = 
(X^{T}X)^{-1}X^{T}\sigma^{2}I - \sigma^{2}(X^{T}X)^{-1}X^{T} = 0$. $\blacksquare$

## Лемма 6 {#lemma-12-6}

В нормальной регрессии

$$\frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}} = n \cdot \frac{\hat{\sigma}^{2}}{\sigma^{2}} \sim
\chi^{2}(n - s)$$

### Доказательство {-}

Пусть $E_{0} = \frac{E}{\sigma}$ Тогда $E_{0} \sim N(0; I)$

$\frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}} \lVert Y - X(X^{T}X)^{-1}X^{T}Y 
\rVert^{2} = \frac{1}{\sigma^{2}} \lVert X\theta + E - X(X^{T}X)^{-1}X^{T}X\theta - X(X^{T}X)^{-1}X
^{T}E \rVert^{2} = \frac{1}{\sigma^{2}} \lVert (I - X(X^{T}X)^{-1}X^{T})E \rVert^{2} = 
E_{0}^{T} A E_{0}$, где 

$A = (I - X(X^{T}X)^{-1}X^{T})^{T}(I - X(X^{T}X)^{-1}X^{T}) = 
I - 2X(X^{T}X)^{-1}X^{T} + X(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}X^{T} = 
I - X(X^{T}X)^{-1}X^{T}$. Откуда следует, что $A^{2} = A$.

Т.к. $A^{T} = A$ и $A \geqslant 0$, то все собственные значения $\lambda_{1}, \dots, \lambda_{n} \in \R$
*неотрицательны*, а переход в нормальный жорданов базис описывается ортогональным преобразованием $C$:

$$C^{T} = C^{-1}, A = C^{T}diag(\lambda_{1}, \dots, \lambda_{n})C$$

Тогда 

$$A^{2} = C^{T}diag(\lambda_{1}^{2}, \dots, \lambda_{n}^{2})C$$

Из $A^{2} = A$ следует, что $\lambda_{i} = \lambda_{i}^{2}, i = \overline{1, n}$, т.е. $\lambda_{i} \in 
\{0; 1\}$

Тогда 

<!-- rebuild second matrix -->
$$\Lambda = 
\begin{pmatrix}
    \lambda_{1} & \dots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \dots & \lambda_{n} \\
\end{pmatrix} = 
\begin{pmatrix}
    1 & \dots & 0 \\
    \vdots & 1 & \vdots \\
    0 & \dots & 0 \\
\end{pmatrix}$$

где $m = rgA$

$\frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}} = E_{0}^{T} C^{T} \cdot \Lambda C E_{0} = 
\sum\limits_{i = 1}^{n} \tilde{\varepsilon_{i}}^{2}$, где с учетом [Леммы 1.3](#lemma-1-3)

$CE_{0} = (\tilde{\varepsilon_{1}}, \dots, \tilde{\varepsilon_{n}}) \sim N(0, CIC^{T}) = 
N(0; I)$

Т.о. по определению $\frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}} \sim \chi^{2}(m)$

Т.к. $tr(AB) = tr(BA), \forall A, B^{T} \in \R^{n \times m}$, то 

$tr(A) = tr(C^{T}\Lambda C) = tr(CC^{T}\Lambda) = tr(\Lambda) = m = rgA$

$m = tr(I - X(X^{T}X)^{-1}X^{T}) = n -tr(X(X^{T}X)^{-1}X^{T}) = n - tr(X^{T}X(X^{T}X)^{-1}) = 
n - tr(\underset{s \times s}{I}) = n - s$. $\blacksquare$

## Лемма 7 {#lemma-12-7}

В нормальной регрессии

$$\frac{\hat{\theta_{k}} - \theta_{k}}{\lVert \hat{E} \rVert \sqrt{\alpha_{k}}} \cdot \sqrt{n - s} 
\sim t(n - s)$$

где $\alpha_{k}$ --- элемент на главной диагонали матрицы $(X^{T}X)^{-1}$

### Доказательство {-}

Из [Леммы 1](#lemma-12-1) следует, что $\hat{\theta_{k}} \sim N(\theta_{k}, \sigma^{2} \alpha_{k})$

Тогда 

$\frac{\hat{\theta_{k}} - \theta_{k}}{\sigma \sqrt{\alpha_{k}}} \sim N(0; 1)$

С учетом [определения распределения Стьюдента](#def-7-2) и [Леммы 5](#lemma-12-5), и [Леммы 6](#lemma-12-6)

$\frac{\hat{\theta_{k}} - \theta_{k}}{\lVert \hat{E} \rVert \sqrt{\alpha_{k}}} \cdot \sqrt{n - s} = 
\frac{\frac{\hat{\theta_{k}} - \theta_{k}}{\sigma \sqrt{\alpha_{k}}}}{\sqrt{\frac{1}{n - s} 
\frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}}}} \sim t(n - s)$. $\blacksquare$

## Лемма 8 {#lemma-12-8}

В нормальной регрессии 

$\frac{\phi(x; \hat{\theta_{k}}) - \phi(x; \theta_{k})}{\lVert \hat{E} \rVert \sqrt{\alpha_{k}}} 
\cdot \sqrt{n - s} \sim t(n - s)$, где

$\alpha(x) = (\phi_{1}(x), \dots, \phi_{s}(x))(X^{T}X)^{-1}
\begin{pmatrix}
    \phi_{1}(x) \\
    \vdots \\
    \phi_{s}(x) \\
\end{pmatrix}, x \in \R$

### Доказательство {-}

Из [Леммы 1](#lemma-12-1) следует, что 

$$\phi(x; \hat{\theta}) = \hat{\theta}\phi_{1}(x) + \dots + \hat{\theta_{s}}\phi_{s}(x) \sim N(\phi(x; 
\theta); \alpha(x)\sigma^{2})$$

Тогда 

$\frac{\phi(x; \hat{\theta_{k}}) - \phi(x; \theta_{k})}{\sigma \sqrt{\alpha_{k}}} \sim N(0; 1)$. 

С учетом лемм 5 и 6, определения Стьюдента

$\frac{\phi(x; \hat{\theta_{k}}) - \phi(x; \theta_{k})}{\lVert \hat{E} \rVert \sqrt{\alpha_{k}}} =
\frac{\frac{\phi(x; \hat{\theta_{k}}) - \phi(x; \theta_{k})}{\sigma \sqrt{\alpha_{k}}}}{
{\sqrt{\frac{1}{n - s} \frac{\lVert \hat{E} \rVert^{2}}{\sigma^{2}}}}} \sim t(n - s)$. $\blacksquare$

## Замечание {-}

Леммы *6, 7, 8* позволяют построить доверительные интервалы для $\theta_{1}, \dots, \theta_{s}, 
\sigma^{2}, \phi(x;\theta)$.

Для построения точеченой оценки $\theta_{1}, \dots, \theta_{s}$ и $\phi(x;\theta)$ исользуется 
[Теорема 1](#theorem-12-1), для точечной оценки --- [Лемма 3](#lemma-12-3).

Зачастую в качестве $\phi_{k}(x)$ рассматривают $\phi_{k}(x) = x^{k - 1}$. Однако порядок многочлена $s$,
как правило, несмещен. Для его определения можно использовать ***критерий Фишера***:

## \underline{Критерий Фишера} {-}

$$H_{0} : \theta_{k} = 0$$

$$H_{1} : \theta_{k} \neq 0$$

$$T(Y) = \frac{\hat{\theta_{k}}^{2}}{\alpha_{k} \lVert \hat{E} \rVert^{2}}(n - s)$$

где $\alpha_{k}$ --- k-й элемент на главной диагонали матрицы $(X^{T}X)^{-1}$.

$$Law(T(Y) | H_{0}) = F(1, n - s)$$

$$G_{\circ\alpha} = [0; F_{1 - \alpha}(1 ; n - s))$$

$$G_{| \alpha} = [F_{1 - \alpha}(1 ; n - s); + \infty)$$
