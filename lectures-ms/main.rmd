---
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
linkcolor: blue
 
mainfont: Linux Libertine O
fontsize: 12pt
numbersections: true
indent: true

header-includes:
- \usepackage{indentfirst}
- \usepackage[russian, english]{babel}
- \usepackage{fancyhdr}
- \usepackage{cancel}

- \pagestyle{fancy}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Ll}{\mathbb{L}}
- \newcommand{\N}{\mathbb{N}}

- \setlength{\headheight}{15pt}
---
\input{./files/title.tex}

\tableofcontents
\clearpage

# Источники {-}

* [Ивченко Г. И., Медведев Ю. И. "Математическая статистика", изд. "Высшая школа", 1984](https://www.hse.ru/pubs/share/direct/content_document/103185710)

* [Кибзун А. И., Наумов А. В., Горяинова Е. Р. "Теория вероятностей и математическая статистика. Базовый курс с примерами и задачами", изд "ФИЗМАТЛИТ", 2013](https://ami.nstu.ru/~post/teaching/tv_ms/kibzun.pdf)

* [Панков А. Р., Платонов Е. Н. "Практикум по математической статистике", изд. "МАИ", 2006](http://zyurvas.narod.ru/knyhy2/Pankov_matstat.pdf)

\pagebreak

# Многомерное нормальное распределение

## Замечание {-}

Вектор $X = (X_{1}, \dots, X_{n})^{T}$ называется ***случайным***, если $X_{1}, \dots, X_{n}$ --- *случайные величины* *(далее **с.в**)*, определенные на одном вероятностном пространстве.

Через $M[X] = m_x$ обозначим вектор математического ожидания:

\begin{align*}
M[X] = m_x = 
    \begin{pmatrix}
    M[X_1] \\
    \vdots \\
    M[X_n]
    \end{pmatrix}
\end{align*}

Через $K_x$ обозначим *ковариационную матрицу* с.в $X$:

\begin{align*}
K_x = 
    \begin{pmatrix}
    \mathrm{cov}(X_1, X_1) & \dots & \mathrm{cov}(X_1, X_n) \\
    \vdots & \ddots & \vdots   \\ 
    \mathrm{cov}(X_n, X_1) & \dots & \mathrm{cov}(X_n, X_n) \\
    \end{pmatrix}
\end{align*}

## Лемма 1

Пусть $K_x \in \R^{n \times n}$ --- ковариационная матрица с.в $X$. Тогда:

1. $K_x \geqslant 0$, т.е. $\forall x \in \R^{n} \backslash \{0\}, x^{T}K_{x}x \geqslant 0$;

2. $K_{x}^{T} = K_x$

## Определение 1

Случайный вектор $X = (X_1, \dots, X_n)^{T}$ называется ***невырожденным нормальным вектором***:

$$X \sim N(m_x, K_x)$$

если совместная плотность вероятности имеет вид:

$$f_{x}(x) = ((2\pi)^{n} \det K_x)^{\frac{-1}{2}} \exp\{\frac{-1}{2}(x - m_{k})^{T}K_x^{-1}(x - m_x)\}$$

где $m_x \in \R^{n}, K_x \in \R^{n \times n}, K_x > 0, K_{x}^{T} = K_x$

## Лемма 2

Пусть $X$ --- невырожденный нормальный вектор с параметрами $m_x$ и $K_x$.

Тогда $M[X] = m_x$, а $K_x$ --- корвариационная матрица $X$.

Рассмотрим основные свойства многомерного нормального распределения.

## Лемма 3

Пусть $X \sim N(m_x, K_x), A \in \R^{m \times n}, b \in \R^{m}$.

Тогда:

\begin{align*}
Y = AX + b \sim N(m_y, K_y), \\
m_y = Am_x + b, \\
K_y = AK_xA^{T}.
\end{align*}

## Лемма 4

Пусть $X \sim N(m_x, K_x)$.

Тогда компоненты вектора $X$ ***независимы*** тогда и только тогда, когда они *некоррелированы*.

### note {-}

Доказательство данных утверждений при помощи аппарата функций распределения и плотности довольно сложно.
Поэтому рассмотрим аппарат характеристических функций.

## Определение 2

Пусть $X = (X_1, \dots, X_n)^{T}$ --- случайный вектор.

Тогда ***характеристической функцией*** называется:

<!-- $$\psi(\lambda) = M[e^{i\lambda^{T}X] = \int_\R^{n}e^{i\lambda^{T}X}dF_X(x)$$ -->
$$\psi_{X}(\lambda) = M[e^{i\lambda^{T}X}] = \int\limits_{\R^{n}}e^{i\lambda^{T}X}dF_X(x)$$

## Замечание {-}

*Характеристическая функция* определена для любого случайного вектора или с.в.

Если с.в **дискретная**, то:

$$\psi_{X}(\lambda) = \sum\limits_{k = 1}^{\infty}e^{i\lambda X_{k}}p_{k}$$

Если с.в **абсолютно непрерывная**, то

$$\psi_{X}(\lambda) = \int\limits_{\R}e^{i\lambda X}f_{X}(x)dx$$

В этом случае $\psi_{X}(\lambda)$ является ***преобразованием Фурье $f_X$***.

Поскольку преобразование Фурье взаимно однозначно, а $f_X$ однозначно определяет распределение, то характеристическая функция
характеристическая функция $\psi_{X}(x)$ также однозначно определяет распределение с.в $X$.

Причем:

$$f_{X}(x) = \frac{1}{(2\pi)^{n}} \int\limits_{\R}e^{-i\lambda^{T} X}\psi_{X}(x)d\lambda$$


## Лемма 5

Пусть $X$ --- случайный вектор, $A \in \R^{n \times n}, b \in \R^{n}$.

Тогда:

1. для $Y = AX + b$
$$\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda)$$

2. компоненты вектора $X$ ***независимы*** тогда и только тогда, когда
$$\psi_{Y}(\lambda) = \prod_{k = 1}^{n} \psi_{X_k}(\lambda_k)$$

### Доказательство {-}

1. $\psi_{Y}(\lambda) = M[e^{i\lambda^{T}Y}] = M[e^{i\lambda^{T} AX} e^{i\lambda^{T} b}] 
= e^{i\lambda{T} b} M[e^{i(A^{T}\lambda)^{T} X}] = e^{i\lambda^{T} b} \psi_{X}(A^{T}\lambda)$

2. $\psi_{X}(\lambda) = \int\limits_{\R} \dots \int\limits_{\R} e^{i(\lambda_{1} x_{1} + \dots + \lambda_{n} x_{n})} f_{X}(x_1, \dots, x_n)dx_1 \cdot \dots \cdot dx_n =$
{н/з} $= \int\limits_{\R} e^{i\lambda_{1} x_{1}} \cdot \dots \cdot e^{\lambda_{n}
x_{n}} \cdot f_{X_{1}}(x) \cdot \dots \cdot f_{X_{n}}dx_1 \cdot \dots \cdot dx_n =
\int\limits_{\R} e^{i\lambda_{1} x_{1}} f_{x_1}(x_1)dx_1 \cdot \dots \cdot \int\limits_{\R} e^{i\lambda_{n} x_{n}} f_{x_n}(x_n)dx_n=
\prod_{k = 1}^{n} \psi_{X_k})(\lambda_{k})$

$\blacksquare$
<!-- mb make a bit pretty later -->

## Замечание {-}

При помощи характеристической функции можно дать другое определение нормального распределения. В том числе для вырожденного $K_{X}$.

## Определение 3

Случайный вектор $X$ называется ***нормальным***: $X \sim N(m_{X}, K_{X})$, если:

$$\psi_{X}(\lambda) = \exp\{i\lambda^{T} m_{X} - \frac{1}{2} \lambda^{T} K_{X}\lambda \}$$

### Доказательство леммы 3

В силу  [Лемма 5] *(пункт 1)*

\begin{align*}
\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda) = e^{i\lambda^{T}b} \exp\{i\lambda^{T} Am_x - \frac{1}{2} \lambda^{T}
AK_{X}A^{T}\lambda\} = \\
= \exp\{i\lambda^{T} (Am_x + b) - \frac{1}{2}\lambda^{T} (AK_xA^{T})\lambda\} \\
\blacksquare
\end{align*}

### Доказательство леммы 4

Пусть $X_{i}, \dots, X_{n}$ *попарно некоррелированы*. Тогда $cov(X_{i}, X_{j}) = 0$, $i \ne 0$, т.е. :

\begin{align*}
K_x = diag(\sigma_{X_1}^{2}, \dots, \sigma_{X_n}^{2}) = \\
    =  
    \begin{pmatrix}
    \sigma_{X_1}^{2} & \dots & 0 \\
    \vdots & \ddots & \vdots   \\ 
    0 & \dots & \sigma_{X_n}^{2} \\
    \end{pmatrix}
\end{align*} 

$\psi_{X}(\lambda) = \exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2} \lambda^{T} K_{X} \lambda \} =
\exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2}(\lambda_{1}^{2} \sigma_{X_1}^{2} + \dots + 
\lambda_{n}^{2} \sigma_{X_n}^{2})\} = \prod\limits_{k = 1}^{n} \exp\{i\lambda_{n} m_{X_n} - \frac{1}{2} \lambda_{k}^{2} \sigma_{K_n}^{2}\} = \prod\limits_{k = 1}^{n} \psi_{X_k} (\lambda_{k})$.

Откуда с учетом [Лемма 5] *(пункт 2)* $X_1, \dots, X_n$ --- н/з.

Пусть $X_1, \dots, X_n$ --- н/з. Тогда $X_1, \dots, X_n$ *попарно некоррелированы*. $\blacksquare$

## Замечание {-}

Поскольку $K_x$ --- невырожденная, симметричная и положительноопределенная, то существует $S \in \R^{n \times n}$ --- *ортогональная*
(т. е. $S^{T} = S^{-1}$) такая, что:

$$S^{T}K_{X}S = \Lambda = diag(\lambda_1, \dots, \lambda_n)$$

где $\lambda_{i} > 0, i = \overline{1, n}$

Определим матрицу $\Lambda^{- \frac{1}{2}} = diag(\lambda_{1}^{- \frac{1}{2}}, \dots, \lambda_{n}^{- \frac{1}{2}})$.

Рассмотрим вектор

$$Y = \Lambda^{- \frac{1}{2}} S^{T}(X - m_{X})$$

Тогда $A = \Lambda^{- \frac{1}{2}}S^{T}, b = - \Lambda^{- \frac{1}{2}}S^{T}m_{X}$.

В силу [Лемма 3]:

$$m_{Y} = Am_{X} + b = \Lambda^{- \frac{1}{2}}S^{T} - \Lambda^{- \frac{1}{2}}S^{T}m_{X} = 0,$$
$$K_{Y} = AK_{X}A^{T} = \Lambda^{- \frac{1}{2}}S^{T}K_{X}S\Lambda^{- \frac{1}{2}} = I,$$

т. е. $Y \sim N(0, I)$.

При помощи невырожденного линейного преобразования с.в. $X$ может быть преобразован в стандартный нормальный вектор. 

Верно и обратное:

$$X = m_{X} + S\Lambda^{\frac{1}{2}} Y,$$

откуда следует [Лемма 2].

\pagebreak

# Теорема о нормальной корреляции

## Определение 1

***Условным математическим ожиданием абсолютно непрерывного случайного вектора*** $X$ относительно абсолютно непрерывного случайного вектора $Y$ называется:

$$M[X] = \int\limits_{- \infty}^{+ \infty} xf_{X | Y} (x \mid Y)dx,$$

где $f_{X \mid Y} (x \mid Y) = \frac{f_{z}(x, Y)}{f_{y}(Y)}, z = 
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix}$

## Основные свойства условного М. О.

### Свойство 1

\fbox{$M[C \mid Y] = C$}

**\underline{Доказательство}**

$M[C \mid Y] = \int\limits_{- \infty}^{+ \infty} Cf_{X \mid Y} (x \mid Y)dx = \frac{C \int\limits_{- \infty}^{+ \infty} f_{z}(x, Y)dx}
{f_{Y}(Y)} = C \frac{f_{Y}(Y)}{f_{Y}(Y)} = C$. $\blacksquare$

### Свойство 2 

\fbox{$M[X \phi(Y) | Y] = \phi(Y)M[X \mid Y]$}

**\underline{Доказательство}**

$M[\phi(Y) X \mid Y] = \int\limits_{- \infty}^{+ \infty} \phi(Y)X f_{X \mid Y}(x \mid Y)dx = \phi(Y) \int\limits_{- \infty}^{+ \infty} x
f_{X \mid Y} (x \mid Y)dx =$

$= \phi(Y) M[X \mid Y]$. $\blacksquare$

### Свойство 3 

\fbox{$M[\alpha X_{1} + \beta X_{2} \mid Y] = \alpha M[X_{1} \mid Y] + \beta M[X_{2} \mid Y]$}

### Свойство 4 

Пусть $X, Y$ --- *независимые*. Тогда \fbox{$M[X \mid Y] = M[X]$}

**\underline{Доказательство}**

$M[X \mid Y] = \int\limits_{- \infty}^{+ \infty} xf_{X \mid Y} (x \mid Y)dx = \int\limits_{- \infty}^{+ \infty} x \frac{f_{z}(x, Y)}
{f_{Y}(Y)}dx = \int\limits_{- \infty}{+ \infty}x \frac{f_{X}(x) f_{Y}(Y)}{f_{Y}(Y)} dx = M[X]$. $\blacksquare$

### Свойство 5 

\fbox{$M[M[X \mid Y]] = M[X]$} ***(формула повторного М. О.)***

\pagebreak

**\underline{Доказательство}**

$M[M[X \mid Y]]= \int\limits_{- \infty}^{+ \infty} M[X \mid Y] f_{Y} (y)dy = \int\limits_{- \infty}^{+ \infty} f_{Y}(y) 
\int\limits_{- \infty}^{+ \infty} x f_{X \mid Y} (x \mid y)dxdy = \int\limits_{- \infty}^{+ \infty}
\int\limits_{- \infty}^{+ \infty} x \frac{f_{Y}(y) f_{z}(x, y)}{f_{Y}(y)} dydx = \int\limits_{- \infty}^{+ \infty} x
\int\limits_{- \infty}^{+ \infty} f_{z} (x, y)dydx = \int\limits_{- \infty}^{+ \infty} x f_{X}(x)dx = M[X]$. $\blacksquare$

## Лемма 1

Пусть $X, Y$ --- случайные векторы с конечными вторыми моментами. Тогда:

<!-- check to power of 'phi(Y)..' -->
$M[(X - \hat{X})\phi(Y)] = 0$

где $\hat{X} = M[X \mid Y]$

### Доказательство {-}

$M[(X - \hat{X})\phi(Y)^{T}] = M[X \phi(Y)^{T}] - M[M[X \mid Y] \phi(Y)^{T}] =$

= по [Свойство 2] $= M[X \phi(Y)^{T}] - M[M[X \phi(Y)^{T} \mid Y]] =$ по [Свойство 5] $= M[X \phi(Y)^{T}] - M[X \phi(Y)^{T}]
= 0$. $\blacksquare$


## Замечание {-}

Если рассмотреть евклидово пространство $\Ll_{2}(\Omega)$ со скалярным произведением:

$$(X, Y) = M[X \cdot Y]$$

то *условное* М. О. --- ***оператор ортогонального проектирования*** $X$ на подпространство, порождаемое $Y$.

## Определение 2

***Оценкой $X$ по наблюдениям $Y$*** называется любая измеримая функция $\phi(Y)$.

## Определение 3

***Оценка $\hat{X}$ называется с.к.-оптимальной оценкой $X$***, если для любой другой оценки $\tilde{X}$ верно

$$M[| \tilde{X} - \hat{X}|^{2}] \leqslant M[| X - \tilde{X} |^{2}]$$ 

## Теорема 1

$M[X \mid Y]$ --- ***с.к.-оптимальная оценка $X$ по наблюдениям $Y$***.

### Доказательство {-}

$M[|X - \tilde{X}|^{2}] = M[|X - \hat{X} + \hat{X} - \tilde{X}|^{2}] = M[|X - \hat{X}|^{2}] + 2M[(X - \hat{X})^{T} (\hat{X} -
\tilde{X})] + M[|\hat{X} - \tilde{X}|^{2}] \overset{*}{=}$

Поскольку по определению $\tilde{X} - \hat{X} = \phi(Y)$, то в силу [Лемма 1] $M[(X - \hat{X})^{T}(\tilde{X} - \hat{X})] = 0$.

$\overset{*}{=} M[|X - \hat{X}|^{2}] + M[|\hat{X} - \tilde{X}|^{2}] \geqslant M[|X - \hat{X}|^{2}]$. $\blacksquare$

## Теорема 2 (О нормальной корреляции)

Пусть 
\begin{align*}
    \begin{pmatrix}
        X \\
        Y \\
    \end{pmatrix}
    \sim N
    \begin{pmatrix}
        \begin{pmatrix}
        m_{X} \\
        m_{Y} \\
        \end{pmatrix},
        \begin{pmatrix}
        K_{X} & K_{XY} \\
        K_{XY}^{T} & K_{Y} \\
        \end{pmatrix}
    \end{pmatrix}
\end{align*}

Тогда 

1. $Law(X \mid Y) = N(\mu(Y), \Delta)$,

где 

$$\mu(Y) = M[X \mid Y] = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

$$\Delta = K_{X} - K_{XY}K_{Y}^{-1}K_{YX}$$

2. $M[|X - \mu(Y)|^{2}] = tr(K_{X} - K_{XY}K_{Y}^{-1}K_{YX})$

### Доказательство {-}

Рассмотрим линейное преобразование $Y$:

$$\mu(Y) = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

В силу [Лемма 1] *(пункт 3)*

$X - \mu(Y) = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} \sim N(\mu, K)$

$\mu = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    m_{X} \\
    m_{Y} \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} = 0$

$K = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    K_{X} & K_{XY} \\
    K_{XY}^{T} & K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\ 
    -(K_{XY}K_{Y}^{-1})^{T} \\
\end{pmatrix} =$

$= \begin{pmatrix}
    K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{T} & K_{XY} - K_{XY}K_{Y}^{-1}K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\
    K_{Y}^{-1}K_{XY}^{T} \\
\end{pmatrix} =$

$= K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{-1} = \Delta$

$cov(X - \mu(Y), Y) = cov(X, Y) - cov(\mu(Y), Y) = cov(X, Y) - cov(K_{XY}K_{Y}^{-1}Y + m_{X} - K_{XY}K_{Y}^{-1}m_{Y}, Y) =
cov(X, Y) - K_{XY}K_{Y}^{-1}cov(Y, Y) = K_{XY} - K_{XY}K_{Y}^{-2}K_{Y} = 0$

т.е. $X - \mu(Y)$ и $Y$ **некорреливаны**.

Тогда в силу [Лемма 1] *(пункт 2)* $X - \mu(Y)$ и $Y$ **независимы**. Построим характеристическую функцию условного распределения $X$
относительно $Y$:

$\psi_{X \mid Y}(\lambda \mid Y) = \int\limits_{\R^{n}} e^{i\lambda^{T} X} f_{X \mid Y}(x \mid Y)dx = M[e^{i\lambda^{T}X} \mid Y] =
M[e^{i\lambda^{T}(X - \mu(Y))} e^{i\lambda^{T} \mu(Y)} \mid Y] \overset{*}{=}$

в силу [Лемма 2] и *независимости* $X - \mu(Y)$ и $Y$

$\overset{*}{=} M[e^{i\lambda^{T}(X - \mu(Y))} \mid Y] \cdot M[e^{i\lambda^{T}\mu(Y)} \mid Y] = M[e^{i\lambda^{T}(X - \mu(Y))}]
e^{i\lambda^{T} \mu(Y)} = \psi_{X - \mu(Y)}(\lambda) e^{i\lambda^{T} \mu(Y)} = \exp\{- \frac{1}{2} \lambda^{T} \Delta \lambda \}
\cdot \exp\{i\lambda^{T} \mu(Y) - \frac{1}{2} \lambda^{T} \Delta \lambda\}$

т.е. Условное распределение ***нормальное***:

$$X(Y \sim N(\mu(Y), \Delta)$$

Вычислим с.к. ошибку:

$M[|X - \mu(Y)|^{2}] = M[\Delta X_{1}^{2} + \Delta X_{2}^{2} + \dots + \Delta X_{n}^{2}] = \sum\limits_{k = 1}^{n}M[\Delta X_{k}^{2}] = 
\sum\limits_{k = 1}^{n} D[\Delta X_{k}] = \sum\limits_{k = 1}^{n} \Delta_{kk} = tr\Delta$. $\blacksquare$

## Замечание {-}

1. Из [Теорема 2 (О нормальной корреляции)] следует, что в *гауссовском* случае с.к.-оптимальная оценка является ***линейной***.

2. Если $X$ и $Y$ --- *независимы*, то с.к.-оптимальная оценка --- $m_{X}$.

3. С.к.-оптимальная оценка ***несмещенная***, т.к. $M[X - \mu(Y)] = 0$. 

\pagebreak

# Виды сходимости последовательностей случайных величин

## Определение 1

Говорят, что $\{X_{n}\}_{n = 1}^{\infty}$ образует ***последовательность случайных величин***, если $\forall N \in \N$ $X_{n}$
определены на одном вероятностном пространстве.

## Определение 2

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится по вероятности*** к с.в. $X$, если $\forall \varepsilon > 0$:

$$\lim\limits_{n \to \infty} P(|X_{n} - X| \leqslant \varepsilon) = 1$$

ИЛИ

$$\lim\limits_{n \to \infty} P(|X_{n} - X| > \varepsilon) = 0$$

## Определение 3

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится почти наверное*** к с.в. $X$, если

$$P(\{\omega :X_{n}(\omega) \cancel{\overset{n \to \infty}{\longrightarrow}} X(\omega)\}) = 0$$

ИЛИ

$$P(\{\omega :X_{n}(\omega) \overset{n \to \infty}{\longrightarrow} X(\omega)\}) = 1$$

## Определение 4

Говорят, что последовательность с.в. $\{X\}_{n \in \N}$ ***сходится в среднем квадратическом*** к с.в. $X$, если

$$M[|X_{n} - X|^{2}] \overset{n \to \infty}{\longrightarrow} 0$$
