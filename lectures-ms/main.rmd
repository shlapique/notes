---
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
linkcolor: blue
 
mainfont: Linux Libertine O
fontsize: 12pt
numbersections: true
indent: true

header-includes:
- \usepackage{indentfirst}
- \usepackage[russian, english]{babel}
- \usepackage{fancyhdr}
- \usepackage{cancel}

- \pagestyle{fancy}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Ll}{\mathbb{L}}
- \newcommand{\N}{\mathbb{N}}

- \setlength{\headheight}{15pt}
---
\input{./files/title.tex}

\tableofcontents
\clearpage

# Источники {-}

* [Ивченко Г. И., Медведев Ю. И. "Математическая статистика", изд. "Высшая школа", 1984](https://www.hse.ru/pubs/share/direct/content_document/103185710)

* [Кибзун А. И., Наумов А. В., Горяинова Е. Р. "Теория вероятностей и математическая статистика. Базовый курс с примерами и задачами", изд "ФИЗМАТЛИТ", 2013](https://ami.nstu.ru/~post/teaching/tv_ms/kibzun.pdf)

* [Панков А. Р., Платонов Е. Н. "Практикум по математической статистике", изд. "МАИ", 2006](http://zyurvas.narod.ru/knyhy2/Pankov_matstat.pdf)

\pagebreak

# Многомерное нормальное распределение

## Замечание {-}

Вектор $X = (X_{1}, \dots, X_{n})^{T}$ называется ***случайным***, если $X_{1}, \dots, X_{n}$ --- *случайные величины* *(далее **с.в**)*, определенные на одном вероятностном пространстве.

Через $M[X] = m_{X}$ обозначим ***вектор математического ожидания***:

\begin{align*}
M[X] = m_{X} = 
    \begin{pmatrix}
    M[X_1] \\
    \vdots \\
    M[X_n]
    \end{pmatrix}
\end{align*}

Через $K_x$ обозначим *ковариационную матрицу* с.в $X$:

\begin{align*}
K_{X} = 
    \begin{pmatrix}
    \mathrm{cov}(X_1, X_1) & \dots & \mathrm{cov}(X_1, X_n) \\
    \vdots & \ddots & \vdots   \\ 
    \mathrm{cov}(X_n, X_1) & \dots & \mathrm{cov}(X_n, X_n) \\
    \end{pmatrix}
\end{align*}

## Лемма 1

Пусть $K_{X} \in \R^{n \times n}$ --- ковариационная матрица с.в $X$. Тогда:

1. $K_{X} \geqslant 0$, т.е. $\forall x \in \R^{n} \backslash \{0\}, x^{T}K_{X}x \geqslant 0$;

2. $K_{X}^{T} = K_{X}$

## Определение 1

Случайный вектор $X = (X_1, \dots, X_n)^{T}$ называется ***невырожденным нормальным вектором***:

$$X \sim N(m_{X}, K_{X})$$

если совместная плотность вероятности имеет вид:

$$f_{X}(x) = ((2\pi)^{n} \det K_{X})^{\frac{-1}{2}} \exp\{\frac{-1}{2}(x - m_{X})^{T}K_{X}^{-1}(x - m_{X})\}$$

где $m_{X} \in \R^{n}, K_{X} \in \R^{n \times n}, K_{X} > 0, K_{x}^{T} = K_{X}$

## Лемма 2 {#lemma-1-2}

Пусть $X$ --- невырожденный нормальный вектор с параметрами $m_{X}$ и $K_{X}$.

Тогда $M[X] = m_{X}$, а $K_{X}$ --- корвариационная матрица $X$.

Рассмотрим основные свойства многомерного нормального распределения.

## Лемма 3 {#lemma-1-3}

Пусть $X \sim N(m_{X}, K_{X}), A \in \R^{m \times n}, b \in \R^{m}$.

Тогда:

\begin{align*}
Y = AX + b \sim N(m_Y, K_Y), \\
m_Y = Am_X + b, \\
K_Y = AK_XA^{T}.
\end{align*}

## Лемма 4 {#lemma-1-4}

Пусть $X \sim N(m_{X}, K_{X})$.

Тогда компоненты вектора $X$ ***независимы*** тогда и только тогда, когда они *некоррелированы*.

### note {-}

Доказательство данных утверждений при помощи аппарата функций распределения и плотности довольно сложно.
Поэтому рассмотрим аппарат характеристических функций.

## Определение 2

Пусть $X = (X_1, \dots, X_n)^{T}$ --- случайный вектор.

Тогда ***характеристической функцией*** называется:

<!-- $$\psi(\lambda) = M[e^{i\lambda^{T}X] = \int_\R^{n}e^{i\lambda^{T}X}dF_X(x)$$ -->
$$\psi_{X}(\lambda) = M[e^{i\lambda^{T}X}] = \int\limits_{\R^{n}}e^{i\lambda^{T}X}dF_X(x)$$

## Замечание {-}

*Характеристическая функция* определена для любого случайного вектора или с.в.

Если с.в **дискретная**, то:

$$\psi_{X}(\lambda) = \sum\limits_{k = 1}^{\infty}e^{i\lambda X_{k}}p_{k}$$

Если с.в **абсолютно непрерывная**, то

$$\psi_{X}(\lambda) = \int\limits_{\R}e^{i\lambda X}f_{X}(x)dx$$

В этом случае $\psi_{X}(\lambda)$ является ***преобразованием Фурье $f_X$***.

Поскольку преобразование Фурье взаимно однозначно, а $f_X$ однозначно определяет распределение, то характеристическая функция $\psi_{X}(x)$ также однозначно определяет распределение с.в $X$.

Причем:

$$f_{X}(x) = \frac{1}{(2\pi)^{n}} \int\limits_{\R}e^{-i\lambda^{T} X}\psi_{X}(x)d\lambda$$


## Лемма 5 {#lemma-1-5}

Пусть $X$ --- случайный вектор, $A \in \R^{n \times n}, b \in \R^{n}$.

Тогда:

1. для $Y = AX + b$
$$\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda)$$

2. компоненты вектора $X$ ***независимы*** тогда и только тогда, когда
$$\psi_{Y}(\lambda) = \prod\limits_{k = 1}^{n} \psi_{X_k}(\lambda_k)$$

### Доказательство {-}

1. $\psi_{Y}(\lambda) = M[e^{i\lambda^{T}Y}] = M[e^{i\lambda^{T} AX} e^{i\lambda^{T} b}] 
= e^{i\lambda^{T} b} M[e^{i(A^{T}\lambda)^{T} X}] = e^{i\lambda^{T} b} \psi_{X}(A^{T}\lambda)$

2. $\psi_{X}(\lambda) = \int\limits_{\R} \dots \int\limits_{\R} e^{i(\lambda_{1} x_{1} + \dots + \lambda_{n} x_{n})} f_{X}(x_1, \dots, x_n)dx_1 \cdot \dots \cdot dx_n =$
{н/з} $= \int\limits_{\R} e^{i\lambda_{1} x_{1}} \cdot \dots \cdot e^{\lambda_{n}
x_{n}} \cdot f_{X_{1}}(x) \cdot \dots \cdot f_{X_{n}}dx_1 \cdot \dots \cdot dx_n =
\int\limits_{\R} e^{i\lambda_{1} x_{1}} f_{x_1}(x_1)dx_1 \cdot \dots \cdot \int\limits_{\R} e^{i\lambda_{n} x_{n}} f_{x_n}(x_n)dx_n=
\prod\limits_{k = 1}^{n} \psi_{X_k}(\lambda_{k})$

$\blacksquare$
<!-- mb make a bit pretty later -->

## Замечание {-}

При помощи характеристической функции можно дать другое определение нормального распределения. В том числе для вырожденного $K_{X}$.

## Определение 3

Случайный вектор $X$ называется ***нормальным***: $X \sim N(m_{X}, K_{X})$, если:

$$\psi_{X}(\lambda) = \exp\{i\lambda^{T} m_{X} - \frac{1}{2} \lambda^{T} K_{X}\lambda \}$$

### Доказательство леммы 3

В силу [Леммы 5, п.1](#lemma-1-5)

\begin{align*}
\psi_{Y}(\lambda) = e^{i\lambda^{T}b} \psi_{X}(A^{T}\lambda) = e^{i\lambda^{T}b} \exp\{i\lambda^{T} Am_x - \frac{1}{2} \lambda^{T}
AK_{X}A^{T}\lambda\} = \\
= \exp\{i\lambda^{T} (Am_x + b) - \frac{1}{2}\lambda^{T} (AK_xA^{T})\lambda\} \\
\blacksquare
\end{align*}

### Доказательство леммы 4

Пусть $X_{i}, \dots, X_{n}$ *попарно некоррелированы*. Тогда $cov(X_{i}, X_{j}) = 0$, $i \ne 0$, т.е. :

\begin{align*}
K_x = diag(\sigma_{X_1}^{2}, \dots, \sigma_{X_n}^{2}) = \\
    =  
    \begin{pmatrix}
    \sigma_{X_1}^{2} & \dots & 0 \\
    \vdots & \ddots & \vdots   \\ 
    0 & \dots & \sigma_{X_n}^{2} \\
    \end{pmatrix}
\end{align*} 

$\psi_{X}(\lambda) = \exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2} \lambda^{T} K_{X} \lambda \} =
\exp\{i\lambda_{1} m_{X_1} + \dots + i\lambda_{n} m_{X_n} - \frac{1}{2}(\lambda_{1}^{2} \sigma_{X_1}^{2} + \dots + 
\lambda_{n}^{2} \sigma_{X_n}^{2})\} = \prod\limits_{k = 1}^{n} \exp\{i\lambda_{n} m_{X_n} - \frac{1}{2} \lambda_{k}^{2} \sigma_{K_n}^{2}\} = \prod\limits_{k = 1}^{n} \psi_{X_k} (\lambda_{k})$.

Откуда с учетом [Леммы 5, п.1](#lemma-1-5) $X_1, \dots, X_n$ --- н/з.

Пусть $X_1, \dots, X_n$ --- н/з. Тогда $X_1, \dots, X_n$ *попарно некоррелированы*. $\blacksquare$

## Замечание {-}

Поскольку $K_{X}$ --- невырожденная, симметричная и положительноопределенная, то существует $S \in \R^{n \times n}$ --- *ортогональная*
(т. е. $S^{T} = S^{-1}$) такая, что:

$$S^{T}K_{X}S = \Lambda = diag(\lambda_1, \dots, \lambda_n)$$

где $\lambda_{i} > 0, i = \overline{1, n}$

Определим матрицу $\Lambda^{- \frac{1}{2}} = diag(\lambda_{1}^{- \frac{1}{2}}, \dots, \lambda_{n}^{- \frac{1}{2}})$.

Рассмотрим вектор

$$Y = \Lambda^{- \frac{1}{2}} S^{T}(X - m_{X})$$

Тогда $A = \Lambda^{- \frac{1}{2}}S^{T}, b = - \Lambda^{- \frac{1}{2}}S^{T}m_{X}$.

В силу [Леммы 3](#lemma-1-3):

$$m_{Y} = Am_{X} + b = \Lambda^{- \frac{1}{2}}S^{T} - \Lambda^{- \frac{1}{2}}S^{T}m_{X} = 0,$$
$$K_{Y} = AK_{X}A^{T} = \Lambda^{- \frac{1}{2}}S^{T}K_{X}S\Lambda^{- \frac{1}{2}} = I,$$

т. е. $Y \sim N(0, I)$.

При помощи невырожденного линейного преобразования с.в. $X$ может быть преобразован в стандартный нормальный вектор. 

Верно и обратное:

$$X = m_{X} + S\Lambda^{\frac{1}{2}} Y,$$

откуда следует [Лемма 2](#lemma-1-2)

\pagebreak

# Теорема о нормальной корреляции

## Определение 1

***Условным математическим ожиданием абсолютно непрерывного случайного вектора*** $X$ относительно абсолютно непрерывного случайного вектора $Y$ называется:

$$M[X \mid Y] = \int\limits_{- \infty}^{+ \infty} xf_{X | Y} (x \mid Y)dx,$$

где $f_{X \mid Y} (x \mid Y) = \frac{f_{z}(x, Y)}{f_{y}(Y)}, z = 
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix}$

## Основные свойства условного М. О.

### Свойство 1 {#attr-1-1}

\fbox{$M[C \mid Y] = C$}

**\underline{Доказательство}**

$M[C \mid Y] = \int\limits_{- \infty}^{+ \infty} Cf_{X \mid Y} (x \mid Y)dx = \frac{C \int\limits_{- \infty}^{+ \infty} f_{z}(x, Y)dx}
{f_{Y}(Y)} = C \frac{f_{Y}(Y)}{f_{Y}(Y)} = C$. $\blacksquare$

### Свойство 2 {#attr-1-2}

\fbox{$M[X \phi(Y) | Y] = \phi(Y)M[X \mid Y]$}

**\underline{Доказательство}**

$M[\phi(Y) X \mid Y] = \int\limits_{- \infty}^{+ \infty} \phi(Y)X f_{X \mid Y}(x \mid Y)dx = \phi(Y) \int\limits_{- \infty}^{+ \infty} x
f_{X \mid Y} (x \mid Y)dx =$

$= \phi(Y) M[X \mid Y]$. $\blacksquare$

### Свойство 3 {#attr-1-3}

\fbox{$M[\alpha X_{1} + \beta X_{2} \mid Y] = \alpha M[X_{1} \mid Y] + \beta M[X_{2} \mid Y]$}

### Свойство 4 {#attr-1-4}

Пусть $X, Y$ --- *независимые*. Тогда \fbox{$M[X \mid Y] = M[X]$}

**\underline{Доказательство}**

$M[X \mid Y] = \int\limits_{- \infty}^{+ \infty} xf_{X \mid Y} (x \mid Y)dx = \int\limits_{- \infty}^{+ \infty} x \frac{f_{z}(x, Y)}
{f_{Y}(Y)}dx = \int\limits_{- \infty}{+ \infty}x \frac{f_{X}(x) f_{Y}(Y)}{f_{Y}(Y)} dx = M[X]$. $\blacksquare$

### Свойство 5 {#attr-1-5}

\fbox{$M[M[X \mid Y]] = M[X]$} ***(формула повторного М. О.)***

\pagebreak

**\underline{Доказательство}**

$M[M[X \mid Y]]= \int\limits_{- \infty}^{+ \infty} M[X \mid Y] f_{Y} (y)dy = \int\limits_{- \infty}^{+ \infty} f_{Y}(y) 
\int\limits_{- \infty}^{+ \infty} x f_{X \mid Y} (x \mid y)dxdy = \int\limits_{- \infty}^{+ \infty}
\int\limits_{- \infty}^{+ \infty} x \frac{f_{Y}(y) f_{z}(x, y)}{f_{Y}(y)} dydx = \int\limits_{- \infty}^{+ \infty} x
\int\limits_{- \infty}^{+ \infty} f_{z} (x, y)dydx = \int\limits_{- \infty}^{+ \infty} x f_{X}(x)dx = M[X]$. $\blacksquare$

## Лемма 1 {#lemma-2-1}

Пусть $X, Y$ --- случайные векторы с конечными вторыми моментами. Тогда:

<!-- check power of 'phi(Y)..' -->
$M[(X - \hat{X})\phi(Y)^{T}] = 0$

где $\hat{X} = M[X \mid Y]$

### Доказательство {-}

$M[(X - \hat{X})\phi(Y)^{T}] = M[X \phi(Y)^{T}] - M[M[X \mid Y] \phi(Y)^{T}] =$

= по [Свойству 2](#attr-1-2) $= M[X \phi(Y)^{T}] - M[M[X \phi(Y)^{T} \mid Y]] =$ по [Свойству 5](#attr-1-5) $=
M[X \phi(Y)^{T}] - M[X \phi(Y)^{T}]
= 0$. $\blacksquare$

## Замечание {-}

Если рассмотреть евклидово пространство $\Ll_{2}(\Omega)$ со скалярным произведением:

$$(X, Y) = M[X \cdot Y]$$

то *условное* М. О. --- ***оператор ортогонального проектирования*** $X$ на подпространство, порождаемое $Y$.

## Определение 2

***Оценкой $X$ по наблюдениям $Y$*** называется любая измеримая функция $\phi(Y)$.

## Определение 3

***Оценка $\hat{X}$ называется с.к.-оптимальной оценкой $X$***, если для любой другой оценки $\tilde{X}$ верно

$$M[| \tilde{X} - \hat{X}|^{2}] \leqslant M[| X - \tilde{X} |^{2}]$$ 

## Теорема 1

$M[X \mid Y]$ --- ***с.к.-оптимальная оценка $X$ по наблюдениям $Y$***.

### Доказательство {-}

$M[|X - \tilde{X}|^{2}] = M[|X - \hat{X} + \hat{X} - \tilde{X}|^{2}] = M[|X - \hat{X}|^{2}] + 2M[(X - \hat{X})^{T} (\hat{X} -
\tilde{X})] + M[|\hat{X} - \tilde{X}|^{2}] \overset{*}{=}$

Поскольку по определению $\tilde{X} - \hat{X} = \phi(Y)$, то в силу [Леммы 2.1](#lemma-2-1) $M[(X - \hat{X})^{T}(\tilde{X} - \hat{X})] = 0$.

$\overset{*}{=} M[|X - \hat{X}|^{2}] + M[|\hat{X} - \tilde{X}|^{2}] \geqslant M[|X - \hat{X}|^{2}]$. $\blacksquare$

## Теорема 2 (О нормальной корреляции) {#theorem-2-2}

Пусть 
\begin{align*}
    \begin{pmatrix}
        X \\
        Y \\
    \end{pmatrix}
    \sim N
    \begin{pmatrix}
        \begin{pmatrix}
        m_{X} \\
        m_{Y} \\
        \end{pmatrix},
        \begin{pmatrix}
        K_{X} & K_{XY} \\
        K_{XY}^{T} & K_{Y} \\
        \end{pmatrix}
    \end{pmatrix}
\end{align*}

Тогда 

1. $Law(X \mid Y) = N(\mu(Y), \Delta)$,

где 

$$\mu(Y) = M[X \mid Y] = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

$$\Delta = K_{X} - K_{XY}K_{Y}^{-1}K_{YX}$$

2. $M[|X - \mu(Y)|^{2}] = tr(K_{X} - K_{XY}K_{Y}^{-1}K_{YX})$

### Доказательство {-}

Рассмотрим линейное преобразование $Y$:

$$\mu(Y) = m_{X} + K_{XY}K_{Y}^{-1}(Y - m_{Y})$$

В силу [Леммы 1.3](#lemma-1-3)

$X - \mu(Y) = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    X \\
    Y \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} \sim N(\mu, K)$

$\mu = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    m_{X} \\
    m_{Y} \\
\end{pmatrix} -
m_{X} + K_{XY}K_{Y}^{-1}m_{Y} = 0$

$K = (I - K_{XY}K_{Y}^{-1})
\begin{pmatrix}
    K_{X} & K_{XY} \\
    K_{XY}^{T} & K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\ 
    -(K_{XY}K_{Y}^{-1})^{T} \\
\end{pmatrix} =$

$= \begin{pmatrix}
    K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{T} & K_{XY} - K_{XY}K_{Y}^{-1}K_{Y} \\
\end{pmatrix}
\begin{pmatrix}
    I \\
    K_{Y}^{-1}K_{XY}^{T} \\
\end{pmatrix} =$

$= K_{X} - K_{XY}K_{Y}^{-1}K_{XY}^{-1} = \Delta$

$cov(X - \mu(Y), Y) = cov(X, Y) - cov(\mu(Y), Y) = cov(X, Y) - cov(K_{XY}K_{Y}^{-1}Y + m_{X} - K_{XY}K_{Y}^{-1}m_{Y}, Y) =
cov(X, Y) - K_{XY}K_{Y}^{-1}cov(Y, Y) = K_{XY} - K_{XY}K_{Y}^{-2}K_{Y} = 0$

т.е. $X - \mu(Y)$ и $Y$ **некорреливаны**.

Тогда в силу [Леммы 1.5, п.2](#lemma-1-5) $X - \mu(Y)$ и $Y$ **независимы**. Построим характеристическую функцию условного распределения $X$
относительно $Y$:

$\psi_{X \mid Y}(\lambda \mid Y) = \int\limits_{\R^{n}} e^{i\lambda^{T} X} f_{X \mid Y}(x \mid Y)dx = M[e^{i\lambda^{T}X} \mid Y] =
M[e^{i\lambda^{T}(X - \mu(Y))} e^{i\lambda^{T} \mu(Y)} \mid Y] \overset{*}{=}$

в силу [Леммы 1.2](#lemma-1-2) и *независимости* $X - \mu(Y)$ и $Y$

$\overset{*}{=} M[e^{i\lambda^{T}(X - \mu(Y))} \mid Y] \cdot M[e^{i\lambda^{T}\mu(Y)} \mid Y] = M[e^{i\lambda^{T}(X - \mu(Y))}]
e^{i\lambda^{T} \mu(Y)} = \psi_{X - \mu(Y)}(\lambda) e^{i\lambda^{T} \mu(Y)} = \exp\{- \frac{1}{2} \lambda^{T} \Delta \lambda \}
\cdot \exp\{i\lambda^{T} \mu(Y) - \frac{1}{2} \lambda^{T} \Delta \lambda\}$

т.е. Условное распределение ***нормальное***:

$$X(Y \sim N(\mu(Y), \Delta)$$

Вычислим с.к. ошибку:

$M[|X - \mu(Y)|^{2}] = M[\Delta X_{1}^{2} + \Delta X_{2}^{2} + \dots + \Delta X_{n}^{2}] = \sum\limits_{k = 1}^{n}M[\Delta X_{k}^{2}] = 
\sum\limits_{k = 1}^{n} D[\Delta X_{k}] = \sum\limits_{k = 1}^{n} \Delta_{kk} = tr\Delta$. $\blacksquare$

## Замечание {-}

1. Из [Теоремы о нормальной корреляции](#theorem-2-2) следует, что в *гауссовском* случае с.к.-оптимальная оценка является ***линейной***.

2. Если $X$ и $Y$ --- *независимы*, то с.к.-оптимальная оценка --- $m_{X}$.

3. С.к.-оптимальная оценка ***несмещенная***, т.к. $M[X - \mu(Y)] = 0$. 

\pagebreak

# Виды сходимости последовательностей случайных величин

## Определение 1

Говорят, что $\{X_{n}\}_{n = 1}^{\infty}$ образует ***последовательность случайных величин***, если $\forall N \in \N$ $X_{n}$
определены на одном вероятностном пространстве.

## Определение 2

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится по вероятности*** к с.в. $X$, если $\forall \varepsilon > 0$:

$$\lim\limits_{n \to \infty} P(|X_{n} - X| \leqslant \varepsilon) = 1$$

ИЛИ

$$\lim\limits_{n \to \infty} P(|X_{n} - X| > \varepsilon) = 0$$

## Определение 3

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится почти наверное*** к с.в. $X$, если

$$P(\{\omega :X_{n}(\omega) \cancel{\overset{n \to \infty}{\longrightarrow}} X(\omega)\}) = 0$$

ИЛИ

$$P(\{\omega :X_{n}(\omega) \overset{n \to \infty}{\longrightarrow} X(\omega)\}) = 1$$

## Определение 4

Говорят, что последовательность с.в. $\{X\}_{n \in \N}$ ***сходится в среднем квадратическом*** к с.в. $X$, если

$$M[|X_{n} - X|^{2}] \overset{n \to \infty}{\longrightarrow} 0$$

## Пример 1

Пусть $\{X_{n}\}_{n \in \N}$ --- случайная последовательность.

$$X \sim 
\begin{pmatrix}
    0 & n \\
    1 - \frac{1}{n^{2}} & \frac{1}{n^{2}} \\
\end{pmatrix}
$$

$P(\{ \omega : \lim\limits_{n \to \infty} X_{n}(\omega) \neq 0 \}) = P(\{ \omega : \forall N \in \N \exists n \geqslant N: X_{n}(\omega)
= n \}) = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) =n \}) \overset{*}{=}$

1. $\sum\limits_{n = N + 1}^{\infty}\{\omega : X_{n}(\omega) = n\} \subset \sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) = n\}$

2. $P(\sum\limits_{n = N}^{\infty} \{\omega : X_{n}(\omega) = n\} \leqslant \sum\limits_{n = N}^{\infty}
P(\{\omega : X_{n}(\omega) = n \}) = \sum\limits_{n = N}^{\infty} \frac{1}{n^2}
\overset{n \to \infty}{\longrightarrow} 0$, т.к. $\sum\limits_{n = 1}^{\infty} \frac{1}{n^2} < \infty$

Тогда в силу *аксиомы непрерывности* $\overset{*}{=} 0$, т.е.
$X_{n} \overset{\text{п.н.}}{\longrightarrow} 0$

## Пример 2

Рассмотрим ту же последовательность. Выберем $\varepsilon > 0$

$$P(|X_{n} - 0| \leqslant \varepsilon) =
\begin{cases}
    1, & \varepsilon \geqslant n \\
    1 - \frac{1}{n^2}, & \varepsilon \in (0; n) \\
\end{cases}$$

Тогда $X_{n} \overset{P}{\longrightarrow} 0$

## Пример 3

Рассмотрим ту же последовательность:

$M[|X_{n} - 0|^{2}] = M[X_{n}^2] = 0^{2}(1 - \frac{1}{n^2}) + n^{2} \frac{1}{n^2} = 1 
\cancel{\overset{n \to \infty}{\longrightarrow}} 0$

Тогда $X_{n} \cancel{\overset{\text{с.к.}}{\longrightarrow}} 0$

## Пример 4

Пусть $f_{nk}:[0;1] \longrightarrow \{0; 1\}, n \in \N, k = \overline{1, n}$,

$$f_{n, k} =
\begin{cases}
    0, & t \notin [\frac{k - 1}{n}; \frac{k}{n}], \\
    1, & t \in [\frac{k - 1}{n}; \frac{k}{n}]. \\
\end{cases}$$

Пусть $X \sim R(0; 1)$. Рассмотрим последовательность с.в. $X_{nk} = f_{nk}(X)$. $\forall \omega \in
\Omega X(\omega) \in [0;1]$. Тогда $\forall n \in \N \exists k = \overline{1, n}$ такое, что
$X(\omega) \in [\frac{k - 1}{n}; \frac{k}{n}]$. Т.е. если $\varepsilon = \frac{1}{2}$, то $\forall n
\in \N$ найдется $k = \overline{1, n}$ такой, что

$$|f_{nk}(X(\omega)) - 0| > \varepsilon$$

Тогда $X_{nk}(\omega) \cancel{\overset{n \to \infty}{\longrightarrow}} 0$, т.е.

$$ \{\omega : \lim\limits_{n,k \to \infty} X_{nk}(\omega) = 0\} = \varnothing$$

$$X_{nk} \cancel{\overset{\text{п. н.}}{\longrightarrow}} 0$$

При этом $\forall \varepsilon > 0$

$$R(|X_{nk} - 0| > \varepsilon) =
\begin{cases}
    0, & \varepsilon \geqslant 1, \\
    P(X \in [\frac{k - 1}{n}, & \frac{k}{n}]), \varepsilon \in (0; 1) \\
\end{cases} =
\begin{cases}
    0, & \varepsilon \geqslant 1, \\
    \frac{1}{n}, & \varepsilon \in (0; 1) \\
\end{cases} \overset{n \to \infty}{\longrightarrow} 0$$

$$X_{nk} \overset{P}{\longrightarrow} 0$$

$M[|X_{nk} - 0|^{2}] = M[X_{nk}] = M[f_{nk}(X)] = \int\limits_{0}^{1}f_{nk}(x) f_{x}(x)dx = \int\limits_{0}^
{1} f_{nk}(x)dx = \int\limits_{\frac{k -1}{n}}^{\frac{k}{n}} 1 dx = \frac{1}{n} \overset{n \to \infty}
{\longrightarrow} 0$

$$X_{nk} \overset{\text{с.к}}{\longrightarrow} 0$$

## Пример 5

Рассмотрим последовательность с.в. $Y_{n_{1}k} = nK_{n_{1}k}$ Тогда $Y_{n_{1}k} \overset{\text{п.н.}}
{\longrightarrow} 0$. $\forall \varepsilon > 0$.

$P(|Y_{n_{1}k} - 0| > \varepsilon) =
\begin{cases}
    0, & \varepsilon \geqslant n, \\
    P(X \in [\frac{k -1}{n}; \frac{k}{n}]), & \varepsilon \in (0; n) \\
\end{cases} =
\begin{cases}
    0, & \varepsilon \geqslant n, \\
    \frac{1}{n}, & \varepsilon \in (0; n) \\
\end{cases}
\overset{n to \infty}{\longrightarrow} 0$

$$Y_{n_{1}k} \overset{P}{\longrightarrow} 0$$

$M[|Y_{n_{1}k} - 0|^{2}] = M[n^{2} X_{nk}^{2}] = n^{2}M[X_{nk}] = n \overset{P}{\longrightarrow} \infty$

$$Y_{n_{1}k} \cancel{\overset{\text{с.к}}{\longrightarrow}} 0$$

## Замечание {-}

Согласно определению для исследования на сходимость нужно знать совместное распределение с.в. $X_{n}$ и $X$,
 а для случая *сходимости почти наверное* совместное распределение всей последовательности
$\{X_{n}\}_{n \in \N}$ и $X$. Поэтому исследование на сходимость иначе, чем к детерминированной константе, довольно проблематично.

## Лемма 1

Пусть $X_{n} \overset{\text{п.н.}}{\longrightarrow} X$. Тогда $X_{n} \overset{P}{\longrightarrow} X$.

### Доказательство {-}

$0 = P(\{\omega : \lim\limits_{n \to \infty} X_{n}(\omega) \neq X(\omega)\}) = P(\{\omega : \exists 
\varepsilon > 0 \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega) > \varepsilon\}) = 
P(\sum\limits_{\varepsilon > 0)} \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty}
\{ \omega : |X_{n}(\omega) - X(\omega)| > \varepsilon \}) \geqslant
P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| >
\varepsilon '\})$, $\forall \varepsilon ' > 0$

Тогда $0 = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - 
X(\omega)| > \varepsilon\})$,

$$\sum\limits_{n = N + 1}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| > \varepsilon\}) \subset
\sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| > \varepsilon\}$$

В силу *аксиомы непрерывности*:

$0 = \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} \{\omega : |X_{n}(\omega) - X(\omega)| >
\varepsilon\}) \geqslant \lim\limits_{N \to \infty} P(\{\omega : |X_{N}(\omega) - X(\omega)| > 
\varepsilon\})$

$$X_{N} \overset{P}{\longrightarrow} X$$ 
\hfill $\blacksquare$

## Лемма 2 (Неравенство Маркова)

Пусть $P(X \geqslant 0) = 1$, M[X] < \infty$. Тогда $\forall \varepsilon > 0$

$$P(X > \varepsilon) = \frac{M[X]}{\varepsilon}$$

### Доказательство {-}

$M[X] = \int\limits_{0}^{+ \infty} xdF_{X}(x) \geqslant \int\limits_{\varepsilon}^{+ \infty} xdF_{X}(x)
\geqslant \varepsilon \int\limits_{\varepsilon}^{+ \infty} dF_{X}(X) = \varepsilon P(X > \varepsilon)$.
$\blacksquare$

## Следствие 1

Пусть $M[X^{-k}] < \infty$. Тогда $\forall \varepsilon > 0$:

$$P(|X| > \varepsilon) \leqslant \frac{M[|X|^{k}|]}{\varepsilon^{k}}$$

### Доказательство {-}

$P(|X| > \varepsilon) = P(|X|^{k} > \varepsilon^{k}) \leqslant \frac{M[|X|^{k}]}{\varepsilon^{k}}$.
$\blacksquare$

## Следствие 2 (Неравенство Чебышёва)

Пусть $M[X^{2}] < \infty$. Тогда

$$P(|X - M[X]| > \varepsilon) \leqslant \frac{D[X]}{\varepsilon^{2}}$$

### Доказательство {-}

$P(|X - M[X]| > \varepsilon) \leqslant \frac{M[|X - M[X]|^{2}]}{\varepsilon^{2}} = \frac{D[X]}
{\varepsilon^{2}}$. $\blacksquare$

## Лемма 3 {#lemma-3-3}

Пусть $X_{n} \overset{\text{с.к.}}{\longrightarrow} X$. Тогда $X_{n} \overset{P}{\longrightarrow} X$.

### Доказательство {-}

$P(|X_{n} - X| > \varepsilon) \leqslant \frac{M[|X_{n} - X|^{2}]}{\varepsilon^{2}} 
\overset{n \to \infty}{\longrightarrow} 0$. $\blacksquare$

## Теорема 1 (Бореля --- Кантелли) {#theorem-3-1}

Пусть $A_{1}, \dots, A_{n} \subset \Omega, B = \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty}
A_{n}$. Тогда

1. Если $\sum\limits_{n = 1}^{\infty} P(A_{n}) < \infty$, то $P(B) = 0$;

2. Если $A_{1}, \dots, A_{n}$ **независимы в совокупности** и $\sum\limits_{n = 1}^{\infty} P(A_{n}) = \infty$
, то $P(B) = 1$.

### Доказательство {-}

1. $P(B) = P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} A_{n}) \overset{*}{=}$

т.к. $\sum\limits_{n = N + 1}^{\infty} A_{n} \subset \sum\limits_{n = N}^{\infty} A_{n}$, то *по
аксиоме непрерывности* $\overset{*}{=} \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} A_{n})
\leqslant \lim\limits_{N \to \infty} \sum\limits_{n = N}^{\infty} P(A_{n})
\overset{N \to \infty}{\longrightarrow} 0$, т.к. $\sum\limits_{n = 1}^{\infty} P(A_{n}) < \infty$

2. $P(B) = \dots = \lim\limits_{N \to \infty} P(\sum\limits_{n = N}^{\infty} A_{n}) = 
\lim\limits_{N \to \infty} (1 - P(\prod\limits_{n = N}^{\infty} \overline{A_{n}})) = 
1 - \lim\limits_{N \to \infty} P(\prod\limits_{M = N}^{\infty} \prod\limits_{n = N}^{M} \overline{A_{n}})
\overset{*}{=}$

т.к. $\prod\limits_{n = N}^{M + 1} \overline{A_{n}} \subset \prod\limits_{n = N}^{M} \overline{A_{n}}$, то
по *аксиоме непрерывности*

$\overset{*}{=} 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} P(\prod\limits_{n = N}^{M}
\overline{A_{n}}) = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} \prod\limits_{n = N}^{M}
(1 - P(A_{n})) = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty} \prod\limits_{n = N}^{M} 
e^{\ln(1 - P(A_{n}))} = 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty}
e^{\sum\limits{n = N}^{M} \ln(1 - P(A_{n}))} \overset{*}{\geqslant}$

т.к. $\ln(1 - t) = -t - \frac{t^{2}}{2} - \frac{t^{3}}{3} - \frac{t^{4}}{4} - \dots < -t$, то

$\overset{*}{\geqslant} 1 - \lim\limits_{N \to \infty} \lim\limits_{M \to \infty}
e^{- \sum\limits_{n = N}^{M} P(A_{n})} = 1 - \lim\limits_{n \to \infty} 0 = 1$. $\blacksquare$

<!-- тут в конспекте опечатка было 'Лемма 3' -->
## Лемма 4 {#lemma-3-4}

Пусть $X_{n} \overset{P}{\longrightarrow} X$,

$$\sum\limits_{n = 1}^{\infty} P(|X_{n} - X| > \varepsilon) < \infty$$ 
Тогда $X_{n} \overset{\text{п.н.}}{\longrightarrow} X$

### Доказательство {-}

В силу [Теоремы 3.1](#theorem-3-1) $\forall \varepsilon > 0$

$$P(\prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} \{\omega : |X_{n} - X| > \varepsilon \}) = 0$$

Тогда

$P(\{\omega : \lim\limits_{n \to \infty} X_{n} X_{n}(\omega) \neq X(\omega)\}) =
P(\{\omega : \exists \varepsilon > 0, \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega)|> 
\varepsilon \}) = 
P(\{\omega : \exists M \in \N, \forall N \in \N \exists n \geqslant N : |X_{n}(\omega) - X(\omega)|>
\frac{1}{M}\}) = P(\sum\limits_{M = 1}^{\infty} \prod\limits_{N = 1}^{\infty} \sum\limits_{n = N}^{\infty} 
\{ \omega : |X_{n} - X| > \frac{1}{M} \}) \leqslant 
\sum\limits_{M = 1}^{\infty} P(\prod\limits_{N = 1}^{\infty} \sum\limits{n = N}^{\infty} 
\{ \omega : |X_{n} - X| > \frac{1}{M} \}) = 0$. $\blacksquare$

## Замечание {-}

1.

$X_{n} \overset{\text{с.к.}}{\longrightarrow} X \implies X_{n} \overset{P}{\longrightarrow} X$

ИЛИ

$X_{n} \overset{\text{п.н.}}{\longrightarrow} X \implies X_{n} \overset{P}{\longrightarrow} X$

2. В силу *теоремы Рисса* (функциональный анализ) если $X_{n} \overset{P}{\longrightarrow} X$,
то существует *подпоследовательность* $\{X_{n_{k}}\}_{k \in \N} : X_{n_{k}} 
\underset{k \to \infty}{\overset{\text{п.н.}}{\longrightarrow}} X$

3. В силу *теоремы о мажорирующей сходимости*, если $X_{n} \overset{P}{\longrightarrow} X$ и $\exists Y$ ---
с.в.: $|X_{n}| \leqslant Y, M[Y^{2}] < \infty$, то $X_{n} \overset{\text{с.к.}}{\longrightarrow} X$

4. Также из функционального анализа известно, что операция предела (по мере, почти наверное, в средне
квадратическом) замкнута относительно линейных операций и непрерывных преобразований.

\pagebreak

# Закон больших чисел

## Определение 1

***Выборкой*** объема $n$ будем называть с.в. $Z_{n} = (X_{1}, \dots, X_{n})^{T}$, где 
$X_{1}, \dots, X_{n}$ --- *независимые* с.в.

Через $F_{k}(x)$ обозначим *функцию распределения* k-го элемента выборки.

Если $F_{k} = F_{1}, k = \overline{2, n}$, то выборка называется ***однородной***.

## Определение 2

***Выборочным средним*** $\overline{X_{n}}$ выборки $Z_{n}$ называется $\overline{X_{n}} = \frac{1}{n}
\sum\limits_{k = 1}^{n} X_{k}$

## Теорема 1 (Закон Больших Чисел Чебышёва)

Пусть $Z_{n}$ --- однородная выборка, $M[X_{k}^{2}] < \infty$.

Тогда $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}, \overline{X_{n}} 
\overset{P}{\longrightarrow} m_{X}$

### Доказательство {-}

$M[|\overline{X_{n}} - m_{X}|^{2}] = M[|\overline{X_{n}} - M[\overline{X_{n}}]|^{2}|^{2}] = 
D[\overline{X_{n}}] = \frac{1}{n^{2}} D[\sum\limits_{k = 1}^{n} X_{k}] = 
\frac{n D[X_{1}]}{n^{2}} \overset{n \to \infty}{\longrightarrow} 0$

Т.е. по определению $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}$.

С учетом [Леммы 3.3](#lemma-3-3) $\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$. $\blacksquare$

## Теорема 2 (Закон Больших Чисел Колмогорова)

Пусть $Z_{n}$ --- *однородная* выборка, $M[X_{k}] = m_{X} < \infty$.

Тогда $\overline{X_{n}} \overset{\text{п.н.}}{\longrightarrow} m_{X}$

<!-- понять, че за 'т.о' -->
## Замечание {-}
Т.о. для *однородной* выборки $\overline{X_{n}}$ сходится **почти наверное** и **по вероятности** к $m_{X}$,
\underline{если оно существует}, и **в среднем квадратичном**, \underline{если существует дисперсия}.

## Теорема 3 {#theorem-4-3}

Пусть $Z_{n}$ --- *неоднородная* выборка, $M[X_{k}] = m_{X} < \infty, D[X_{k}] = D_{k} \leqslant D_{max} <
\infty$, где $k \in \N$

Тогда $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}, 
\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$.

### Доказательство {-}

$M[|\overline{X_{n}} - m_{X}|^{2}] = M[|\overline{X_{n}} - M[\overline{X_{n}}]|^{2}] = D[X_{n}] =
\frac{1}{n^{2}} \sum\limits_{k = 1}^{n} D_{k} \leqslant \frac{n D_{max}}{n^{2}} = 
\frac{D_{max}}{n} \overset{n \to \infty}{\longrightarrow} 0$

Т.о. $\overline{X_{n}} \overset{\text{с.к.}}{\longrightarrow} m_{X}$. Тогда в силу [Леммы 3.3](#lemma-3-3)
$\overline{X_{n}} \overset{P}{\longrightarrow} m_{X}$. $\blacksquare$

## Следствие 1

Пусть $Z_{n}$ --- *неоднородная* выборка, $D[X_{k}] = D_{k} \leqslant D_{max} < \infty, k \in \N$.

Тогда $\overline{X_{n}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} \overset{\text{с.к.}}{\longrightarrow}
0, \overline{X_{k}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} \overset{P}{\longrightarrow} 0$.

### Доказательство {-}

$\overline{X_{n}} - \frac{1}{n} \sum\limits_{k = 1}^{n} m_{X_{k}} = 
\frac{1}{n} \sum\limits_{k = 1}^{n} (X_{k} - m_{X_{k}}) = \frac{1}{n} \sum\limits_{k = 1}^{n} Y_{k}$,

где $M[Y_{k}] = 0, D[Y_{k}] = D[X_{k}] = \leqslant D_{max} < \infty$

Тогда $\overline{Y_{k}}$ удовлетворяет условиям [Теоремы 4.3](#theorem-4-3). $\blacksquare$

## Теорема 4 {#theorem-4-4}

Пусть $Z_{n}$ --- *неоднородная* выборка, $M[X_{k}] = m_{X} < \infty, D[X_{k}] = D_{k} < \infty$,

$$\sum\limits_{k = 1}^{n} \frac{D[X_{k}]}{k^{2}} < \infty$$

Тогда

$$\overline{X_{n}} \overset{\text{п.н.}}{\longrightarrow} m_{X}$$

## Замечание {-}

Условие [Теоремы 4](#theorem-4-4) более мягкое, чем условие [Теоремы 3](#theorem-4-3). Пусть 
$D[X_{k}] \leqslant D_{max}, k \in \N$. Тогда 

<!-- dont know what exactly means sign like '6' or \sigma -->
$$\sum\limits_{k = 1}^{n} \frac{D[X_{k}]}{k^{2}} \leqslant \sum\limits_{k = 1}^{n} \frac{D_{max}}{k^{2}} =
\frac{\pi^{2} D_{max}}{\sigma} < \infty$$

## Следствие 2

Пусть $N(A)$ --- число появления события $A$ в серии из $N$ *независимых* опытов. Тогда

$$\frac{N(A)}{N} \overset{\text{п.н.}}{\longrightarrow} P(A), \frac{N(A)}{N} 
\overset{\text{с.к.}}{\longrightarrow} P(A)$$

### Доказательство {-}

По условию $N(A) \sim Bi(N; P(A))$. Тогда $\exists X_{1}, \dots, X_{n} \sim Be(P(A))$ --- независимые с.в.

При этом $M[X_{1}] = P(A), D[X_{1}] = P(A)(1 - P(A)) \leqslant \frac{1}{4}$.

Тогда в силу [Теоремы 4](#theorem-4-1)

$$\frac{N(A)}{N} = \frac{1}{N} \sum\limits_{k = 1}^{N} X_{k} \overset{\text{с.к.}}{\longrightarrow} P(A)$$

в силу [Теоремы 2](#theorem-4-2)

$$\frac{N(A)}{N} = \frac{1}{N} \sum\limits_{k = 1}^{N} X_{k} \overset{\text{п.н.}}{\longrightarrow} P(A)$$

\hfill $\blacksquare$

\pagebreak 

# Центральная предельная теорема (ЦПТ)

## Замечание {-}

Сходимости ***с.к.***, ***п.н.*** и ***$P$*** в общем случае исследования предполагают либо знание
совместного распределения элементов последовательности, либо наличие точкой функциональной зависимости
от $\omega \in \Omega$.

Как правило, в теории вероятностей это неизвестно, а с.в. описываются при помощи их распределений, а не
как функции. При этом если у двух величин совпадают распределения, то это вовсе не значит, что они равны.

Поэтому довольно важным является вид сходимости *по распределению*, т.е. в смысле **"описательного
инструмента" с.в.

## Определение 1

Говорят, что последовательность с.в. $\{X_{n}\}_{n \in \N}$ ***сходится по распределению*** к с.в. $X$,
если 

$F_{X_{n}}(x) \overset{n \to \infty}{\longrightarrow} F_{X}(x)$, $\forall x$ --- точки непрерывности
$F_{X}(x)$.

## Лемма 1 {#lemma-5-1}

Пусть $X_{n} \overset{P}{\longrightarrow} X$. Тогда $X_{n} \overset{d}{\longrightarrow} X$

## Лемма 2 {#lemma-5-2}

Пусть $X_{n} \overset{d}{\longrightarrow} X$, $Y_{n} \overset{P}{\longrightarrow} C$

Тогда $X_{n} + Y_{n} \overset{d}{\longrightarrow} C$

### Доказательство {-}

Пусть $C = 0$, $x_{0}$ --- точка **непрерывности** $F_{X}(x)$. Тогда $\forall \varepsilon > 0$

$F_{X_{n} + Y_{n}}(x_{0}) = P(X_{n} + Y_{n} \leqslant x_{0}) = P(\{X_{n} + Y_{n} \leqslant x_{0}\}
\{|Y_{n}| > \varepsilon\}) =$

